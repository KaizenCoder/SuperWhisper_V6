#!/usr/bin/env python3
"""
ü§ñ VALIDATION LLM INDIVIDUELLE - SUPERWHISPER V6
===============================================
Script validation LLM critique pour pipeline voix-√†-voix < 1.2s end-to-end

MISSION CRITIQUE:
- Valider LLM < 400ms pour objectif total < 1.2s
- Configuration RTX 3090 (CUDA:1) OBLIGATOIRE
- Tests qualit√© r√©ponses conversationnelles fran√ßaises
- S√©lection meilleur mod√®le pour production

Usage: python scripts/test_llm_validation.py
"""

import os
import sys
import asyncio
import json
import time
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import httpx

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Configuration logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("llm_validation")

@dataclass
class LLMTestResult:
    """R√©sultats validation LLM"""
    model_name: str
    endpoint_url: str
    available: bool = False
    latency_ms: float = 0.0
    response_text: str = ""
    quality_score: float = 0.0
    error: Optional[str] = None
    test_count: int = 0
    success_rate: float = 0.0

def validate_rtx3090_configuration():
    """Validation obligatoire configuration RTX 3090"""
    try:
        import torch
        
        if not torch.cuda.is_available():
            raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
        
        cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
        if cuda_devices != '1':
            raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
        
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory < 20:  # RTX 3090 = ~24GB
            raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
        
        print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è PyTorch non disponible - validation GPU ignor√©e")
        return True
    except Exception as e:
        raise RuntimeError(f"üö´ Validation GPU √©chou√©e: {e}")

class OllamaLLMValidator:
    """Validateur LLM sp√©cialis√© Ollama pour SuperWhisper V6"""
    
    def __init__(self):
        self.base_url = "http://localhost:11434"
        self.timeout = 30
        # Mod√®les prioritaires identifi√©s par diagnostic
        self.priority_models = [
            "llama3.2:latest",      # 3.2B - Optimal conversation
            "llama3.2:1b",          # 1.2B - Rapide
            "qwen2.5-coder:1.5b",   # 1.5B - Tr√®s rapide  
            "lux_model:latest",     # 3.2B - Sp√©cialis√©
        ]
        
        # Tests conversation fran√ßaise
        self.test_prompts = [
            "Bonjour ! Comment allez-vous aujourd'hui ?",
            "Pouvez-vous m'expliquer bri√®vement ce qu'est l'intelligence artificielle ?",
            "Quelle est la capitale de la France ?",
            "Racontez-moi une courte histoire en fran√ßais.",
            "Comment dit-on 'hello' en fran√ßais ?"
        ]
    
    async def get_available_models(self) -> List[str]:
        """R√©cup√®re les mod√®les disponibles sur Ollama"""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                
                logger.info(f"üìã {len(models)} mod√®les Ollama disponibles")
                return models
                
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration mod√®les: {e}")
            return []
    
    async def test_model_inference(self, model_name: str, prompt: str) -> Dict[str, Any]:
        """Test inf√©rence LLM pour un mod√®le sp√©cifique"""
        start_time = time.time()
        
        try:
            payload = {
                'model': model_name,
                'prompt': prompt,
                'stream': False,
                'options': {
                    'temperature': 0.7,
                    'num_predict': 100,
                    'top_p': 0.9
                }
            }
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json=payload
                )
                response.raise_for_status()
                
                data = response.json()
                response_text = data.get('response', '').strip()
                
                latency_ms = (time.time() - start_time) * 1000
                
                return {
                    'success': True,
                    'latency_ms': latency_ms,
                    'response_text': response_text,
                    'error': None
                }
                
        except Exception as e:
            latency_ms = (time.time() - start_time) * 1000
            return {
                'success': False,
                'latency_ms': latency_ms,
                'response_text': '',
                'error': str(e)
            }
    
    def evaluate_response_quality(self, prompt: str, response: str) -> float:
        """√âvaluation qualit√© r√©ponse (0-10)"""
        if not response or len(response.strip()) < 5:
            return 0.0
        
        score = 5.0  # Score base
        
        # Crit√®res qualit√©
        if len(response) >= 20:  # Longueur appropri√©e
            score += 1.0
        if len(response) <= 200:  # Pas trop long
            score += 0.5
        
        # D√©tection fran√ßais (heuristiques)
        french_indicators = ['le', 'la', 'les', 'de', 'du', 'est', 'sont', 'avec', 'pour', 'dans']
        french_count = sum(1 for word in french_indicators if word in response.lower())
        if french_count >= 2:
            score += 1.5
        
        # Coh√©rence contextuelle
        if 'bonjour' in prompt.lower() and any(word in response.lower() for word in ['bonjour', 'salut', 'bonsoir']):
            score += 1.0
        if 'capital' in prompt.lower() and 'paris' in response.lower():
            score += 1.0
        if 'hello' in prompt.lower() and 'bonjour' in response.lower():
            score += 1.0
        
        return min(10.0, score)
    
    async def validate_model(self, model_name: str) -> LLMTestResult:
        """Validation compl√®te d'un mod√®le LLM"""
        logger.info(f"üß™ Validation mod√®le: {model_name}")
        
        result = LLMTestResult(
            model_name=model_name,
            endpoint_url=self.base_url
        )
        
        latencies = []
        quality_scores = []
        responses = []
        successful_tests = 0
        
        for i, prompt in enumerate(self.test_prompts):
            logger.info(f"   Test {i+1}/5: {prompt[:50]}...")
            
            test_result = await self.test_model_inference(model_name, prompt)
            
            if test_result['success']:
                successful_tests += 1
                latencies.append(test_result['latency_ms'])
                
                quality = self.evaluate_response_quality(prompt, test_result['response_text'])
                quality_scores.append(quality)
                responses.append(test_result['response_text'])
                
                logger.info(f"   ‚úÖ {test_result['latency_ms']:.1f}ms - Qualit√©: {quality:.1f}/10")
            else:
                logger.warning(f"   ‚ùå √âchec: {test_result['error']}")
        
        # Calcul m√©triques finales
        if successful_tests > 0:
            result.available = True
            result.latency_ms = sum(latencies) / len(latencies)  # Moyenne
            result.quality_score = sum(quality_scores) / len(quality_scores)
            result.test_count = len(self.test_prompts)
            result.success_rate = successful_tests / len(self.test_prompts)
            result.response_text = responses[0] if responses else ""
        else:
            result.error = "Aucun test r√©ussi"
        
        return result
    
    async def validate_all_models(self) -> List[LLMTestResult]:
        """Validation de tous les mod√®les prioritaires"""
        logger.info("üöÄ D√âMARRAGE VALIDATION LLM SUPERWHISPER V6")
        
        # Validation GPU obligatoire
        validate_rtx3090_configuration()
        
        # R√©cup√©ration mod√®les disponibles
        available_models = await self.get_available_models()
        
        # Intersection mod√®les prioritaires et disponibles
        models_to_test = [model for model in self.priority_models if model in available_models]
        
        if not models_to_test:
            logger.error("‚ùå Aucun mod√®le prioritaire disponible")
            return []
        
        logger.info(f"üìã Validation {len(models_to_test)} mod√®les prioritaires")
        
        results = []
        for model in models_to_test:
            try:
                result = await self.validate_model(model)
                results.append(result)
            except Exception as e:
                logger.error(f"‚ùå Erreur validation {model}: {e}")
                error_result = LLMTestResult(model, self.base_url, error=str(e))
                results.append(error_result)
        
        return results

def select_best_model(results: List[LLMTestResult]) -> Optional[LLMTestResult]:
    """S√©lection du meilleur mod√®le selon crit√®res SuperWhisper V6"""
    
    # Filtrer mod√®les fonctionnels
    working_models = [r for r in results if r.available and r.success_rate >= 0.8]
    
    if not working_models:
        return None
    
    # Crit√®res de s√©lection (pond√©r√©s)
    def model_score(result: LLMTestResult) -> float:
        # Latence (50% du score) - objectif < 400ms
        latency_score = max(0, (400 - result.latency_ms) / 400) * 5.0
        
        # Qualit√© (30% du score)
        quality_score = (result.quality_score / 10.0) * 3.0
        
        # Fiabilit√© (20% du score)
        reliability_score = result.success_rate * 2.0
        
        return latency_score + quality_score + reliability_score
    
    # S√©lection meilleur score
    best_model = max(working_models, key=model_score)
    return best_model

def generate_validation_report(results: List[LLMTestResult], best_model: Optional[LLMTestResult]) -> str:
    """G√©n√©ration rapport validation LLM"""
    
    report = "="*70 + "\n"
    report += "ü§ñ RAPPORT VALIDATION LLM - SUPERWHISPER V6\n"
    report += "="*70 + "\n"
    
    # M√©triques globales
    working_count = len([r for r in results if r.available])
    total_count = len(results)
    
    report += f"üìä Mod√®les test√©s: {total_count}\n"
    report += f"‚úÖ Mod√®les fonctionnels: {working_count}\n"
    
    if best_model:
        report += f"üèÜ MOD√àLE S√âLECTIONN√â: {best_model.model_name}\n"
        report += f"‚ö° Latence moyenne: {best_model.latency_ms:.1f}ms\n"
        report += f"üéØ Objectif < 400ms: {'‚úÖ ATTEINT' if best_model.latency_ms < 400 else '‚ùå D√âPASS√â'}\n"
        report += f"üèÖ Qualit√©: {best_model.quality_score:.1f}/10\n"
        report += f"üìà Fiabilit√©: {best_model.success_rate*100:.1f}%\n"
    else:
        report += "‚ùå AUCUN MOD√àLE FONCTIONNEL\n"
    
    report += "\n" + "üìã D√âTAIL MOD√àLES:\n"
    for result in results:
        if result.available:
            status = f"‚úÖ {result.latency_ms:.1f}ms - Q:{result.quality_score:.1f}/10 - R:{result.success_rate*100:.0f}%"
        else:
            status = f"‚ùå {result.error or 'Non fonctionnel'}"
        
        report += f"   {result.model_name}: {status}\n"
    
    # Recommandations
    report += "\n" + "üéØ RECOMMANDATIONS:\n"
    if best_model and best_model.latency_ms < 400:
        report += f"‚úÖ Mod√®le {best_model.model_name} valid√© pour production\n"
        report += f"‚úÖ Latence {best_model.latency_ms:.1f}ms compatible objectif < 1.2s total\n"
        report += "üöÄ Pr√™t pour int√©gration pipeline complet\n"
    elif best_model:
        report += f"‚ö†Ô∏è Mod√®le {best_model.model_name} fonctionne mais latence √©lev√©e ({best_model.latency_ms:.1f}ms)\n"
        report += "üîß Optimisation n√©cessaire ou mod√®le plus petit requis\n"
    else:
        report += "‚ùå Aucun mod√®le utilisable - v√©rifier configuration Ollama\n"
    
    report += "="*70
    return report

async def main():
    """Validation LLM principale SuperWhisper V6"""
    
    try:
        validator = OllamaLLMValidator()
        results = await validator.validate_all_models()
        
        if not results:
            print("‚ùå Aucun r√©sultat de validation")
            return 1
        
        # S√©lection meilleur mod√®le
        best_model = select_best_model(results)
        
        # G√©n√©ration rapport
        report = generate_validation_report(results, best_model)
        print(report)
        
        # Sauvegarde rapport
        report_path = "docs/VALIDATION_LLM_REPORT.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# RAPPORT VALIDATION LLM - SUPERWHISPER V6\n\n")
            f.write(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"```\n{report}\n```\n")
        
        print(f"\nüìÅ Rapport sauvegard√©: {report_path}")
        
        # Validation humaine si mod√®le s√©lectionn√©
        if best_model:
            print(f"\nü§ñ VALIDATION HUMAINE REQUISE:")
            print(f"Mod√®le: {best_model.model_name}")
            print(f"Exemple r√©ponse: {best_model.response_text[:100]}...")
            print(f"\n‚ùì Le mod√®le {best_model.model_name} vous semble-t-il appropri√© pour la conversation ?")
            
            # Retourner code succ√®s si mod√®le < 400ms
            return 0 if best_model.latency_ms < 400 else 1
        else:
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Erreur validation: {e}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 