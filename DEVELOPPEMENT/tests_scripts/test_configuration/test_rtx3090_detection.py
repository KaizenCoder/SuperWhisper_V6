#!/usr/bin/env python3
"""
Test dÃ©tection GPU RTX 3090 - Configuration double GPU
ğŸš¨ CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

ğŸš¨ CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# ğŸš€ PORTABILITÃ‰ AUTOMATIQUE - EXÃ‰CUTABLE DEPUIS N'IMPORTE OÃ™
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour exÃ©cution portable"""
    # DÃ©terminer le rÃ©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le rÃ©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"ğŸ® GPU Configuration: RTX 3090 (CUDA:1) forcÃ©e")
    print(f"ğŸ“ Project Root: {project_root}")
    print(f"ğŸ’» Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import torch

def validate_rtx3090_mandatory():
    """Validation obligatoire de la configuration RTX 3090"""
    if not torch.cuda.is_available():
        raise RuntimeError("ğŸš« CUDA non disponible - RTX 3090 requise")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"ğŸš« CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit Ãªtre '1'")
    
    cuda_order = os.environ.get('CUDA_DEVICE_ORDER', '')
    if cuda_order != 'PCI_BUS_ID':
        raise RuntimeError(f"ğŸš« CUDA_DEVICE_ORDER='{cuda_order}' incorrect - doit Ãªtre 'PCI_BUS_ID'")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 = ~24GB
        raise RuntimeError(f"ğŸš« GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
    
    print(f"âœ… RTX 3090 validÃ©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")

def test_gpu_detection():
    """Test dÃ©tection GPU avec configuration RTX 3090"""
    print("ğŸ” TEST DÃ‰TECTION GPU RTX 3090")
    print("=" * 40)
    
    # Test PyTorch avec validation RTX 3090
    try:
        validate_rtx3090_mandatory()
        print(f"âœ… PyTorch version: {torch.__version__}")
        print(f"âœ… CUDA disponible: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)  # Device 0 visible = RTX 3090
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            compute_cap = torch.cuda.get_device_capability(0)
            
            print(f"ğŸ® GPU dÃ©tectÃ©: {gpu_name}")
            print(f"ğŸ’¾ VRAM: {gpu_memory:.1f}GB")
            print(f"ğŸ”§ Compute Capability: {compute_cap}")
            
            # VÃ©rifier si c'est RTX 3090
            is_rtx_3090 = "RTX 3090" in gpu_name and gpu_memory >= 20
            print(f"ğŸ† RTX 3090 dÃ©tectÃ©: {'âœ… OUI' if is_rtx_3090 else 'âŒ NON'}")
            
            if not is_rtx_3090:
                raise RuntimeError(f"GPU incorrecte dÃ©tectÃ©e: {gpu_name}")
            
            return is_rtx_3090
        else:
            print("âŒ CUDA non disponible")
            raise RuntimeError("CUDA non disponible")
            
    except ImportError as e:
        print(f"âŒ PyTorch non installÃ©: {e}")
        raise
    except Exception as e:
        print(f"âŒ Erreur PyTorch: {e}")
        raise

def test_faster_whisper():
    """Test faster-whisper avec RTX 3090"""
    print("\nğŸ¤ TEST FASTER-WHISPER RTX 3090")
    print("=" * 40)
    
    try:
        from faster_whisper import WhisperModel
        print("âœ… faster-whisper importÃ©")
        
        # Test initialisation GPU RTX 3090 - faster-whisper utilise "cuda" gÃ©nÃ©rique
        print("ğŸ”„ Test initialisation GPU RTX 3090...")
        model = WhisperModel("tiny", device="cuda", compute_type="int8")
        print("âœ… ModÃ¨le Whisper GPU RTX 3090 initialisÃ©")
        
        # Test transcription rapide
        import numpy as np
        dummy_audio = np.zeros(16000, dtype=np.float32)
        segments, info = model.transcribe(dummy_audio)
        segments_list = list(segments)  # Force exÃ©cution
        
        print("âœ… Test transcription rÃ©ussi")
        print(f"ğŸ“Š Langue dÃ©tectÃ©e: {info.language}")
        print(f"ğŸ”¢ Segments traitÃ©s: {len(segments_list)}")
        
        # Cleanup mÃ©moire
        del model
        torch.cuda.empty_cache()
        print("âœ… Nettoyage mÃ©moire RTX 3090 effectuÃ©")
        
        return True
        
    except ImportError as e:
        print(f"âŒ faster-whisper non installÃ©: {e}")
        print("   Installer avec: pip install faster-whisper")
        return False
    except Exception as e:
        print(f"âŒ Erreur faster-whisper: {e}")
        print(f"   DÃ©tail: {type(e).__name__}: {str(e)}")
        return False

def main():
    """Fonction principale de diagnostic RTX 3090"""
    print("ğŸš€ DIAGNOSTIC COMPLET RTX 3090")
    print("Configuration: RTX 3090 (CUDA:1) exclusive - CORRECTION CRITIQUE")
    print("âš ï¸  ANCIENNE CONFIG RTX 5060 Ti SUPPRIMÃ‰E - RTX 3090 exclusive maintenant")
    print()
    
    try:
        # Test 1: DÃ©tection GPU
        gpu_ok = test_gpu_detection()
        
        # Test 2: faster-whisper si GPU OK
        whisper_ok = False
        if gpu_ok:
            whisper_ok = test_faster_whisper()
        
        # RÃ©sumÃ©
        print("\nğŸ“‹ RÃ‰SUMÃ‰ DIAGNOSTIC RTX 3090")
        print("=" * 40)
        print(f"ğŸ® GPU RTX 3090: {'âœ… OK' if gpu_ok else 'âŒ Ã‰CHEC'}")
        print(f"ğŸ¤ faster-whisper: {'âœ… OK' if whisper_ok else 'âŒ Ã‰CHEC'}")
        
        if gpu_ok and whisper_ok:
            print("\nğŸ‰ CONFIGURATION RTX 3090 OPÃ‰RATIONNELLE")
            print("   SuperWhisper V6 peut maintenant fonctionner")
        else:
            print("\nğŸš¨ PROBLÃˆME CONFIGURATION RTX 3090")
            if not gpu_ok:
                print("   - VÃ©rifier installation CUDA/PyTorch")
            if gpu_ok and not whisper_ok:
                print("   - ProblÃ¨me faster-whisper ou CUDA libraries")
        
        return gpu_ok and whisper_ok
        
    except Exception as e:
        print(f"\nâŒ ERREUR CRITIQUE: {e}")
        return False

# APPELER DANS TOUS LES SCRIPTS PRINCIPAUX
if __name__ == "__main__":
    validate_rtx3090_mandatory()
    main() 