#!/usr/bin/env python3
"""
SOLUTION MEMORY LEAK GPU V2 - SuperWhisper V6 [VULN√âRABILIT√âS CORRIG√âES]
üö® CONFIGURATION: RTX 3090 CUDA:1 avec corrections critiques Claude + O3
"""

import os
import sys
import torch
import gc
import threading
import multiprocessing
import contextlib
import functools
import signal
from typing import Optional, Dict, Any
import time
import traceback
from pathlib import Path

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

# Seuil param√©trable memory leak (correction O3)
LEAK_THRESHOLD_MB = float(os.environ.get('LUXA_LEAK_THRESHOLD_MB', '100'))  # Default 100MB (Claude)
TIMEOUT_SECONDS = int(os.environ.get('LUXA_GPU_TIMEOUT_S', '300'))  # Default 5min

print("üöÄ SOLUTION MEMORY LEAK GPU V2 - CORRECTIONS CRITIQUES")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
print(f"‚öôÔ∏è Seuil Memory Leak: {LEAK_THRESHOLD_MB}MB")
print(f"‚è∞ Timeout GPU: {TIMEOUT_SECONDS}s")

# S√©maphore inter-processus pour queue GPU exclusive (correction O3)
LOCK_FILE = Path.cwd() / ".luxa_gpu_lock"
global_gpu_semaphore = None

def init_global_gpu_semaphore():
    """Initialiser le s√©maphore inter-processus GPU"""
    global global_gpu_semaphore
    if global_gpu_semaphore is None:
        global_gpu_semaphore = multiprocessing.Semaphore(1)
    return global_gpu_semaphore

class GPUMemoryManager:
    """Gestionnaire automatique des fuites m√©moire GPU RTX 3090 [CORRIG√â V2]"""
    
    def __init__(self):
        self.device = "cuda:0"  # RTX 3090 via CUDA_VISIBLE_DEVICES='1'
        self.initial_memory = None
        self.lock = threading.Lock()
        self.gpu_semaphore = init_global_gpu_semaphore()
        self._validate_gpu()
        self._initialize_baseline()  # Correction O3: apr√®s validation
    
    def _validate_gpu(self):
        """Validation critique RTX 3090"""
        if not torch.cuda.is_available():
            raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
        
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory < 20:  # RTX 3090 = ~24GB
            raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
        
        print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
    
    def _initialize_baseline(self):
        """Initialiser baseline m√©moire APR√àS cleanup initial (correction O3)"""
        with self.lock:
            # Cleanup initial obligatoire avant baseline
            self._force_cleanup_internal()
            self.initial_memory = torch.cuda.memory_allocated(0)
            print(f"üìè Baseline m√©moire: {self.initial_memory / 1024**3:.3f}GB")
    
    def get_memory_stats(self) -> Dict[str, float]:
        """Statistiques m√©moire GPU d√©taill√©es [AM√âLIOR√â V2]"""
        with self.lock:
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            max_allocated = torch.cuda.max_memory_allocated(0) / 1024**3
            max_reserved = torch.cuda.max_memory_reserved(0) / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            # Nouvelles m√©triques (correction Claude)
            reserved_vs_allocated_ratio = reserved / allocated if allocated > 0 else 0
            potential_fragmentation = reserved - allocated
            
            return {
                'allocated_gb': allocated,
                'reserved_gb': reserved,
                'max_allocated_gb': max_allocated,
                'max_reserved_gb': max_reserved,
                'total_gb': total,
                'free_gb': total - reserved,
                'utilization_pct': (reserved / total) * 100,
                'reserved_vs_allocated_ratio': reserved_vs_allocated_ratio,  # Nouveau
                'potential_fragmentation_gb': potential_fragmentation  # Nouveau
            }
    
    def _force_cleanup_internal(self):
        """Cleanup interne avec timeout (correction Claude + O3)"""
        try:
            # Timeout sur synchronize (correction Claude)
            def timeout_handler(signum, frame):
                raise TimeoutError("GPU synchronize timeout")
            
            if hasattr(signal, 'SIGALRM'):  # Unix seulement
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(30)  # 30s timeout
            
            # 1. Vider cache PyTorch
            torch.cuda.empty_cache()
            
            # 2. Garbage collection Python
            gc.collect()
            
            # 3. Synchronisation GPU avec timeout
            torch.cuda.synchronize()
            
            if hasattr(signal, 'SIGALRM'):
                signal.alarm(0)  # Cancel timeout
            
            # 4. Reset statistiques m√©moire [CORRIG√â PyTorch ‚â•2.3]
            torch.cuda.reset_max_memory_allocated()
            
            # Correction O3: remplacer reset_max_memory_cached obsol√®te
            if hasattr(torch.cuda, 'reset_peak_memory_stats'):
                torch.cuda.reset_peak_memory_stats()  # PyTorch ‚â•2.1
            elif hasattr(torch.cuda, 'reset_max_memory_cached'):
                torch.cuda.reset_max_memory_cached()  # PyTorch <2.3 fallback
            
            print("üßπ Cleanup GPU complet avec timeout")
            
        except TimeoutError:
            print(f"‚ö†Ô∏è Timeout cleanup GPU apr√®s 30s")
            if hasattr(signal, 'SIGALRM'):
                signal.alarm(0)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur cleanup GPU: {e}")
    
    def force_cleanup(self):
        """Cleanup forc√© public avec lock"""
        with self.lock:
            self._force_cleanup_internal()
    
    @contextlib.contextmanager
    def gpu_context(self, test_name: str = "unknown", timeout_s: Optional[int] = None):
        """Context manager avec cleanup automatique + timeout + queue GPU [CORRIG√â V2]"""
        timeout = timeout_s or TIMEOUT_SECONDS
        
        # Queue GPU inter-processus (correction O3)
        with self.gpu_semaphore:
            print(f"üîÑ D√©but test GPU: {test_name} (timeout: {timeout}s)")
            
            # Watchdog timeout (correction O3)
            def watchdog_timeout():
                print(f"üö® WATCHDOG: Test {test_name} timeout apr√®s {timeout}s - Emergency reset")
                self.emergency_gpu_reset()
            
            watchdog = threading.Timer(timeout, watchdog_timeout)
            watchdog.start()
            
            # Statistiques avant
            stats_before = self.get_memory_stats()
            start_time = time.time()
            
            try:
                with self.lock:  # Thread safety
                    yield self.device
                    
            except Exception as e:
                print(f"‚ùå Erreur test {test_name}: {e}")
                traceback.print_exc()
                raise
                
            finally:
                # Cancel watchdog
                watchdog.cancel()
                
                # Cleanup automatique apr√®s test
                self._force_cleanup_internal()
                
                # Statistiques apr√®s
                stats_after = self.get_memory_stats()
                duration = time.time() - start_time
                
                # Rapport test avec seuil param√©trable (correction O3)
                memory_diff = stats_after['allocated_gb'] - stats_before['allocated_gb']
                threshold_gb = LEAK_THRESHOLD_MB / 1024  # MB -> GB
                
                print(f"üìä Test {test_name} termin√© ({duration:.2f}s)")
                print(f"   M√©moire avant: {stats_before['allocated_gb']:.3f}GB")
                print(f"   M√©moire apr√®s: {stats_after['allocated_gb']:.3f}GB")
                print(f"   Diff√©rence: {memory_diff:+.3f}GB (seuil: {threshold_gb:.3f}GB)")
                print(f"   Fragmentation: {stats_after['potential_fragmentation_gb']:.3f}GB")
                
                if abs(memory_diff) > threshold_gb:
                    print(f"üö® MEMORY LEAK D√âTECT√â: {memory_diff:+.3f}GB > {threshold_gb:.3f}GB")
                else:
                    print("‚úÖ Pas de memory leak d√©tect√©")

# =============================================================================
# D√âCORATEURS POUR TESTS GPU AUTOMATIQUES [CORRIG√âS V2]
# =============================================================================

gpu_manager = GPUMemoryManager()

def gpu_test_cleanup(test_name: Optional[str] = None, timeout_s: Optional[int] = None):
    """D√©corateur pour cleanup automatique tests GPU avec timeout"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            name = test_name or func.__name__
            with gpu_manager.gpu_context(name, timeout_s):
                return func(*args, **kwargs)
        return wrapper
    return decorator

def gpu_memory_monitor(threshold_gb: float = 1.0):
    """D√©corateur pour monitoring m√©moire GPU avec m√©triques √©tendues"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            stats_before = gpu_manager.get_memory_stats()
            
            result = func(*args, **kwargs)
            
            stats_after = gpu_manager.get_memory_stats()
            memory_used = stats_after['allocated_gb'] - stats_before['allocated_gb']
            fragmentation = stats_after['potential_fragmentation_gb']
            
            if memory_used > threshold_gb:
                print(f"üö® ALERTE M√âMOIRE: {func.__name__} utilise {memory_used:.2f}GB")
                print(f"   Seuil: {threshold_gb}GB | Utilisation: {stats_after['utilization_pct']:.1f}%")
                print(f"   Fragmentation: {fragmentation:.3f}GB")
            
            return result
        return wrapper
    return decorator

# =============================================================================
# FONCTIONS UTILITAIRES MEMORY LEAK PREVENTION [CORRIG√âES V2]
# =============================================================================

def validate_no_memory_leak():
    """Validation qu'aucun memory leak n'existe [SEUIL HARMONIS√â]"""
    if gpu_manager.initial_memory is None:
        print("‚ö†Ô∏è Baseline m√©moire non initialis√©e")
        return False
        
    initial = gpu_manager.initial_memory / 1024**3
    current = torch.cuda.memory_allocated(0) / 1024**3
    diff = current - initial
    threshold_gb = LEAK_THRESHOLD_MB / 1024  # Seuil param√©trable (correction O3)
    
    print(f"üîç VALIDATION MEMORY LEAK")
    print(f"   M√©moire baseline: {initial:.3f}GB")
    print(f"   M√©moire actuelle: {current:.3f}GB")
    print(f"   Diff√©rence: {diff:+.3f}GB (seuil: {threshold_gb:.3f}GB)")
    
    if abs(diff) > threshold_gb:  # Seuil harmonis√© (correction Claude)
        print(f"‚ùå MEMORY LEAK D√âTECT√â: {diff:+.3f}GB > {threshold_gb:.3f}GB")
        return False
    else:
        print("‚úÖ Aucun memory leak d√©tect√©")
        return True

def emergency_gpu_reset():
    """Reset GPU d'urgence en cas de memory leak critique [AM√âLIOR√â V2]"""
    print("üö® RESET GPU D'URGENCE")
    
    try:
        # Forcer cleanup complet
        gpu_manager._force_cleanup_internal()
        
        # R√©initialiser contexte CUDA si possible
        if hasattr(torch.cuda, 'reset_accumulated_memory_stats'):
            torch.cuda.reset_accumulated_memory_stats()
        
        # R√©initialiser baseline apr√®s reset
        gpu_manager._initialize_baseline()
        
        print("‚úÖ Reset GPU d'urgence termin√© avec nouvelle baseline")
        
    except Exception as e:
        print(f"‚ùå √âchec reset GPU: {e}")
        print("üí° Solution: Red√©marrer le script complet")

# =============================================================================
# EXEMPLE D'UTILISATION POUR TESTS PARALL√âLISATION [VALID√â V2]
# =============================================================================

@gpu_test_cleanup("test_model_loading", timeout_s=120)
@gpu_memory_monitor(threshold_gb=2.0)
def test_load_model_with_cleanup():
    """Exemple test avec cleanup automatique et timeout"""
    device = gpu_manager.device
    
    # Simulation chargement mod√®le plus r√©aliste
    model = torch.randn(2000, 2000, device=device, dtype=torch.float32)
    for _ in range(3):  # Plusieurs op√©rations
        result = torch.matmul(model, model.t())
        intermediate = torch.relu(result)
    
    print(f"‚úÖ Test mod√®le termin√© sur {device}")
    return intermediate.cpu()  # Retourner sur CPU pour lib√©rer GPU

def run_parallel_tests_with_cleanup():
    """Exemple ex√©cution tests parall√®les avec cleanup [V2 S√âCURIS√â]"""
    print("üîÑ TESTS PARALL√àLES V2 - CORRECTIONS CRITIQUES APPLIQU√âES")
    
    for i in range(5):
        print(f"\n--- Test {i+1}/5 ---")
        
        try:
            result = test_load_model_with_cleanup()
            print(f"   R√©sultat shape: {result.shape}")
            
            # Validation apr√®s chaque test
            validate_no_memory_leak()
            
        except Exception as e:
            print(f"‚ùå √âchec test {i+1}: {e}")
            emergency_gpu_reset()
    
    print("\nüèÜ TOUS LES TESTS TERMIN√âS")
    
    # Validation finale
    print("\nüìä RAPPORT FINAL:")
    final_stats = gpu_manager.get_memory_stats()
    for key, value in final_stats.items():
        if 'gb' in key or 'pct' in key or 'ratio' in key:
            print(f"   {key}: {value:.3f}")
    
    if validate_no_memory_leak():
        print("‚úÖ AUCUN MEMORY LEAK - PARALL√âLISATION V2 VALID√âE")
        return True
    else:
        print("‚ùå MEMORY LEAK PERSISTANT - INVESTIGATION REQUISE")
        emergency_gpu_reset()
        return False

if __name__ == "__main__":
    print("üß™ D√âMONSTRATION SOLUTION MEMORY LEAK V2 - CORRECTIONS APPLIQU√âES")
    
    # Test initial avec nouvelles m√©triques
    print(f"\nüìä Statistiques GPU initiales V2:")
    stats = gpu_manager.get_memory_stats()
    for key, value in stats.items():
        print(f"   {key}: {value:.3f}")
    
    # Tests avec toutes les corrections
    success = run_parallel_tests_with_cleanup()
    
    if success:
        print("\nüéØ SOLUTION MEMORY LEAK V2 VALID√âE - PR√äTE POUR 40 FICHIERS PARALL√àLES")
        print("‚úÖ Toutes vuln√©rabilit√©s critiques corrig√©es (Claude + O3)")
    else:
        print("\n‚ö†Ô∏è Tests partiels - V√©rification manuelle requise") 