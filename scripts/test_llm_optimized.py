#!/usr/bin/env python3
"""
üöÄ TEST LLM OPTIMIS√â - SUPERWHISPER V6
====================================
Test LLM avec optimisations pour r√©duire latence < 400ms

OPTIMISATIONS:
- R√©ponses courtes (max 30 tokens)
- Temperature basse (0.3)
- Mod√®le le plus rapide (qwen2.5-coder:1.5b)
- Syst√®me prompt optimis√©

Usage: python scripts/test_llm_optimized.py

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import asyncio
import time
import logging
import httpx

# Configuration GPU RTX 3090
os.environ['CUDA_VISIBLE_DEVICES'] = '1'

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("llm_optimized")

class OptimizedLLMTester:
    """Testeur LLM optimis√© pour latence minimale"""
    
    def __init__(self):
        self.base_url = "http://localhost:11434"
        self.timeout = 15
        self.best_model = "qwen2.5-coder:1.5b"  # Mod√®le le plus rapide identifi√©
        
        # Prompts optimis√©s pour r√©ponses courtes
        self.test_prompts = [
            "Bonjour !",
            "M√©t√©o ?", 
            "Heure ?",
            "Merci !",
            "Au revoir"
        ]
    
    async def test_optimized_inference(self, prompt: str) -> dict:
        """Test inf√©rence optimis√©e"""
        start_time = time.time()
        
        try:
            # Configuration optimis√©e pour vitesse
            payload = {
                'model': self.best_model,
                'prompt': f"R√©ponds en 1-3 mots maximum en fran√ßais: {prompt}",
                'stream': False,
                'options': {
                    'temperature': 0.1,    # Tr√®s d√©terministe 
                    'num_predict': 15,     # Maximum 15 tokens
                    'top_p': 0.8,         # Focus sur tokens probables
                    'top_k': 20,          # Limitation vocabulaire
                    'repeat_penalty': 1.1,# √âviter r√©p√©titions
                    'stop': ['.', '!', '?', '\n']  # Arr√™t pr√©coce
                }
            }
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json=payload
                )
                response.raise_for_status()
                
                data = response.json()
                response_text = data.get('response', '').strip()
                latency_ms = (time.time() - start_time) * 1000
                
                return {
                    'success': True,
                    'latency_ms': latency_ms,
                    'response_text': response_text,
                    'tokens': len(response_text.split())
                }
                
        except Exception as e:
            latency_ms = (time.time() - start_time) * 1000
            return {
                'success': False,
                'latency_ms': latency_ms,
                'error': str(e)
            }
    
    async def run_optimization_test(self):
        """Test optimisation compl√®te"""
        logger.info(f"üöÄ TEST OPTIMISATION LLM: {self.best_model}")
        logger.info("üéØ Objectif: R√©duire latence via r√©ponses courtes")
        
        latencies = []
        responses = []
        
        for i, prompt in enumerate(self.test_prompts):
            logger.info(f"Test {i+1}/5: '{prompt}'")
            
            result = await self.test_optimized_inference(prompt)
            
            if result['success']:
                latencies.append(result['latency_ms'])
                responses.append(result['response_text'])
                
                logger.info(f"   ‚úÖ {result['latency_ms']:.1f}ms - '{result['response_text']}' ({result['tokens']} tokens)")
            else:
                logger.warning(f"   ‚ùå √âchec: {result.get('error', 'Erreur inconnue')}")
        
        # Analyse r√©sultats
        if latencies:
            avg_latency = sum(latencies) / len(latencies)
            min_latency = min(latencies)
            max_latency = max(latencies)
            
            print("\n" + "="*60)
            print("üéØ R√âSULTATS OPTIMISATION LLM")
            print("="*60)
            print(f"‚ö° Latence moyenne: {avg_latency:.1f}ms")
            print(f"üöÄ Latence minimum: {min_latency:.1f}ms")
            print(f"‚è±Ô∏è Latence maximum: {max_latency:.1f}ms")
            print(f"üéØ Objectif < 400ms: {'‚úÖ ATTEINT' if avg_latency < 400 else '‚ùå D√âPASS√â'}")
            
            # Projection pipeline complet
            stt_latency = 833  # Valid√©
            tts_latency = 975  # Valid√©
            total_pipeline = stt_latency + avg_latency + tts_latency
            
            print(f"\nüìä PROJECTION PIPELINE COMPLET:")
            print(f"   STT: {stt_latency}ms")
            print(f"   LLM: {avg_latency:.1f}ms (optimis√©)")
            print(f"   TTS: {tts_latency}ms")
            print(f"   TOTAL: {total_pipeline:.1f}ms")
            print(f"   Objectif < 1200ms: {'‚úÖ ATTEINT' if total_pipeline < 1200 else '‚ùå D√âPASS√â'}")
            
            if total_pipeline < 2500:
                print(f"   Objectif < 2500ms: ‚úÖ R√âALISTE")
            
            print("="*60)
            
            # Exemples r√©ponses
            if responses:
                print(f"\nüí¨ EXEMPLES R√âPONSES OPTIMIS√âES:")
                for prompt, response in zip(self.test_prompts, responses):
                    print(f"   '{prompt}' ‚Üí '{response}'")
            
            return avg_latency < 400
        
        return False

async def main():
    """Test principal optimisation LLM"""
    try:
        tester = OptimizedLLMTester()
        success = await tester.run_optimization_test()
        
        if success:
            print("\nüéä OPTIMISATION R√âUSSIE - Mod√®le pr√™t pour pipeline")
            return 0
        else:
            print("\n‚ö†Ô∏è OPTIMISATION LIMIT√âE - Objectifs √† r√©viser")
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Erreur test optimisation: {e}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 