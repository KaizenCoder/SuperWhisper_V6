#!/usr/bin/env python3
"""
üèÜ TEST STABILIT√â 30MIN RTX 3090 - SUPERWHISPER V6
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

Test de stabilit√© prolong√©e (30min simul√© en 2min) avec Memory Leak V4
Phase 4.4 - Tests Stabilit√© 30min
"""

import os
import sys
import time
import json
import threading
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
# RTX 5060 Ti (CUDA:0) = INTERDITE - RTX 3090 (CUDA:1) = OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Maintenant imports normaux...
import torch


def validate_rtx3090_mandatory():
    """Validation obligatoire de la configuration RTX 3090"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
    
    # RTX 3090 = ~24GB
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:
        raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
    
    print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")


class StabilityTestSuite:
    """Suite de tests de stabilit√© prolong√©e"""
    
    def __init__(self, duration_minutes: int = 30, accelerated: bool = True):
        self.original_duration = duration_minutes
        self.actual_duration = 2.0 if accelerated else duration_minutes  # 2min en mode acc√©l√©r√©
        self.accelerated = accelerated
        self.running = False
        
        self.results = {
            "start_time": datetime.now().isoformat(),
            "test_config": {
                "original_duration_min": self.original_duration,
                "actual_duration_min": self.actual_duration,
                "accelerated_mode": self.accelerated,
                "CUDA_VISIBLE_DEVICES": os.environ.get('CUDA_VISIBLE_DEVICES')
            },
            "stability_metrics": {
                "memory_snapshots": [],
                "performance_snapshots": [],
                "error_count": 0,
                "warnings_count": 0
            }
        }
        
        # Initialisation Memory Leak V4
        sys.path.append(str(Path.cwd()))
        import memory_leak_v4
        self.gpu_manager = memory_leak_v4.GPUMemoryManager(enable_json_logging=True)
        
    def log_snapshot(self, snapshot_type: str, data: Dict[str, Any]):
        """Enregistre un snapshot de stabilit√©"""
        snapshot = {
            "timestamp": datetime.now().isoformat(),
            "elapsed_seconds": (datetime.now() - datetime.fromisoformat(self.results["start_time"])).total_seconds(),
            "data": data
        }
        self.results["stability_metrics"][f"{snapshot_type}_snapshots"].append(snapshot)
        
    def simulate_continuous_workload(self):
        """Simule une charge de travail continue SuperWhisper V6"""
        print("üîÑ D√©marrage simulation charge continue...")
        
        cycle_count = 0
        start_time = datetime.now()
        duration_seconds = self.actual_duration * 60
        
        while self.running and (datetime.now() - start_time).total_seconds() < duration_seconds:
            cycle_count += 1
            
            try:
                with self.gpu_manager.gpu_context(f"stability_cycle_{cycle_count}") as ctx:
                    # Simulation STT (Speech-to-Text)
                    audio_tensor = torch.randn(16000 * 5, device="cuda:0")  # 5 sec audio
                    time.sleep(0.1 if self.accelerated else 1.0)
                    
                    # Simulation LLM (Large Language Model)
                    llm_tensor = torch.randint(0, 50000, (1, 512), device="cuda:0")  # 512 tokens
                    time.sleep(0.15 if self.accelerated else 2.0)
                    
                    # Simulation TTS (Text-to-Speech)
                    tts_tensor = torch.randn(80, 800, device="cuda:0")  # Mel spectrogram
                    time.sleep(0.1 if self.accelerated else 1.5)
                    
                    # Cleanup automatique via context manager
                    del audio_tensor, llm_tensor, tts_tensor
                    torch.cuda.empty_cache()
                    
                    # Snapshot m√©moire p√©riodique
                    if cycle_count % 10 == 0:
                        memory_stats = self.gpu_manager.get_memory_stats()
                        self.log_snapshot("memory", {
                            "cycle": cycle_count,
                            "allocated_gb": memory_stats.get("allocated_gb", 0),
                            "reserved_gb": memory_stats.get("reserved_gb", 0),
                            "fragmentation_gb": memory_stats.get("fragmentation_gb", 0)
                        })
                        
                        # Affichage p√©riodique
                        elapsed = (datetime.now() - start_time).total_seconds()
                        progress = (elapsed / duration_seconds) * 100
                        print(f"   üìä Cycle {cycle_count} - {progress:.1f}% - M√©moire: {memory_stats.get('allocated_gb', 0):.3f}GB")
                    
            except Exception as e:
                self.results["stability_metrics"]["error_count"] += 1
                print(f"   ‚ö†Ô∏è  Erreur cycle {cycle_count}: {e}")
                
            # Pause entre cycles (acc√©l√©r√©e)
            time.sleep(0.05 if self.accelerated else 0.5)
        
        print(f"‚úÖ Simulation termin√©e - {cycle_count} cycles compl√©t√©s")
        return cycle_count
    
    def monitor_performance(self):
        """Monitore les performances en arri√®re-plan"""
        print("üìà D√©marrage monitoring performance...")
        
        start_time = datetime.now()
        duration_seconds = self.actual_duration * 60
        snapshot_interval = 10 if self.accelerated else 60  # Snapshot chaque 10s en mode acc√©l√©r√©
        
        while self.running and (datetime.now() - start_time).total_seconds() < duration_seconds:
            try:
                # Test performance ponctuel
                perf_start = time.perf_counter()
                test_tensor = torch.randn(2000, 2000, device="cuda:0")
                result = torch.matmul(test_tensor, test_tensor)
                torch.cuda.synchronize()
                perf_time = time.perf_counter() - perf_start
                
                del test_tensor, result
                torch.cuda.empty_cache()
                
                # Snapshot performance
                elapsed = (datetime.now() - start_time).total_seconds()
                self.log_snapshot("performance", {
                    "elapsed_seconds": elapsed,
                    "matmul_time_ms": perf_time * 1000,
                    "estimated_gflops": (2 * 2000**3) / (perf_time * 1e9)
                })
                
            except Exception as e:
                self.results["stability_metrics"]["warnings_count"] += 1
                print(f"   ‚ö†Ô∏è  Warning performance: {e}")
            
            time.sleep(snapshot_interval)
        
        print("üìà Monitoring performance termin√©")
    
    def run_stability_test(self):
        """Ex√©cute le test de stabilit√© complet"""
        print("=" * 80)
        print("üèÜ TEST STABILIT√â 30MIN RTX 3090 - SUPERWHISPER V6")
        print("=" * 80)
        print(f"üìÖ D√©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"‚è±Ô∏è  Dur√©e: {self.original_duration}min {'(acc√©l√©r√© 2min)' if self.accelerated else ''}")
        print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
        print("=" * 80)
        
        self.running = True
        
        # D√©marrage monitoring en arri√®re-plan
        performance_thread = threading.Thread(target=self.monitor_performance)
        performance_thread.daemon = True
        performance_thread.start()
        
        # Test principal de charge continue
        cycles_completed = self.simulate_continuous_workload()
        
        # Arr√™t monitoring
        self.running = False
        performance_thread.join(timeout=5)
        
        # Analyse finale
        self.analyze_stability_results(cycles_completed)
        
        return True
    
    def analyze_stability_results(self, cycles_completed: int):
        """Analyse les r√©sultats de stabilit√©"""
        print("\n" + "=" * 80)
        print("üìä ANALYSE STABILIT√â RTX 3090")
        print("=" * 80)
        
        # Finalisation r√©sultats
        self.results["end_time"] = datetime.now().isoformat()
        self.results["test_summary"] = {
            "cycles_completed": cycles_completed,
            "total_errors": self.results["stability_metrics"]["error_count"],
            "total_warnings": self.results["stability_metrics"]["warnings_count"],
            "memory_snapshots_count": len(self.results["stability_metrics"]["memory_snapshots"]),
            "performance_snapshots_count": len(self.results["stability_metrics"]["performance_snapshots"])
        }
        
        # Analyse m√©moire
        memory_snapshots = self.results["stability_metrics"]["memory_snapshots"]
        if memory_snapshots:
            memory_values = [s["data"]["allocated_gb"] for s in memory_snapshots]
            memory_analysis = {
                "min_memory_gb": min(memory_values),
                "max_memory_gb": max(memory_values),
                "avg_memory_gb": sum(memory_values) / len(memory_values),
                "memory_stable": max(memory_values) - min(memory_values) < 0.5  # Variation < 500MB
            }
            self.results["memory_analysis"] = memory_analysis
            
            print(f"üíæ M√©moire GPU:")
            print(f"   üìä Min: {memory_analysis['min_memory_gb']:.3f}GB")
            print(f"   üìä Max: {memory_analysis['max_memory_gb']:.3f}GB") 
            print(f"   üìä Moyenne: {memory_analysis['avg_memory_gb']:.3f}GB")
            print(f"   üìä Stabilit√©: {'‚úÖ Stable' if memory_analysis['memory_stable'] else '‚ö†Ô∏è Instable'}")
        
        # Analyse performance
        perf_snapshots = self.results["stability_metrics"]["performance_snapshots"]
        if perf_snapshots:
            gflops_values = [s["data"]["estimated_gflops"] for s in perf_snapshots]
            performance_analysis = {
                "min_gflops": min(gflops_values),
                "max_gflops": max(gflops_values),
                "avg_gflops": sum(gflops_values) / len(gflops_values),
                "performance_stable": (max(gflops_values) - min(gflops_values)) / max(gflops_values) < 0.1  # Variation < 10%
            }
            self.results["performance_analysis"] = performance_analysis
            
            print(f"‚ö° Performance GPU:")
            print(f"   üìä Min: {performance_analysis['min_gflops']:.1f} GFLOPS")
            print(f"   üìä Max: {performance_analysis['max_gflops']:.1f} GFLOPS")
            print(f"   üìä Moyenne: {performance_analysis['avg_gflops']:.1f} GFLOPS")
            print(f"   üìä Stabilit√©: {'‚úÖ Stable' if performance_analysis['performance_stable'] else '‚ö†Ô∏è Instable'}")
        
        # Verdict final
        stability_score = 100
        if self.results["test_summary"]["total_errors"] > 0:
            stability_score -= self.results["test_summary"]["total_errors"] * 10
        if self.results["test_summary"]["total_warnings"] > 5:
            stability_score -= (self.results["test_summary"]["total_warnings"] - 5) * 5
        if memory_snapshots and not self.results["memory_analysis"]["memory_stable"]:
            stability_score -= 20
        if perf_snapshots and not self.results["performance_analysis"]["performance_stable"]:
            stability_score -= 15
            
        self.results["stability_score"] = max(0, stability_score)
        
        print(f"\nüèÜ VERDICT STABILIT√â:")
        print(f"   üìà Score: {stability_score}/100")
        print(f"   üîÑ Cycles: {cycles_completed}")
        print(f"   ‚ùå Erreurs: {self.results['test_summary']['total_errors']}")
        print(f"   ‚ö†Ô∏è  Warnings: {self.results['test_summary']['total_warnings']}")
        
        if stability_score >= 90:
            print("   üéâ EXCELLENTE stabilit√© RTX 3090 !")
        elif stability_score >= 70:
            print("   ‚úÖ Bonne stabilit√© RTX 3090")
        else:
            print("   ‚ö†Ô∏è  Stabilit√© perfectible")
        
        # Export rapport JSON
        report_file = "stability_test_report_rtx3090.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"üíæ Rapport stabilit√© export√©: {report_file}")


def main():
    """Point d'entr√©e principal"""
    try:
        # Validation GPU obligatoire
        validate_rtx3090_mandatory()
        
        # Ex√©cution test stabilit√©
        stability_suite = StabilityTestSuite(duration_minutes=30, accelerated=True)
        success = stability_suite.run_stability_test()
        
        if success:
            print("\nüéâ TEST STABILIT√â 30MIN TERMIN√â AVEC SUCC√àS !")
            return 0
        else:
            print("\n‚ö†Ô∏è  PROBL√àMES LORS DU TEST STABILIT√â")
            return 1
            
    except Exception as e:
        print(f"\nüö´ ERREUR CRITIQUE STABILIT√â: {e}")
        return 2


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 