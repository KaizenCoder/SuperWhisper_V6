#!/usr/bin/env python3
"""
UnifiedSTTManager - SuperWhisper V6 Phase 4
ðŸš¨ CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
from typing import Dict, Any, Optional, List
import asyncio
import time
import hashlib
import numpy as np
import torch
from contextlib import asynccontextmanager
from dataclasses import dataclass
from collections import OrderedDict

# =============================================================================
# ðŸš¨ CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation mÃ©moire

print("ðŸŽ® UnifiedSTTManager - Configuration GPU RTX 3090 (CUDA:1) forcÃ©e")
print(f"ðŸ”’ CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Validation GPU obligatoire
def validate_rtx3090_mandatory():
    """Validation systÃ©matique RTX 3090 pour STT"""
    if not torch.cuda.is_available():
        raise RuntimeError("ðŸš« CUDA non disponible - RTX 3090 requise pour STT")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"ðŸš« CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit Ãªtre '1'")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 = ~24GB
        raise RuntimeError(f"ðŸš« GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
    
    gpu_name = torch.cuda.get_device_name(0)
    print(f"âœ… RTX 3090 validÃ©e pour STT: {gpu_name} ({gpu_memory:.1f}GB)")

# Import des backends aprÃ¨s configuration GPU
try:
    from STT.backends.prism_stt_backend import PrismSTTBackend, STTResult
    from prometheus_client import Counter, Histogram, Gauge
except ImportError as e:
    print(f"âš ï¸ Import optionnel manquant: {e}")

@dataclass
class STTResult:
    """RÃ©sultat de transcription STT"""
    text: str
    confidence: float
    segments: List[dict]
    processing_time: float
    device: str
    rtf: float
    backend_used: str
    success: bool
    cached: bool = False
    error: Optional[str] = None

class STTCache:
    """Cache LRU pour rÃ©sultats STT avec TTL"""
    
    def __init__(self, max_size: int = 200*1024*1024, ttl: int = 7200):
        """
        Initialise le cache LRU.
        
        Args:
            max_size: Taille maximale en bytes (dÃ©faut: 200MB)
            ttl: DurÃ©e de vie des entrÃ©es en secondes (dÃ©faut: 2h)
        """
        self.max_size = max_size
        self.ttl = ttl
        self.cache = OrderedDict()  # {key: (value, timestamp, size)}
        self.current_size = 0
        self.hits = 0
        self.misses = 0
    
    def get(self, key: str) -> Optional[STTResult]:
        """RÃ©cupÃ¨re une valeur du cache avec gestion TTL"""
        if key in self.cache:
            value, timestamp, _ = self.cache[key]
            
            # VÃ©rifier TTL
            if time.time() - timestamp > self.ttl:
                self._remove(key)
                self.misses += 1
                return None
            
            # Hit - dÃ©placer en fin de LRU
            self.cache.move_to_end(key)
            self.hits += 1
            return value
        
        self.misses += 1
        return None
    
    def put(self, key: str, value: STTResult):
        """Ajoute une valeur au cache avec Ã©viction LRU si nÃ©cessaire"""
        # Estimer taille (approximation avec sÃ©rialisation)
        estimated_size = len(str(value).encode('utf-8'))
        
        # VÃ©rifier si la valeur peut rentrer
        if estimated_size > self.max_size:
            return  # Trop grande pour le cache
        
        # Ã‰viction LRU si nÃ©cessaire
        while self.current_size + estimated_size > self.max_size and self.cache:
            self._remove_lru()
        
        # Ajouter nouvelle entrÃ©e
        self.cache[key] = (value, time.time(), estimated_size)
        self.current_size += estimated_size
        self.cache.move_to_end(key)  # DÃ©placer en fin de LRU
    
    def _remove(self, key: str):
        """Supprime une entrÃ©e du cache"""
        if key in self.cache:
            _, _, size = self.cache[key]
            self.current_size -= size
            del self.cache[key]
    
    def _remove_lru(self):
        """Supprime l'entrÃ©e la moins rÃ©cemment utilisÃ©e"""
        if self.cache:
            key = next(iter(self.cache))  # Premier Ã©lÃ©ment (LRU)
            self._remove(key)

class CircuitBreaker:
    """Protection contre les Ã©checs en cascade"""
    
    def __init__(self, failure_threshold: int = 3, recovery_timeout: int = 30):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failures = 0
        self.last_failure_time = 0
        self.state = "closed"  # closed, open, half-open
    
    def record_failure(self):
        """Enregistre un Ã©chec"""
        self.failures += 1
        self.last_failure_time = time.time()
        
        if self.failures >= self.failure_threshold:
            self.state = "open"
    
    def record_success(self):
        """Enregistre un succÃ¨s"""
        self.failures = 0
        self.state = "closed"
    
    def is_open(self) -> bool:
        """VÃ©rifie si le circuit est ouvert"""
        if self.state == "open":
            # VÃ©rifier si on peut passer en half-open
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = "half-open"
                return False
            return True
        return False

class PrometheusSTTMetrics:
    """MÃ©triques Prometheus pour STT"""
    
    def __init__(self):
        try:
            self.transcriptions_total = Counter('stt_transcriptions_total', 'Total STT transcriptions', ['backend', 'status'])
            self.transcription_latency = Histogram('stt_transcription_latency_seconds', 'STT transcription latency', ['backend'])
            self.cache_hits = Counter('stt_cache_hits_total', 'STT cache hits')
            self.cache_misses = Counter('stt_cache_misses_total', 'STT cache misses')
            self.circuit_breaker_skips = Counter('stt_circuit_breaker_skips_total', 'Circuit breaker skips', ['backend'])
            self.total_failures = Counter('stt_total_failures_total', 'Total STT failures')
            self.transcriptions_success = Counter('stt_transcriptions_success_total', 'Successful transcriptions', ['backend'])
            self.transcriptions_failed = Counter('stt_transcriptions_failed_total', 'Failed transcriptions', ['backend'])
        except ImportError:
            # MÃ©triques factices si Prometheus non disponible
            self.transcriptions_total = self._dummy_metric()
            self.transcription_latency = self._dummy_metric()
            self.cache_hits = self._dummy_metric()
            self.cache_misses = self._dummy_metric()
            self.circuit_breaker_skips = self._dummy_metric()
            self.total_failures = self._dummy_metric()
            self.transcriptions_success = self._dummy_metric()
            self.transcriptions_failed = self._dummy_metric()
    
    def _dummy_metric(self):
        """MÃ©trique factice pour tests"""
        class DummyMetric:
            def inc(self): pass
            def observe(self, value): pass
            def set(self, value): pass
            def labels(self, **kwargs): return self
        return DummyMetric()

class UnifiedSTTManager:
    """Manager STT unifiÃ© avec fallback, cache, et circuit breaker"""

    _instance = None
    _lock = asyncio.Lock()

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(UnifiedSTTManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        if hasattr(self, '_initialized') and self._initialized:
            return
            
        validate_rtx3090_mandatory()
        print("âœ… UnifiedSTTManager initialisÃ© sur RTX 3090")

        self.config = config or {}
        self.backends: Dict[str, PrismSTTBackend] = {}
        self.circuit_breakers: Dict[str, CircuitBreaker] = {}
        self.cache = STTCache()
        self.metrics = PrometheusSTTMetrics()
        self.forced_backend: Optional[str] = None
        self.fallback_chain: List[str] = self.config.get('fallback_chain', [])

        self._initialize_backends()
        self._initialized = True

    def _initialize_backends(self):
        """Initialise les backends STT Ã  partir de la configuration."""
        backend_configs = self.config.get('backends', [])
        
        # PrÃ©-charger les modÃ¨les dans le pool pour un contrÃ´le centralisÃ©
        from STT.model_pool import model_pool
        print("Pre-loading models into the pool...")
        for backend_config in backend_configs:
            if backend_config['type'] == 'prism':
                model_size = backend_config.get('model', 'large-v2')
                model_pool.get_model(model_size) # Charge si non prÃ©sent
        print("âœ… All models pre-loaded or verified in the pool.")

        for backend_config in backend_configs:
            backend_name = backend_config['name']
            if backend_name not in self.backends:
                if backend_config['type'] == 'prism':
                    self.backends[backend_name] = PrismSTTBackend(backend_config)
                    self.circuit_breakers[backend_name] = CircuitBreaker()
        print(f"âœ… Backends STT initialisÃ©s: {list(self.backends.keys())}")

    def _generate_cache_key(self, audio: np.ndarray) -> str:
        """GÃ©nÃ¨re une clÃ© de cache unique pour un np.array audio."""
        # Hash MD5 des donnÃ©es audio
        audio_hash = hashlib.md5(audio.tobytes()).hexdigest()
        return f"stt_{audio_hash}_{len(audio)}"
    
    @asynccontextmanager
    async def _memory_management_context(self):
        """Contexte pour gestion mÃ©moire GPU."""
        try:
            # Nettoyage mÃ©moire avant traitement
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            yield
        finally:
            # Nettoyage mÃ©moire aprÃ¨s traitement
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    async def transcribe(self, audio: np.ndarray) -> STTResult:
        """
        Transcrit l'audio en texte avec gestion de cache et fallback.
        
        Args:
            audio: DonnÃ©es audio numpy (16kHz, mono, float32)
            
        Returns:
            RÃ©sultat de la transcription avec mÃ©triques
            
        Raises:
            Exception: Si tous les backends Ã©chouent
        """
        start_time = time.time()
        
        # VÃ©rification cache
        cache_key = self._generate_cache_key(audio)
        if cached_result := self.cache.get(cache_key):
            self.metrics.cache_hits.inc()
            cached_result.cached = True
            return cached_result
        
        self.metrics.cache_misses.inc()
        
        # Calcul timeout dynamique (5s par minute d'audio)
        audio_duration = len(audio) / 16000  # secondes
        timeout = max(5.0, audio_duration * self.config['timeout_per_minute'])
        
        async with self._memory_management_context():
            # Tentative avec chaque backend dans l'ordre
            for backend_name in self.fallback_chain:
                if self.circuit_breakers[backend_name].is_open():
                    self.metrics.circuit_breaker_skips.labels(backend=backend_name).inc()
                    continue
                
                backend = self.backends.get(backend_name)
                if backend is None:
                    continue
                
                try:
                    # Transcription avec timeout
                    result = await asyncio.wait_for(
                        backend.transcribe(audio),
                        timeout=timeout
                    )
                    
                    # SuccÃ¨s - mise en cache et mÃ©triques
                    self.cache.put(cache_key, result)
                    self.circuit_breakers[backend_name].record_success()
                    self.metrics.transcriptions_success.labels(backend=backend_name).inc()
                    
                    # Ajout mÃ©triques de latence
                    latency = time.time() - start_time
                    self.metrics.transcription_latency.labels(backend=backend_name).observe(latency)
                    
                    result.cached = False
                    return result
                    
                except Exception as e:
                    self.circuit_breakers[backend_name].record_failure()
                    self.metrics.transcriptions_failed.labels(backend=backend_name).inc()
                    print(f"âš ï¸ Backend {backend_name} Ã©chec: {str(e)}")
                    continue
        
        # Tous les backends ont Ã©chouÃ©
        self.metrics.total_failures.inc()
        
        # Retourner rÃ©sultat d'Ã©chec
        return STTResult(
            text="",
            confidence=0.0,
            segments=[],
            processing_time=time.time() - start_time,
            device="cuda:0",
            rtf=999.0,
            backend_used="none",
            success=False,
            cached=False,
            error="Tous les backends STT ont Ã©chouÃ©"
        )

    async def transcribe_pcm(self, pcm_bytes: bytes, sr: int) -> str:
        """
        Helper pour StreamingMicrophoneManager - transcrit PCM bytes directement.
        
        Args:
            pcm_bytes: DonnÃ©es PCM en bytes (int16)
            sr: Sample rate (doit Ãªtre 16000 pour compatibilitÃ©)
            
        Returns:
            str: Texte transcrit
        """
        # Convertir PCM bytes en numpy array float32
        pcm_array = np.frombuffer(pcm_bytes, dtype=np.int16)
        audio_float = pcm_array.astype(np.float32) / 32768.0  # Normaliser int16 â†’ float32
        
        # Transcription via mÃ©thode principale
        result = await self.transcribe(audio_float)
        
        return result.text if result.success else ""
    
    def get_backend_status(self) -> Dict[str, str]:
        """Retourne le statut des backends"""
        return {
            name: "open" if self.circuit_breakers[name].is_open() else "closed"
            for name in self.fallback_chain
        }
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du cache"""
        total = self.cache.hits + self.cache.misses
        hit_rate = self.cache.hits / total if total > 0 else 0
        
        return {
            "size_bytes": self.cache.current_size,
            "max_size_bytes": self.cache.max_size,
            "usage_percent": (self.cache.current_size / self.cache.max_size) * 100,
            "entries": len(self.cache.cache),
            "hits": self.cache.hits,
            "misses": self.cache.misses,
            "hit_rate": hit_rate
        }
    
    def force_backend(self, backend_name: str) -> None:
        """Force l'utilisation d'un backend spÃ©cifique"""
        if backend_name in self.fallback_chain:
            # RÃ©organiser la chaÃ®ne de fallback
            self.fallback_chain = [backend_name] + [b for b in self.fallback_chain if b != backend_name]
            print(f"ðŸ”„ Backend forcÃ©: {backend_name}")
        else:
            print(f"âš ï¸ Backend inconnu: {backend_name}")
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        Retourne les mÃ©triques du systÃ¨me STT.
        
        Returns:
            Dict avec mÃ©triques systÃ¨me
        """
        return {
            "cache": self.get_cache_stats(),
            "backends": self.get_backend_status(),
            "gpu": self._get_gpu_metrics() if torch.cuda.is_available() else {}
        }
    
    def _get_gpu_metrics(self) -> Dict[str, float]:
        """
        Collecte les mÃ©triques GPU RTX 3090.
        
        Returns:
            Dict avec mÃ©triques GPU
        """
        return {
            "memory_allocated": torch.cuda.memory_allocated(0) / 1024**3,  # GB
            "memory_reserved": torch.cuda.memory_reserved(0) / 1024**3,    # GB
            "max_memory": torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        }

# Point d'entrÃ©e pour tests
if __name__ == "__main__":
    print("ðŸ§ª Test UnifiedSTTManager")
    
    # Test d'initialisation
    manager = UnifiedSTTManager()
    
    # Test audio factice
    test_audio = np.zeros(16000 * 3, dtype=np.float32)  # 3 secondes
    
    # Test asynchrone
    async def test_transcription():
        result = await manager.transcribe(test_audio)
        print(f"âœ… Test transcription: {result.success}")
        print(f"ðŸ“Š MÃ©triques: {manager.get_metrics()}")
    
    asyncio.run(test_transcription()) 