#!/usr/bin/env python3
"""
Backend STT Prism Optimis√© - SuperWhisper V6
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
Performance cible: WER < 15%, Latence < 300ms
"""

import os
import sys
import time
import asyncio
import numpy as np
import logging
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import torch
from faster_whisper import WhisperModel
import re
from difflib import SequenceMatcher

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
# RTX 5060 (CUDA:0) = INTERDITE - RTX 3090 (CUDA:1) = OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

class OptimizedPrismSTTBackend:
    """Backend STT avec optimisations pour r√©duire WER et latence"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # Configuration optimis√©e
        self.model_size = config.get('model', 'large-v2')
        self.compute_type = config.get('compute_type', 'float16')
        self.language = 'fr'  # FORCER le fran√ßais - CRITIQUE
        
        # Param√®tres optimis√©s pour pr√©cision
        self.beam_size = 10  # Augment√© de 5 ‚Üí 10
        self.best_of = 10   # Augment√© pour meilleure s√©lection
        self.temperature = 0.0  # D√©terministe
        self.compression_ratio_threshold = 2.4
        self.log_prob_threshold = -1.0
        self.no_speech_threshold = 0.6
        
        # VAD optimis√© pour ne pas couper (param√®tres faster-whisper valides)
        self.vad_filter = True
        self.vad_parameters = {
            "threshold": 0.2,  # Plus sensible
            "min_speech_duration_ms": 50,
            "max_speech_duration_s": float('inf'),
            "min_silence_duration_ms": 2000,
            "speech_pad_ms": 800  # Plus de contexte
        }
        
        # Dictionnaire de corrections contextuelles
        self.corrections = {
            # Corrections sp√©cifiques √† vos tests
            "super whispers": "SuperWhisper",
            "super whisper": "SuperWhisper", 
            "char √†": "chat,",
            "after whisper": "faster-whisper",
            "faster whispers": "faster-whisper",
            "cr√©sentemps": "chrysanth√®me",
            "kakemono": "kak√©mono",
            "identifiant": "int8",
            "in√©dite": "int8",
            "sainte vitesse": "fin du test",
            "sacrement": "cinqui√®mement",
            "dixi√®mement": "sixi√®mement",
            "modificieurs": "mots difficiles",
            
            # Corrections techniques
            "gpu": "GPU",
            "rtx": "RTX",
            "machine learning": "machine learning",
            "agorique": "algorithme",
            "la tige artificielle": "l'intelligence artificielle",
            "monde monarme": "monde moderne",
            
            # Nombres
            "23-47-1995": "vingt-trois, quarante-sept, mille neuf cent quatre-vingt-quinze",
            "23, 47, 1995": "vingt-trois, quarante-sept, mille neuf cent quatre-vingt-quinze"
        }
        
        self.model = None
        self.logger = self._setup_logging()
        self._initialize_backend()
    
    def _setup_logging(self):
        logger = logging.getLogger(f'OptimizedPrism_{self.model_size}')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def validate_rtx3090_configuration(self):
        """Validation obligatoire de la configuration RTX 3090"""
        if not torch.cuda.is_available():
            raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
        
        cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
        if cuda_devices != '1':
            raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
        
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory < 20:  # RTX 3090 = ~24GB
            raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
        
        gpu_name = torch.cuda.get_device_name(0)
        print(f"‚úÖ RTX 3090 valid√©e: {gpu_name} ({gpu_memory:.1f}GB)")
    
    def _initialize_backend(self):
        """Initialisation avec optimisations RTX 3090"""
        try:
            self.logger.info(f"üöÄ Initialisation Backend Optimis√© {self.model_size}")
            
            # Validation GPU RTX 3090
            self.validate_rtx3090_configuration()
            
            # Chargement mod√®le optimis√©
            start_time = time.time()
            # Configuration faster-whisper optimis√©e
            # Note: faster-whisper utilise 'cuda' avec CUDA_VISIBLE_DEVICES pour s√©lection GPU
            self.model = WhisperModel(
                self.model_size, 
                device="cuda",  # Utilise CUDA_VISIBLE_DEVICES=1 pour RTX 3090
                compute_type=self.compute_type,
                cpu_threads=4,
                num_workers=1
            )
            
            load_time = time.time() - start_time
            self.logger.info(f"‚úÖ Mod√®le charg√© ({load_time:.2f}s)")
            
            # Warm-up avec phrases fran√ßaises
            self._warm_up_french()
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation: {e}")
            raise
    
    def _warm_up_french(self):
        """Warm-up avec contexte fran√ßais pour priming"""
        try:
            self.logger.info("üî• Warm-up fran√ßais...")
            
            warmup_texts = [
                "Bonjour, ceci est un test en fran√ßais.",
                "L'intelligence artificielle transforme notre monde.",
                "SuperWhisper utilise faster-whisper sur RTX 3090."
            ]
            
            for text in warmup_texts:
                duration = len(text) * 0.06
                samples = int(16000 * duration)
                dummy_audio = np.random.randn(samples).astype(np.float32) * 0.1
                
                segments, _ = self.model.transcribe(
                    dummy_audio,
                    language='fr',
                    initial_prompt=text,
                    beam_size=5,
                    vad_filter=False
                )
                list(segments)  # Consommer
            
            self.logger.info("‚úÖ Warm-up termin√©")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Warm-up √©chou√©: {e}")
    
    async def transcribe(self, audio: np.ndarray):
        """Transcription optimis√©e avec corrections"""
        start_time = time.perf_counter()
        audio_duration = len(audio) / 16000
        
        try:
            # Prompt initial pour forcer le fran√ßais
            initial_prompt = (
                "Ceci est une transcription en fran√ßais. "
                "Les mots techniques incluent : intelligence artificielle, "
                "machine learning, algorithme, GPU, RTX 3090, faster-whisper, "
                "SuperWhisper, chrysanth√®me, kak√©mono."
            )
            
            # Transcription avec param√®tres optimis√©s
            segments, info = await asyncio.to_thread(
                self._transcribe_sync,
                audio,
                initial_prompt
            )
            
            # Reconstruction du texte
            full_text = " ".join([s['text'] for s in segments])
            
            # Application des corrections
            corrected_text, corrections = self._apply_corrections(full_text)
            
            # Calcul confiance moyenne
            avg_confidence = np.mean([s.get('confidence', 0.9) for s in segments]) if segments else 0.0
            
            processing_time = time.perf_counter() - start_time
            rtf = processing_time / audio_duration
            
            if corrections > 0:
                self.logger.info(f"üìù {corrections} corrections appliqu√©es")
            
            return {
                'text': corrected_text,
                'confidence': avg_confidence,
                'segments': segments,
                'processing_time': processing_time,
                'rtf': rtf,
                'success': True
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur transcription: {e}")
            return {
                'text': "",
                'confidence': 0.0,
                'segments': [],
                'processing_time': time.perf_counter() - start_time,
                'rtf': 999.0,
                'success': False,
                'error': str(e)
            }
    
    def _transcribe_sync(self, audio: np.ndarray, initial_prompt: str):
        """Transcription synchrone optimis√©e"""
        segments_gen, info = self.model.transcribe(
            audio,
            language='fr',  # FORCER fran√ßais
            task='transcribe',
            beam_size=self.beam_size,
            best_of=self.best_of,
            patience=2.0,  # Plus de patience
            length_penalty=1.0,
            repetition_penalty=1.2,
            temperature=self.temperature,
            compression_ratio_threshold=self.compression_ratio_threshold,
            log_prob_threshold=self.log_prob_threshold,
            no_speech_threshold=self.no_speech_threshold,
            condition_on_previous_text=True,
            initial_prompt=initial_prompt,
            suppress_blank=True,
            suppress_tokens=[-1],
            without_timestamps=False,
            max_initial_timestamp=2.0,
            prepend_punctuations="\"'¬ø([{-",
            append_punctuations="\"'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö\")]}„ÄÅ",
            vad_filter=self.vad_filter,
            vad_parameters=self.vad_parameters,
            chunk_length=30,
            clip_timestamps="0"
        )
        
        segments = []
        for segment in segments_gen:
            segments.append({
                'start': segment.start,
                'end': segment.end,
                'text': segment.text.strip(),
                'confidence': min(1.0, max(0.0, segment.avg_logprob + 5) / 5)
            })
        
        return segments, info
    
    def _apply_corrections(self, text: str) -> Tuple[str, int]:
        """Applique les corrections contextuelles"""
        corrected = text
        corrections_count = 0
        
        for wrong, correct in self.corrections.items():
            if wrong.lower() in corrected.lower():
                pattern = re.compile(re.escape(wrong), re.IGNORECASE)
                corrected = pattern.sub(correct, corrected)
                corrections_count += 1
        
        # Corrections suppl√©mentaires
        corrected = re.sub(r'\b(\d+)-(\d+)-(\d+)\b', r'\1, \2, \3', corrected)
        corrected = re.sub(r'([a-z])([A-Z])', r'\1. \2', corrected)
        corrected = re.sub(r'\s+', ' ', corrected).strip()
        
        return corrected, corrections_count 