#!/usr/bin/env python3
"""
Test direct Ollama - D√©marrage et test des mod√®les
"""

import subprocess
import time
import requests
import json
import os

def start_ollama():
    """D√©marre Ollama avec le bon r√©pertoire de mod√®les"""
    print("üöÄ D√©marrage Ollama...")
    
    env = os.environ.copy()
    env['OLLAMA_MODELS'] = 'D:\\modeles_llm'
    
    try:
        # Tuer processus existant
        subprocess.run(['cmd.exe', '/c', 'taskkill /F /IM ollama.exe'], 
                      capture_output=True)
        time.sleep(2)
        
        # D√©marrer Ollama
        process = subprocess.Popen(
            ['cmd.exe', '/c', 'ollama serve'],
            env=env,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        print("‚è≥ Attente d√©marrage Ollama...")
        time.sleep(10)
        
        return process
        
    except Exception as e:
        print(f"‚ùå Erreur d√©marrage: {e}")
        return None

def test_ollama():
    """Test la connexion et les mod√®les Ollama"""
    print("üîç Test connexion Ollama...")
    
    try:
        # Test sant√©
        response = requests.get('http://127.0.0.1:11434/api/tags', timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"‚úÖ Ollama accessible - {len(models)} mod√®les")
            
            for model in models:
                print(f"  üì¶ {model['name']} ({model['size']} bytes)")
            
            return models
        else:
            print(f"‚ùå Status: {response.status_code}")
            return []
            
    except Exception as e:
        print(f"‚ùå Erreur test: {e}")
        return []

def test_model_inference(model_name):
    """Test d'inf√©rence avec un mod√®le"""
    print(f"üß† Test inf√©rence: {model_name}")
    
    try:
        data = {
            "model": model_name,
            "prompt": "Bonjour, comment allez-vous ?",
            "stream": False
        }
        
        response = requests.post(
            'http://127.0.0.1:11434/api/generate',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ R√©ponse: {result.get('response', '')[:100]}...")
            return True
        else:
            print(f"‚ùå Erreur inf√©rence: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False

def main():
    print("üîß Test Ollama SuperWhisper V6")
    print("=" * 50)
    
    # D√©marrer Ollama
    process = start_ollama()
    
    # Tester connexion
    models = test_ollama()
    
    if models:
        # Tester premier mod√®le disponible
        first_model = models[0]['name']
        test_model_inference(first_model)
    else:
        print("‚ùå Aucun mod√®le trouv√©")
    
    print("\nüìã R√©sum√©:")
    print(f"  - Ollama d√©marr√©: {'‚úÖ' if process else '‚ùå'}")
    print(f"  - Mod√®les disponibles: {len(models)}")
    
    if process:
        input("\n‚è∏Ô∏è Appuyez sur Entr√©e pour arr√™ter Ollama...")
        process.terminate()

if __name__ == "__main__":
    main()