#!/usr/bin/env python3
"""
D√©marrage correct Ollama avec mod√®les D:/modeles_llm
"""

import subprocess
import time
import os
import requests

def start_ollama_service():
    """D√©marre le service Ollama correctement"""
    print("üöÄ Configuration et d√©marrage Ollama...")
    
    try:
        # Arr√™ter tous les processus Ollama existants
        print("üõë Arr√™t processus Ollama existants...")
        subprocess.run(['cmd.exe', '/c', 'taskkill /F /IM ollama.exe /T'], 
                      capture_output=True)
        subprocess.run(['cmd.exe', '/c', 'taskkill /F /IM "ollama app.exe" /T'], 
                      capture_output=True)
        time.sleep(3)
        
        # D√©finir les variables d'environnement
        env_vars = {
            'OLLAMA_MODELS': 'D:\\modeles_llm',
            'OLLAMA_HOST': '127.0.0.1:11434',
            'OLLAMA_ORIGINS': '*'
        }
        
        print(f"üìÅ OLLAMA_MODELS: {env_vars['OLLAMA_MODELS']}")
        
        # Cr√©er commande de d√©marrage
        cmd = [
            'cmd.exe', '/c',
            f'set OLLAMA_MODELS={env_vars["OLLAMA_MODELS"]} && '
            f'set OLLAMA_HOST={env_vars["OLLAMA_HOST"]} && '
            f'set OLLAMA_ORIGINS={env_vars["OLLAMA_ORIGINS"]} && '
            'C:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Ollama\\ollama.exe serve'
        ]
        
        print("‚è≥ D√©marrage du service Ollama...")
        
        # D√©marrer Ollama en arri√®re-plan
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        # Attendre d√©marrage
        print("‚è±Ô∏è Attente initialisation (15s)...")
        for i in range(15):
            time.sleep(1)
            print(f"  {i+1}/15s", end='\r')
        print()
        
        return process
        
    except Exception as e:
        print(f"‚ùå Erreur d√©marrage: {e}")
        return None

def test_ollama_connection():
    """Test la connexion Ollama"""
    print("üîç Test connexion Ollama...")
    
    max_retries = 5
    for attempt in range(max_retries):
        try:
            response = requests.get('http://127.0.0.1:11434/api/tags', timeout=10)
            if response.status_code == 200:
                models = response.json().get('models', [])
                print(f"‚úÖ Ollama connect√© - {len(models)} mod√®les disponibles")
                
                for model in models[:3]:  # Afficher les 3 premiers
                    name = model.get('name', 'unknown')
                    size_mb = model.get('size', 0) / (1024*1024)
                    print(f"  üì¶ {name} ({size_mb:.0f}MB)")
                
                return True, models
            else:
                print(f"‚ö†Ô∏è Status {response.status_code} - Tentative {attempt+1}/{max_retries}")
                
        except requests.exceptions.ConnectionError:
            print(f"‚è≥ Connexion impossible - Tentative {attempt+1}/{max_retries}")
        except Exception as e:
            print(f"‚ùå Erreur: {e}")
        
        if attempt < max_retries - 1:
            time.sleep(3)
    
    print("‚ùå Connexion Ollama √©chou√©e")
    return False, []

def test_model_simple(models):
    """Test simple d'un mod√®le"""
    if not models:
        print("‚ö†Ô∏è Aucun mod√®le √† tester")
        return False
    
    model_name = models[0]['name']
    print(f"üß† Test mod√®le: {model_name}")
    
    try:
        data = {
            "model": model_name,
            "prompt": "Bonjour, r√©pondez bri√®vement en fran√ßais.",
            "stream": False,
            "options": {
                "num_predict": 50
            }
        }
        
        response = requests.post(
            'http://127.0.0.1:11434/api/generate',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get('response', '').strip()
            print(f"‚úÖ R√©ponse mod√®le: '{response_text[:100]}...'")
            return True
        else:
            print(f"‚ùå Erreur mod√®le: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur test mod√®le: {e}")
        return False

def main():
    print("üîß SuperWhisper V6 - Configuration Ollama")
    print("=" * 60)
    
    # D√©marrer service
    process = start_ollama_service()
    if not process:
        print("‚ùå √âchec d√©marrage Ollama")
        return
    
    # Tester connexion
    connected, models = test_ollama_connection()
    
    if connected:
        # Tester un mod√®le
        model_works = test_model_simple(models)
        
        print("\nüìä R√âSULTATS:")
        print(f"  üöÄ Ollama d√©marr√©: ‚úÖ")
        print(f"  üîó Connexion: ‚úÖ")
        print(f"  üì¶ Mod√®les: {len(models)}")
        print(f"  üß† Test mod√®le: {'‚úÖ' if model_works else '‚ùå'}")
        
        if model_works:
            print("\nüéâ OLLAMA OP√âRATIONNEL POUR SUPERWHISPER V6!")
            print("   Le LLM peut maintenant √™tre utilis√© dans le pipeline.")
        else:
            print("\n‚ö†Ô∏è Ollama connect√© mais mod√®le non fonctionnel")
    else:
        print("\n‚ùå √âCHEC CONFIGURATION OLLAMA")
        print("   V√©rifiez l'installation et les mod√®les dans D:/modeles_llm")
    
    # Laisser Ollama tourner
    print(f"\nüîÑ Ollama reste actif (PID: {process.pid})")
    print("   Utiliser Ctrl+C dans le terminal Ollama pour arr√™ter")

if __name__ == "__main__":
    main()