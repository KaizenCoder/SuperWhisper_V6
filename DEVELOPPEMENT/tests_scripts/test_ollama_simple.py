#!/usr/bin/env python3
"""
ğŸ”§ TEST SIMPLE OLLAMA API
Test rapide pour identifier le problÃ¨me HTTP 404
ğŸš¨ CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import requests
import json

# =============================================================================
# ğŸš¨ CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU

print("ğŸ® GPU Configuration: RTX 3090 (CUDA:1) forcÃ©e")
print("ğŸ”§ Test Simple API Ollama")
print("=" * 50)

def test_ollama_endpoint(url, data=None, method="GET"):
    """Test un endpoint Ollama"""
    try:
        if method == "GET":
            response = requests.get(url, timeout=10)
        else:
            response = requests.post(url, json=data, timeout=30)
        
        print(f"ğŸ”— {method} {url}")
        print(f"  ğŸ“Š Status: {response.status_code}")
        
        if response.status_code == 200:
            print(f"  âœ… SuccÃ¨s")
            if response.text:
                try:
                    result = response.json()
                    if isinstance(result, dict) and 'response' in result:
                        print(f"  ğŸ’¬ RÃ©ponse: {result['response'][:100]}...")
                    elif isinstance(result, list) and len(result) > 0:
                        print(f"  ğŸ“¦ Ã‰lÃ©ments: {len(result)}")
                    else:
                        print(f"  ğŸ“„ DonnÃ©es: {str(result)[:100]}...")
                except:
                    print(f"  ğŸ“„ Texte: {response.text[:100]}...")
        else:
            print(f"  âŒ Erreur: {response.status_code}")
            print(f"  ğŸ“„ Texte: {response.text[:200]}")
        
        return response.status_code == 200, response
        
    except Exception as e:
        print(f"  ğŸ’¥ Exception: {e}")
        return False, None

# Tests des endpoints
print("\n1ï¸âƒ£ Test SantÃ© Ollama")
test_ollama_endpoint("http://127.0.0.1:11434/api/tags")

print("\n2ï¸âƒ£ Test API Generate (format natif)")
data_native = {
    "model": "nous-hermes-2-mistral-7b-dpo:latest",
    "prompt": "Quelle est la capitale de la France?",
    "stream": False
}
success_native, resp_native = test_ollama_endpoint("http://127.0.0.1:11434/api/generate", data_native, "POST")

print("\n3ï¸âƒ£ Test API Chat (format OpenAI-like)")
data_openai = {
    "model": "nous-hermes-2-mistral-7b-dpo:latest",
    "messages": [{"role": "user", "content": "Quelle est la capitale de la France?"}],
    "stream": False
}
success_openai, resp_openai = test_ollama_endpoint("http://127.0.0.1:11434/v1/chat/completions", data_openai, "POST")

print("\n4ï¸âƒ£ Test API Chat (endpoint alternatif)")
success_chat, resp_chat = test_ollama_endpoint("http://127.0.0.1:11434/api/chat", data_openai, "POST")

# RÃ©sumÃ©
print("\n" + "=" * 50)
print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS")
print("=" * 50)
print(f"âœ… API Native (/api/generate): {'OUI' if success_native else 'NON'}")
print(f"âœ… API OpenAI (/v1/chat/completions): {'OUI' if success_openai else 'NON'}")
print(f"âœ… API Chat (/api/chat): {'OUI' if success_chat else 'NON'}")

if success_native:
    print("\nğŸ¯ SOLUTION: Utiliser l'API native /api/generate")
elif success_openai:
    print("\nğŸ¯ SOLUTION: Utiliser l'API OpenAI /v1/chat/completions")
elif success_chat:
    print("\nğŸ¯ SOLUTION: Utiliser l'API chat /api/chat")
else:
    print("\nâŒ PROBLÃˆME: Aucune API ne fonctionne")

print("\nğŸ”§ Test terminÃ©!") 