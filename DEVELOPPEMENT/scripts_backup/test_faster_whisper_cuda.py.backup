#!/usr/bin/env python3
"""
Test Diagnostic - faster-whisper + CUDA RTX 3090
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

import torch

def test_torch_cuda():
    """Test configuration PyTorch CUDA"""
    print("\nüîç TEST PYTORCH CUDA")
    print("-" * 40)
    
    print(f"‚úÖ CUDA disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"‚úÖ Nombre de GPU: {torch.cuda.device_count()}")
        print(f"‚úÖ GPU actuel: {torch.cuda.current_device()}")
        print(f"‚úÖ Nom GPU: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

def test_faster_whisper_import():
    """Test import faster-whisper"""
    print("\nüì¶ TEST IMPORT FASTER-WHISPER")
    print("-" * 40)
    
    try:
        from faster_whisper import WhisperModel
        print("‚úÖ Import faster_whisper r√©ussi")
        return True
    except ImportError as e:
        print(f"‚ùå Import faster_whisper √©chou√©: {e}")
        return False

def test_faster_whisper_devices():
    """Test devices disponibles pour faster-whisper"""
    print("\nüéÆ TEST DEVICES FASTER-WHISPER")
    print("-" * 40)
    
    try:
        from faster_whisper import WhisperModel
        
        # Test diff√©rentes configurations device
        devices_to_test = [
            "cuda",
            "cuda:0", 
            "cuda:1",
            "cpu"
        ]
        
        for device in devices_to_test:
            try:
                print(f"üß™ Test device '{device}'...")
                model = WhisperModel("tiny", device=device, compute_type="float16")
                print(f"   ‚úÖ Device '{device}' fonctionne")
                del model  # Lib√©rer m√©moire
                return device
            except Exception as e:
                print(f"   ‚ùå Device '{device}' √©choue: {e}")
        
        print("‚ùå Aucun device CUDA fonctionnel")
        return None
        
    except Exception as e:
        print(f"‚ùå Erreur test devices: {e}")
        return None

def test_faster_whisper_model():
    """Test cr√©ation mod√®le faster-whisper"""
    print("\nü§ñ TEST MOD√àLE FASTER-WHISPER")
    print("-" * 40)
    
    try:
        from faster_whisper import WhisperModel
        
        # Test avec device auto-d√©tect√©
        print("üöÄ Cr√©ation mod√®le tiny (test rapide)...")
        
        # Essayer d'abord avec device auto
        try:
            model = WhisperModel("tiny", device="auto", compute_type="float16")
            print("‚úÖ Mod√®le cr√©√© avec device='auto'")
            device_info = model.device
            print(f"   Device utilis√©: {device_info}")
            del model
            return True
        except Exception as e:
            print(f"‚ùå Device 'auto' √©choue: {e}")
        
        # Essayer avec CPU en fallback
        try:
            model = WhisperModel("tiny", device="cpu", compute_type="int8")
            print("‚úÖ Mod√®le cr√©√© avec device='cpu'")
            del model
            return True
        except Exception as e:
            print(f"‚ùå Device 'cpu' √©choue aussi: {e}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur cr√©ation mod√®le: {e}")
        return False

def main():
    """Test principal diagnostic"""
    print("üéØ DIAGNOSTIC FASTER-WHISPER + CUDA RTX 3090")
    print("=" * 60)
    
    # 1. Test PyTorch CUDA
    test_torch_cuda()
    
    # 2. Test import faster-whisper
    import_ok = test_faster_whisper_import()
    if not import_ok:
        print("\n‚ùå ARR√äT: faster-whisper non importable")
        return False
    
    # 3. Test devices
    working_device = test_faster_whisper_devices()
    
    # 4. Test mod√®le
    model_ok = test_faster_whisper_model()
    
    # 5. R√©sum√©
    print("\n" + "="*60)
    print("üìä R√âSUM√â DIAGNOSTIC")
    print("="*60)
    
    print(f"‚úÖ PyTorch CUDA: {torch.cuda.is_available()}")
    print(f"‚úÖ faster-whisper import: {import_ok}")
    print(f"{'‚úÖ' if working_device else '‚ùå'} Device fonctionnel: {working_device or 'Aucun'}")
    print(f"{'‚úÖ' if model_ok else '‚ùå'} Mod√®le cr√©able: {model_ok}")
    
    if working_device and model_ok:
        print("\nüéâ DIAGNOSTIC POSITIF!")
        print(f"   ‚Üí Utiliser device='{working_device}' dans backend optimis√©")
    else:
        print("\n‚ö†Ô∏è PROBL√àMES D√âTECT√âS")
        print("   ‚Üí V√©rifier installation CUDA/faster-whisper")
    
    return working_device and model_ok

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\n‚úÖ Diagnostic termin√© avec succ√®s")
    else:
        print(f"\n‚ùå Diagnostic r√©v√®le des probl√®mes") 