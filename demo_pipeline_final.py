#!/usr/bin/env python3
"""
SuperWhisper V6 - D√©monstration Pipeline Final
=============================================

D√©monstration compl√®te du pipeline voix-√†-voix
Mode simulation si pas de microphone disponible
"""

import os
import sys
import time
import logging
import asyncio
from pathlib import Path

# Configuration GPU RTX 3090
os.environ.setdefault("CUDA_VISIBLE_DEVICES", "1")
os.environ.setdefault("CUDA_DEVICE_ORDER", "PCI_BUS_ID")
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:1024")

# Ajout du projet au PATH
PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

# Configuration des logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(name)s] %(levelname)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("PipelineDemo")

class SuperWhisperV6Pipeline:
    """Pipeline complet SuperWhisper V6"""
    
    def __init__(self):
        self.stt_manager = None
        self.llm_manager = None
        self.tts_manager = None
        
    def initialize(self):
        """Initialisation compl√®te du pipeline"""
        logger.info("üöÄ SuperWhisper V6 - Initialisation Pipeline Complet")
        logger.info("="*60)
        
        # 1. STT Manager
        logger.info("üé§ STT Manager...")
        from STT.unified_stt_manager import UnifiedSTTManager
        
        stt_config = {
            'timeout_per_minute': 10.0,
            'cache_size_mb': 200,
            'fallback_chain': ['prism_primary']
        }
        
        self.stt_manager = UnifiedSTTManager(stt_config)
        logger.info("‚úÖ STT Manager pr√™t (Whisper large-v2, RTX 3090)")
        
        # 2. LLM Manager
        logger.info("üß† LLM Manager...")
        from LLM.llm_manager_enhanced import EnhancedLLMManager
        
        llm_config = {
            "model_name": "nous-hermes",
            "max_context_turns": 5,
            "timeout_seconds": 10,
            "max_tokens": 80,
            "temperature": 0.7
        }
        
        self.llm_manager = EnhancedLLMManager(llm_config)
        logger.info("‚úÖ LLM Manager pr√™t (Nous-hermes Mistral 7B)")
        
        # 3. TTS Manager
        logger.info("üîä TTS Manager...")
        from TTS.tts_manager import UnifiedTTSManager
        
        tts_config = {
            "cache": {
                "enabled": True,
                "max_size": 100,
                "ttl_seconds": 3600
            },
            "circuit_breaker": {
                "failure_threshold": 5,
                "reset_timeout_seconds": 30
            },
            "backends": {
                "silent_emergency": {
                    "enabled": True
                }
            },
            "default_handler": "silent_emergency"
        }
        
        self.tts_manager = UnifiedTTSManager(tts_config)
        logger.info("‚úÖ TTS Manager pr√™t (Voix fran√ßaise)")
        
        logger.info("üéâ Pipeline SuperWhisper V6 initialis√© !")
        
    async def process_conversation(self, user_input):
        """Traite une conversation compl√®te"""
        logger.info(f"üí¨ Traitement: '{user_input}'")
        
        conversation_start = time.time()
        
        # 1. LLM - G√©n√©ration r√©ponse
        llm_start = time.time()
        try:
            response = await self.llm_manager.generate_response(
                user_input=user_input,
                max_tokens=100,
                temperature=0.7,
                include_context=True
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LLM erreur: {e}")
            response = "Bonjour ! Je suis SuperWhisper V6, votre assistant vocal intelligent."
            
        llm_time = time.time() - llm_start
        
        # 2. TTS - Synth√®se vocale
        tts_start = time.time()
        try:
            audio_data = await self.tts_manager.synthesize(text=response)
            tts_success = audio_data is not None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è TTS erreur: {e}")
            tts_success = False
            
        tts_time = time.time() - tts_start
        
        total_time = time.time() - conversation_start
        
        return {
            'response': response,
            'llm_time': llm_time,
            'tts_time': tts_time,
            'total_time': total_time,
            'tts_success': tts_success
        }
        
    def run_demo(self):
        """D√©monstration du pipeline"""
        logger.info("\n" + "="*60)
        logger.info("üéØ D√âMONSTRATION PIPELINE SUPERWHISPER V6")
        logger.info("="*60)
        
        # Conversations de test
        test_conversations = [
            "Bonjour, comment allez-vous ?",
            "Quel temps fait-il aujourd'hui ?",
            "Pouvez-vous me parler de l'intelligence artificielle ?",
            "Merci, au revoir !"
        ]
        
        total_conversations = len(test_conversations)
        success_count = 0
        total_latency = 0
        
        for i, user_input in enumerate(test_conversations, 1):
            print(f"\nüé§ Conversation {i}/{total_conversations}")
            print(f"üë§ Utilisateur: '{user_input}'")
            
            # Traitement pipeline
            result = asyncio.run(self.process_conversation(user_input))
            
            print(f"ü§ñ Assistant: '{result['response']}'")
            print(f"‚è±Ô∏è Latences: LLM {result['llm_time']:.1f}s | TTS {result['tts_time']:.1f}s | Total {result['total_time']:.1f}s")
            
            if result['total_time'] < 3.0:
                print("‚úÖ Latence < 3s")
                success_count += 1
            else:
                print("‚ö†Ô∏è Latence > 3s")
                
            total_latency += result['total_time']
            
            # Pause entre conversations
            time.sleep(1)
        
        # Statistiques finales
        avg_latency = total_latency / total_conversations
        success_rate = (success_count / total_conversations) * 100
        
        print("\n" + "="*60)
        print("üìä R√âSULTATS D√âMONSTRATION")
        print("="*60)
        print(f"üí¨ Conversations test√©es: {total_conversations}")
        print(f"‚úÖ Succ√®s latence <3s: {success_count}/{total_conversations} ({success_rate:.1f}%)")
        print(f"‚è±Ô∏è Latence moyenne: {avg_latency:.1f}s")
        print(f"üéØ Objectif <3s: {'‚úÖ ATTEINT' if avg_latency < 3.0 else '‚ö†Ô∏è Non atteint'}")
        
        # √âvaluation finale
        if success_rate >= 75 and avg_latency < 3.0:
            print("\nüèÜ PIPELINE SUPERWHISPER V6 VALID√â !")
            print("‚úÖ Pr√™t pour d√©ploiement production")
        elif success_rate >= 50:
            print("\nüéØ PIPELINE FONCTIONNEL")
            print("‚ö†Ô∏è Optimisations recommand√©es")
        else:
            print("\n‚ùå PIPELINE √Ä AM√âLIORER")
            print("üîß Corrections requises")
            
        return {
            'conversations': total_conversations,
            'success_rate': success_rate,
            'avg_latency': avg_latency,
            'pipeline_ready': success_rate >= 75 and avg_latency < 3.0
        }

def main():
    """Point d'entr√©e principal"""
    pipeline = SuperWhisperV6Pipeline()
    
    try:
        # Initialisation
        pipeline.initialize()
        
        # D√©monstration
        results = pipeline.run_demo()
        
        # Message final
        if results['pipeline_ready']:
            print("\nüéâ SuperWhisper V6 est pr√™t pour une utilisation en production !")
            print("üöÄ Pipeline voix-√†-voix fonctionnel avec performances optimales")
        else:
            print(f"\nüìà Pipeline fonctionnel √† {results['success_rate']:.1f}%")
            print("üîß Optimisations disponibles pour am√©liorer les performances")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()