#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test de dÃ©tection CUDA avec PyTorch
ğŸš¨ CONFIGURATION GPU: RTX 3090 via CUDA_VISIBLE_DEVICES='1'
ğŸš¨ RTX 5060 Ti (CUDA:0 physique) MASQUÃ‰E
"""

import os
import torch

# =============================================================================
# ğŸš¨ CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
# CUDA_VISIBLE_DEVICES='1' masque RTX 5060 Ti et rend RTX 3090 visible comme device 0
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU

print("ğŸ”’ CUDA_VISIBLE_DEVICES:", os.environ.get('CUDA_VISIBLE_DEVICES'))
print("ğŸš¨ RTX 5060 Ti MASQUÃ‰E / RTX 3090 devient device 0 visible")
print("=== TEST RTX 3090 EXCLUSIF ===")
print(f"ğŸ¯ CUDA disponible: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    device_count = torch.cuda.device_count()
    print(f"ğŸ”¥ Nombre de GPU visibles: {device_count}")
    
    for i in range(device_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"\n   GPU {i}: {gpu_name}")
        print(f"   MÃ©moire: {gpu_memory:.1f} GB")
        
        # Validation RTX 3090 exclusive
        if "RTX 3090" in gpu_name and gpu_memory >= 20:
            print(f"   âœ… RTX 3090 confirmÃ©e sur device {i}")
        elif "RTX 5060" in gpu_name:
            print(f"   ğŸš« RTX 5060 Ti dÃ©tectÃ©e - DEVRAIT ÃŠTRE MASQUÃ‰E!")
        
        # Test d'allocation sur RTX 3090
        if "RTX 3090" in gpu_name:
            try:
                torch.cuda.set_device(i)
                x = torch.randn(3000, 3000).cuda()  # Test 36MB sur RTX 3090
                print(f"   âœ… Allocation 36MB RTX 3090 rÃ©ussie!")
                print(f"   ğŸ“Š Tensor sur: {x.device}")
                del x
                torch.cuda.empty_cache()
            except Exception as e:
                print(f"   âŒ Erreur allocation RTX 3090: {e}")
    
    print(f"\nğŸ¯ Version CUDA: {torch.version.cuda}")
    print(f"ğŸ¯ GPU courant: {torch.cuda.current_device()}")

else:
    print("âŒ CUDA non disponible")

print("\n" + "="*50) 