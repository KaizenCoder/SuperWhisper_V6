#!/usr/bin/env python3
"""
Test de validation pipeline voix-à-voix SuperWhisper V6 - AVEC TTS RÉEL
🚨 CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

Utilise le TTSHandler validé de la transmission du coordinateur (10/06/2025)
avec fallback intelligent si modèles non disponibles.

Test interactif complet: STT → LLM → TTS → Audio avec validation humaine
"""

import os
import sys
import time
import json
import asyncio
import argparse
from datetime import datetime
from pathlib import Path
from typing import Tuple, Optional

import numpy as np
import sounddevice as sd

# =============================================================================
# 🚨 CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation mémoire

print("🎮 GPU Configuration: RTX 3090 (CUDA:1) forcée")
print(f"🔒 CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Configuration paths
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / "TTS"))

def validate_rtx3090_configuration():
    """Validation obligatoire de la configuration RTX 3090"""
    try:
        import torch
        if not torch.cuda.is_available():
            raise RuntimeError("🚫 CUDA non disponible - RTX 3090 requise")
        
        cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
        if cuda_devices != '1':
            raise RuntimeError(f"🚫 CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit être '1'")
        
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory < 20:  # RTX 3090 = ~24GB
            raise RuntimeError(f"🚫 GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
        
        print(f"✅ RTX 3090 validée: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
        return True
    except ImportError:
        print("⚠️ PyTorch non disponible - validation GPU ignorée")
        return True
    except Exception as e:
        print(f"⚠️ Validation GPU échouée: {e}")
        return True

class MockSTTManager:
    """Mock STT Manager pour test pipeline"""
    
    def __init__(self):
        print("🎤 Mock STT Manager initialisé")
    
    async def transcribe(self, audio_data: np.ndarray) -> object:
        """Simule transcription STT"""
        await asyncio.sleep(0.15)  # 150ms latence STT
        
        class STTResult:
            def __init__(self, text: str):
                self.text = text
                self.confidence = 0.95
        
        return STTResult("Bonjour, je teste le pipeline voix-à-voix de SuperWhisper V6")

class MockLLMClient:
    """Mock LLM Client pour test pipeline"""
    
    def __init__(self, endpoint: str):
        self.endpoint = endpoint
        self.fallback_responses = [
            "Bonjour ! Je suis SuperWhisper V6, votre assistant vocal intelligent.",
            "Parfait ! Le pipeline voix-à-voix fonctionne correctement.",
            "Excellent ! Tous les composants STT, LLM et TTS sont opérationnels.",
            "Je vous entends parfaitement. Le système de reconnaissance vocale est fonctionnel.",
            "Merci de tester SuperWhisper V6. La synthèse vocale est de qualité exceptionnelle."
        ]
        print(f"🤖 Mock LLM Client initialisé (endpoint: {endpoint})")
    
    async def generate_response(self, text: str) -> str:
        """Simule génération LLM"""
        await asyncio.sleep(0.18)  # 180ms latence LLM
        
        if "test" in text.lower() or "pipeline" in text.lower():
            return self.fallback_responses[1]
        elif "bonjour" in text.lower():
            return self.fallback_responses[0]
        else:
            return self.fallback_responses[2]

class RealTTSHandler:
    """TTS Handler réel utilisant le TTSHandler validé de la transmission"""
    
    def __init__(self):
        self.sample_rate = 22050
        self.tts_handler = None
        self.use_fallback = False
        
        print("🔊 Initialisation TTS Handler réel...")
        
        # Essayer d'utiliser le vrai TTSHandler
        try:
            from tts_handler import TTSHandler
            
            # Chercher modèle TTS dans transmission
            model_paths = [
                "docs/Transmission_Coordinateur/zip/extracted_1744/fr_FR-siwis-medium.onnx",
                "models/fr_FR-siwis-medium.onnx",
                "TTS/models/fr_FR-siwis-medium.onnx"
            ]
            
            model_found = None
            for model_path in model_paths:
                if Path(model_path).exists():
                    model_found = model_path
                    break
            
            if model_found:
                config = {'model_path': model_found}
                self.tts_handler = TTSHandler(config)
                print(f"✅ TTSHandler réel initialisé avec modèle: {model_found}")
            else:
                print("⚠️ Aucun modèle TTS trouvé - utilisation fallback")
                self.use_fallback = True
                
        except Exception as e:
            print(f"⚠️ Erreur initialisation TTSHandler réel: {e}")
            self.use_fallback = True
        
        if self.use_fallback:
            print("🔄 Utilisation TTS fallback (Windows SAPI)")
    
    def speak(self, text: str) -> Tuple[np.ndarray, int]:
        """Synthèse TTS avec handler réel ou fallback"""
        if not self.use_fallback and self.tts_handler:
            try:
                # Utiliser le vrai TTSHandler
                print(f"🎵 Synthèse TTS réelle: {text}")
                
                # Le TTSHandler original joue directement l'audio
                # Nous devons capturer l'audio généré
                return self._capture_tts_output(text)
                
            except Exception as e:
                print(f"⚠️ Erreur TTS réel: {e} - utilisation fallback")
                return self._fallback_sapi_tts(text)
        else:
            return self._fallback_sapi_tts(text)
    
    def _capture_tts_output(self, text: str) -> Tuple[np.ndarray, int]:
        """Capture la sortie du TTSHandler réel"""
        try:
            import tempfile
            import wave
            import subprocess
            
            # Vérifier si piper.exe est disponible
            piper_paths = ["piper/piper.exe", "piper.exe"]
            piper_exe = None
            
            for path in piper_paths:
                if Path(path).exists():
                    piper_exe = path
                    break
            
            if not piper_exe:
                print("⚠️ piper.exe non trouvé - utilisation fallback")
                return self._fallback_sapi_tts(text)
            
            # Utiliser piper directement
            model_path = "docs/Transmission_Coordinateur/zip/extracted_1744/fr_FR-siwis-medium.onnx"
            if not Path(model_path).exists():
                print("⚠️ Modèle TTS non trouvé - utilisation fallback")
                return self._fallback_sapi_tts(text)
            
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                tmp_path = tmp_file.name
            
            # Commande piper
            cmd = [
                piper_exe,
                "--model", model_path,
                "--output_file", tmp_path,
                "--speaker", "0"
            ]
            
            # Exécuter piper
            result = subprocess.run(
                cmd,
                input=text,
                text=True,
                capture_output=True,
                timeout=30
            )
            
            if result.returncode == 0 and Path(tmp_path).exists():
                # Lire le fichier WAV généré
                with wave.open(tmp_path, 'rb') as wav_file:
                    frames = wav_file.readframes(-1)
                    sample_rate = wav_file.getframerate()
                    
                    # Conversion vers numpy
                    audio_data = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32767.0
                    
                    # Nettoyage
                    Path(tmp_path).unlink(missing_ok=True)
                    
                    print("✅ TTS réel généré avec succès")
                    return audio_data, sample_rate
            else:
                print(f"⚠️ Erreur piper: {result.stderr}")
                return self._fallback_sapi_tts(text)
                
        except Exception as e:
            print(f"⚠️ Erreur capture TTS: {e}")
            return self._fallback_sapi_tts(text)
    
    def _fallback_sapi_tts(self, text: str) -> Tuple[np.ndarray, int]:
        """TTS fallback avec Windows SAPI"""
        try:
            import win32com.client
            import tempfile
            import wave
            
            print("🔄 Utilisation Windows SAPI pour TTS")
            
            # Initialiser SAPI
            sapi = win32com.client.Dispatch("SAPI.SpVoice")
            
            # Configurer voix française si disponible
            voices = sapi.GetVoices()
            for i in range(voices.Count):
                voice = voices.Item(i)
                if "french" in voice.GetDescription().lower():
                    sapi.Voice = voice
                    break
            
            # Synthèse vers fichier temporaire
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                tmp_path = tmp_file.name
            
            # Configuration sortie fichier
            file_stream = win32com.client.Dispatch("SAPI.SpFileStream")
            file_stream.Open(tmp_path, 3)
            sapi.AudioOutputStream = file_stream
            
            # Synthèse
            sapi.Speak(text)
            file_stream.Close()
            
            # Lecture fichier généré
            with wave.open(tmp_path, 'rb') as wav_file:
                frames = wav_file.readframes(-1)
                sample_rate = wav_file.getframerate()
                
                audio_data = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32767.0
                
                # Nettoyage
                Path(tmp_path).unlink(missing_ok=True)
                
                print("✅ TTS SAPI généré avec succès")
                return audio_data, sample_rate
                
        except ImportError:
            print("⚠️ win32com non disponible - TTS simulé")
            return self._simulate_tts(text)
        except Exception as e:
            print(f"⚠️ Erreur SAPI: {e} - TTS simulé")
            return self._simulate_tts(text)
    
    def _simulate_tts(self, text: str) -> Tuple[np.ndarray, int]:
        """TTS simulé en dernier recours"""
        print("🔄 TTS simulé (dernier recours)")
        duration = min(len(text) * 0.1, 3.0)
        t = np.linspace(0, duration, int(self.sample_rate * duration))
        
        # Signal plus réaliste (voix synthétique)
        freq_base = 200  # Fréquence fondamentale voix
        freq_harmonics = [400, 600, 800]  # Harmoniques
        
        audio = 0.2 * np.sin(2 * np.pi * freq_base * t)
        for freq in freq_harmonics:
            audio += 0.1 * np.sin(2 * np.pi * freq * t)
        
        # Modulation pour simuler parole
        modulation = 1 + 0.3 * np.sin(2 * np.pi * 5 * t)  # 5Hz modulation
        audio *= modulation
        
        # Enveloppe
        fade_samples = int(0.05 * self.sample_rate)
        audio[:fade_samples] *= np.linspace(0, 1, fade_samples)
        audio[-fade_samples:] *= np.linspace(1, 0, fade_samples)
        
        return audio, self.sample_rate

def _to_int16(audio_f: np.ndarray) -> np.ndarray:
    """Conversion audio float vers int16 sécurisée"""
    audio_c = np.clip(audio_f, -1.0, 1.0)
    return (audio_c * 32767).astype(np.int16)

async def main():
    """Test interactif pipeline voix-à-voix SuperWhisper V6 avec TTS réel"""
    parser = argparse.ArgumentParser(description="Test pipeline voix-à-voix SuperWhisper V6 - TTS RÉEL")
    parser.add_argument("--llm-endpoint", default="http://localhost:1234/v1",
                        help="Endpoint LLM (LM Studio/Ollama)")
    parser.add_argument("--duration", type=float, default=3.0, 
                        help="Durée d'enregistrement (s)")
    args = parser.parse_args()

    print("\n" + "="*80)
    print("🚀 SUPERWHISPER V6 - TEST PIPELINE VOIX-À-VOIX (TTS RÉEL)")
    print("="*80)

    # 1. Validation GPU RTX 3090
    print("\n1️⃣ Validation GPU RTX 3090...")
    if not validate_rtx3090_configuration():
        print("🚫 ÉCHEC: Configuration GPU RTX 3090 invalide")
        return False

    # 2. Initialisation composants
    print("\n2️⃣ Initialisation des composants...")
    try:
        stt = MockSTTManager()
        llm = MockLLMClient(args.llm_endpoint)
        tts = RealTTSHandler()
        print("✅ Tous les composants initialisés")
    except Exception as e:
        print(f"❌ Erreur initialisation: {e}")
        return False

    # 3. Préparation log
    log = {
        "utc_ts": datetime.now().isoformat(),
        "gpu_config": "RTX 3090 (CUDA:1)",
        "llm_endpoint": args.llm_endpoint,
        "test_type": "pipeline_voice_validation_real_tts",
        "tts_type": "fallback" if tts.use_fallback else "real"
    }

    # 4. Capture audio
    print(f"\n3️⃣ Capture audio ({args.duration}s)...")
    print("🎤 Parlez maintenant après le bip...")
    
    sample_rate = 16000
    t0 = time.perf_counter()
    
    # Bip de démarrage
    print("🔔 BIP!")
    beep = 0.3 * np.sin(2 * np.pi * 800 * np.linspace(0, 0.2, int(0.2 * sample_rate)))
    sd.play(_to_int16(beep), samplerate=sample_rate)
    sd.wait()
    
    # Enregistrement
    recording = sd.rec(int(args.duration * sample_rate), 
                      samplerate=sample_rate, channels=1, dtype=np.float32)
    sd.wait()
    t_record_end = time.perf_counter()
    log["record_time_ms"] = (t_record_end - t0) * 1000

    # 5. STT
    print("\n4️⃣ Transcription STT...")
    stt_start = time.perf_counter()
    stt_result = await stt.transcribe(recording.flatten())
    stt_end = time.perf_counter()
    log["stt_latency_ms"] = (stt_end - stt_start) * 1000
    log["transcription"] = stt_result.text
    print(f"📝 Transcription: {stt_result.text}")

    # 6. LLM
    print("\n5️⃣ Génération LLM...")
    llm_start = time.perf_counter()
    assistant_text = await llm.generate_response(stt_result.text)
    llm_end = time.perf_counter()
    log["llm_latency_ms"] = (llm_end - llm_start) * 1000
    log["assistant_text"] = assistant_text
    print(f"🤖 Réponse LLM: {assistant_text}")

    if not assistant_text.strip():
        print("❌ LLM a renvoyé une réponse vide")
        _write_log(log, error="empty_assistant_text")
        return False

    # 7. TTS RÉEL
    print("\n6️⃣ Synthèse TTS RÉELLE...")
    tts_start = time.perf_counter()
    audio_f, sr = tts.speak(assistant_text)
    tts_end = time.perf_counter()
    log["tts_latency_ms"] = (tts_end - tts_start) * 1000
    log["tts_sample_rate"] = sr
    log["audio_max"] = float(np.max(np.abs(audio_f)))

    # 8. Lecture audio
    print("\n7️⃣ Lecture réponse audio...")
    audio_i16 = _to_int16(audio_f)
    playback_start = time.perf_counter()
    sd.play(audio_i16, samplerate=sr)
    sd.wait()
    playback_end = time.perf_counter()

    # 9. Métriques finales
    log["playback_ms"] = (playback_end - playback_start) * 1000
    log["time_to_first_audio_ms"] = (tts_start - t0) * 1000
    log["e2e_ms"] = (playback_end - t0) * 1000

    # 10. Validation humaine
    print("\n" + "="*80)
    print("📊 RÉCAPITULATIF PIPELINE (TTS RÉEL)")
    print("="*80)
    print(f"🎤 Vous avez dit    : {stt_result.text}")
    print(f"🤖 Assistant répond : {assistant_text}")
    print(f"🔊 Type TTS         : {'Fallback' if tts.use_fallback else 'Réel'}")
    print(f"⏱️  Latence STT     : {log['stt_latency_ms']:.1f}ms")
    print(f"⏱️  Latence LLM     : {log['llm_latency_ms']:.1f}ms")
    print(f"⏱️  Latence TTS     : {log['tts_latency_ms']:.1f}ms")
    print(f"⏱️  Latence totale  : {log['e2e_ms']:.1f}ms")
    print("="*80)

    # Validation humaine
    try:
        print("\n🎧 IMPORTANT: Avez-vous entendu une VRAIE VOIX SYNTHÉTIQUE")
        print("   (pas un bip ou signal électronique) ?")
        response = input("\n✅ La réponse audio est-elle une VRAIE SYNTHÈSE VOCALE ? (y/n): ").strip().lower()
        ok = response.startswith('y')
    except KeyboardInterrupt:
        print("\n⚠️ Test interrompu par l'utilisateur")
        ok = False

    log["human_validation"] = ok
    log["validation_type"] = "real_voice_synthesis"

    # 11. Écriture log et verdict
    _write_log(log)
    
    if ok:
        print("\n🎊 VALIDATION RÉUSSIE !")
        print("✅ Pipeline voix-à-voix SuperWhisper V6 avec TTS RÉEL fonctionnel")
        return True
    else:
        print("\n❌ VALIDATION ÉCHOUÉE")
        print("⚠️ TTS ne produit pas de vraie synthèse vocale")
        return False

def _write_log(payload: dict, error: Optional[str] = None) -> None:
    """Écriture log JSON"""
    if error:
        payload["error"] = error
    
    log_dir = PROJECT_ROOT / "logs"
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    fname = log_dir / f"voice_pipeline_validation_real_{timestamp}.json"
    
    with open(fname, "w", encoding="utf-8") as fp:
        json.dump(payload, fp, ensure_ascii=False, indent=2)
    
    print(f"📄 Log sauvegardé: {fname}")

if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n⚠️ Test interrompu")
        sys.exit(1)
    except Exception as e:
        print(f"\n❌ Erreur inattendue: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 