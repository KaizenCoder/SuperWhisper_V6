#!/usr/bin/env python3
"""
Test Vocal Complet SuperWhisper V6
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

Test pipeline voix-√†-voix complet:
üé§ Microphone ‚Üí STT ‚Üí LLM ‚Üí TTS ‚Üí üîà Speakers

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import asyncio
import time
import json
from datetime import datetime
from pathlib import Path

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Ajouter les r√©pertoires au path pour les imports
sys.path.append(str(Path(__file__).parent.parent))  # PIPELINE
sys.path.append(str(Path(__file__).parent.parent.parent))  # Racine

try:
    from PIPELINE.pipeline_orchestrator import PipelineOrchestrator, ConversationTurn
    from STT.streaming_microphone_manager import StreamingMicrophoneManager
    from STT.unified_stt_manager import UnifiedSTTManager
    from LLM.llm_client import LLMClient
    from TTS.unified_tts_manager import UnifiedTTSManager
    from PIPELINE.audio_output_manager import AudioOutputManager
    import yaml
    import torch
except ImportError as e:
    print(f"‚ùå Erreur import: {e}")
    print("üí° V√©rifiez que tous les modules sont disponibles")
    sys.exit(1)

def validate_rtx3090():
    """Validation obligatoire RTX 3090"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 = ~24GB
        raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
    
    print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")

class TestVocalComplet:
    """Test vocal complet du pipeline SuperWhisper V6"""
    
    def __init__(self):
        self.config_path = Path("PIPELINE/config/pipeline.yaml")
        self.config = None
        self.pipeline = None
        self.metrics = {
            "tests_effectues": 0,
            "tests_reussis": 0,
            "latences": [],
            "erreurs": [],
            "debut_test": None,
            "fin_test": None
        }
    
    def load_config(self):
        """Charger la configuration pipeline"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            print(f"‚úÖ Configuration charg√©e: {self.config_path}")
            return True
        except Exception as e:
            print(f"‚ùå Erreur chargement config: {e}")
            return False
    
    async def initialize_pipeline(self):
        """Initialiser le pipeline complet"""
        try:
            print("\nüîß Initialisation pipeline...")
            
            # Cr√©er le pipeline orchestrator
            self.pipeline = PipelineOrchestrator(
                config_path=str(self.config_path),
                enable_metrics=True
            )
            
            # Initialiser tous les composants
            await self.pipeline.initialize()
            
            print("‚úÖ Pipeline initialis√© avec succ√®s")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur initialisation pipeline: {e}")
            self.metrics["erreurs"].append(f"Init pipeline: {e}")
            return False
    
    async def test_conversation_vocale(self, duree_ecoute=10):
        """Test conversation vocale compl√®te"""
        print(f"\nüé§ D√âBUT TEST VOCAL COMPLET")
        print(f"‚è±Ô∏è Dur√©e d'√©coute: {duree_ecoute}s")
        print("üó£Ô∏è Parlez maintenant au microphone...")
        print("üí¨ Le syst√®me va transcrire, g√©n√©rer une r√©ponse et la synth√©tiser")
        
        debut_test = time.time()
        self.metrics["debut_test"] = datetime.now().isoformat()
        
        try:
            # Cr√©er un tour de conversation
            conversation_turn = ConversationTurn(
                user_input="",  # Sera rempli par STT
                timestamp=datetime.now()
            )
            
            # D√©marrer l'√©coute microphone
            print("\nüé§ √âcoute microphone d√©marr√©e...")
            
            # Simuler l'√©coute (dans un vrai test, on utiliserait StreamingMicrophoneManager)
            await asyncio.sleep(duree_ecoute)
            
            # Pour ce test, utilisons un texte d'exemple
            texte_test = "Bonjour, comment allez-vous aujourd'hui ?"
            conversation_turn.user_input = texte_test
            
            print(f"üìù Texte transcrit (simul√©): '{texte_test}'")
            
            # Traiter avec le pipeline
            debut_pipeline = time.time()
            
            # Ajouter √† la queue du pipeline
            await self.pipeline.add_to_queue(conversation_turn)
            
            # Attendre le traitement (timeout 30s)
            timeout = 30
            attente = 0
            while attente < timeout:
                if conversation_turn.llm_response and conversation_turn.tts_audio:
                    break
                await asyncio.sleep(0.1)
                attente += 0.1
            
            fin_pipeline = time.time()
            latence_totale = (fin_pipeline - debut_pipeline) * 1000
            
            if conversation_turn.llm_response:
                print(f"ü§ñ R√©ponse LLM: '{conversation_turn.llm_response}'")
            
            if conversation_turn.tts_audio:
                print(f"üîä Audio TTS g√©n√©r√©: {len(conversation_turn.tts_audio)} bytes")
                
                # Jouer l'audio (si AudioOutputManager disponible)
                try:
                    audio_manager = AudioOutputManager()
                    await audio_manager.play_audio(conversation_turn.tts_audio)
                    print("üîà Audio jou√© avec succ√®s")
                except Exception as e:
                    print(f"‚ö†Ô∏è Lecture audio √©chou√©e: {e}")
            
            # M√©triques
            self.metrics["tests_effectues"] += 1
            self.metrics["latences"].append(latence_totale)
            
            if conversation_turn.llm_response and conversation_turn.tts_audio:
                self.metrics["tests_reussis"] += 1
                print(f"‚úÖ Test vocal r√©ussi - Latence: {latence_totale:.1f}ms")
                return True
            else:
                print(f"‚ùå Test vocal √©chou√© - R√©ponse incompl√®te")
                return False
                
        except Exception as e:
            print(f"‚ùå Erreur test vocal: {e}")
            self.metrics["erreurs"].append(f"Test vocal: {e}")
            return False
        
        finally:
            fin_test = time.time()
            self.metrics["fin_test"] = datetime.now().isoformat()
            duree_totale = fin_test - debut_test
            print(f"‚è±Ô∏è Dur√©e totale test: {duree_totale:.1f}s")
    
    async def test_microphone_reel(self):
        """Test avec microphone r√©el (si disponible)"""
        print("\nüé§ TEST MICROPHONE R√âEL")
        
        try:
            # Tenter d'initialiser le microphone
            mic_manager = StreamingMicrophoneManager()
            
            print("üé§ Microphone initialis√©")
            print("üó£Ô∏è Parlez pendant 5 secondes...")
            
            # D√©marrer l'enregistrement
            audio_data = []
            
            def audio_callback(audio_chunk):
                audio_data.append(audio_chunk)
            
            # Simuler l'enregistrement (impl√©mentation simplifi√©e)
            await asyncio.sleep(5)
            
            if audio_data:
                print(f"‚úÖ Audio captur√©: {len(audio_data)} chunks")
                return True
            else:
                print("‚ö†Ô∏è Aucun audio captur√©")
                return False
                
        except Exception as e:
            print(f"‚ùå Erreur microphone: {e}")
            print("üí° Test avec microphone simul√© √† la place")
            return await self.test_conversation_vocale(5)
    
    def generer_rapport(self):
        """G√©n√©rer rapport de test"""
        print("\nüìä RAPPORT TEST VOCAL COMPLET")
        print("=" * 50)
        
        print(f"üìÖ D√©but: {self.metrics['debut_test']}")
        print(f"üìÖ Fin: {self.metrics['fin_test']}")
        print(f"üß™ Tests effectu√©s: {self.metrics['tests_effectues']}")
        print(f"‚úÖ Tests r√©ussis: {self.metrics['tests_reussis']}")
        
        if self.metrics['tests_effectues'] > 0:
            taux_succes = (self.metrics['tests_reussis'] / self.metrics['tests_effectues']) * 100
            print(f"üìà Taux de succ√®s: {taux_succes:.1f}%")
        
        if self.metrics['latences']:
            latence_moy = sum(self.metrics['latences']) / len(self.metrics['latences'])
            latence_max = max(self.metrics['latences'])
            latence_min = min(self.metrics['latences'])
            
            print(f"‚è±Ô∏è Latence moyenne: {latence_moy:.1f}ms")
            print(f"‚è±Ô∏è Latence min: {latence_min:.1f}ms")
            print(f"‚è±Ô∏è Latence max: {latence_max:.1f}ms")
            
            if latence_moy < 1200:
                print("üéØ OBJECTIF < 1200ms: ‚úÖ ATTEINT")
            else:
                print("üéØ OBJECTIF < 1200ms: ‚ùå MANQU√â")
        
        if self.metrics['erreurs']:
            print(f"\n‚ùå Erreurs ({len(self.metrics['erreurs'])}):")
            for i, erreur in enumerate(self.metrics['erreurs'], 1):
                print(f"  {i}. {erreur}")
        
        # Sauvegarder rapport
        rapport_path = Path("PIPELINE/reports/test_vocal_complet.json")
        rapport_path.parent.mkdir(exist_ok=True)
        
        with open(rapport_path, 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Rapport sauvegard√©: {rapport_path}")
    
    async def cleanup(self):
        """Nettoyage ressources"""
        if self.pipeline:
            try:
                await self.pipeline.shutdown()
                print("‚úÖ Pipeline ferm√© proprement")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur fermeture pipeline: {e}")

async def main():
    """Fonction principale"""
    print("üöÄ TEST VOCAL COMPLET SUPERWHISPER V6")
    print("=" * 60)
    
    # Validation GPU obligatoire
    try:
        validate_rtx3090()
    except Exception as e:
        print(f"‚ùå {e}")
        return 1
    
    # Cr√©er testeur
    testeur = TestVocalComplet()
    
    try:
        # Charger configuration
        if not testeur.load_config():
            return 1
        
        # Initialiser pipeline
        if not await testeur.initialize_pipeline():
            return 1
        
        print("\nüéØ CHOIX DU TEST:")
        print("1. Test avec microphone r√©el")
        print("2. Test avec simulation vocale")
        
        # Pour l'automatisation, utilisons le test simulation
        print("\nü§ñ Lancement test simulation vocale...")
        
        # Test principal
        succes = await testeur.test_conversation_vocale(duree_ecoute=3)
        
        # Test microphone si possible
        print("\nüé§ Tentative test microphone r√©el...")
        await testeur.test_microphone_reel()
        
        # Rapport final
        testeur.generer_rapport()
        
        if succes:
            print("\nüéä TEST VOCAL COMPLET R√âUSSI !")
            return 0
        else:
            print("\n‚ùå Test vocal √©chou√©")
            return 1
    
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Test interrompu par l'utilisateur")
        return 1
    
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        return 1
    
    finally:
        await testeur.cleanup()

if __name__ == "__main__":
    try:
        result = asyncio.run(main())
        sys.exit(result)
    except Exception as e:
        print(f"‚ùå Erreur fatale: {e}")
        sys.exit(1) 