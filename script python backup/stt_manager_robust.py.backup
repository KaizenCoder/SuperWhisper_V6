# STT/stt_manager_robust.py
"""
RobustSTTManager - Gestionnaire STT robuste avec fallback multi-mod√®les
Conforme aux exigences du PRD v3.1 et du Plan de D√©veloppement Final
"""
import torch
import asyncio
import logging
import time
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import io
import soundfile as sf
import librosa
from faster_whisper import WhisperModel
from prometheus_client import Counter, Histogram, Gauge
from circuitbreaker import circuit
from STT.vad_manager import OptimizedVADManager  # Int√©gration VAD existant

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# M√©triques Prometheus conformes √† l'architecture
stt_transcriptions_total = Counter('stt_transcriptions_total', 'Total transcriptions')
stt_errors_total = Counter('stt_errors_total', 'Total errors')
stt_latency_seconds = Histogram('stt_latency_seconds', 'Transcription latency')
stt_vram_usage_bytes = Gauge('stt_vram_usage_bytes', 'VRAM usage in bytes')

class RobustSTTManager:
    """
    Manager STT robuste avec:
    - S√©lection GPU automatique optimale
    - Cha√Æne de fallback multi-mod√®les
    - Gestion VRAM intelligente avec clear_cache
    - M√©triques temps r√©el (latence, erreurs, succ√®s)
    - Conversion audio robuste (bytes ‚Üî numpy)
    - Int√©gration VAD existant
    """
    
    def __init__(self, config: Dict[str, Any], vad_manager: Optional[OptimizedVADManager] = None):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.device = self._select_optimal_device()
        self.models = {}
        self.fallback_chain = config.get("fallback_chain", ["base", "tiny"])
        self.vad_manager = vad_manager
        
        # Configuration compute_type selon device
        self.compute_type = "float16" if self.device == "cuda" else "int8"
        
        # M√©triques internes pour monitoring
        self.metrics = {
            "transcriptions": 0,
            "errors": 0,
            "avg_latency": 0.0,
            "total_latency": 0.0
        }

    def _select_optimal_device(self) -> str:
        """S√©lection intelligente du device avec fallback GPU‚ÜíCPU"""
        if not self.config.get('use_gpu', True):
            self.logger.info("GPU d√©sactiv√© par configuration")
            return "cpu"
            
        if torch.cuda.is_available():
            # üö® CRITIQUE: Configuration dual-GPU RTX 3090 (CUDA:0) + RTX 5060 Ti (CUDA:1)
            # RTX 3090 (CUDA:0) = SEULE GPU AUTORIS√âE
            # RTX 5060 Ti (CUDA:1) = INTERDITE D'UTILISATION
            
            # Forcer RTX 3090 (CUDA:0) - Configuration valid√©e factuellement
            selected_gpu = 0  # RTX 3090 24GB VRAM UNIQUEMENT
            torch.cuda.set_device(selected_gpu)
            
            gpu_count = torch.cuda.device_count()
            self.logger.info(f"üéÆ GPU d√©tect√©e(s): {gpu_count} - Utilisation RTX 3090 (GPU {selected_gpu})")
            
            # V√©rification VRAM RTX 3090 (24GB attendus) sur CUDA:0
            target_gpu = 0  # RTX 3090 (CUDA:0) EXCLUSIVEMENT
            vram_free = torch.cuda.get_device_properties(target_gpu).total_memory - torch.cuda.memory_allocated(target_gpu)
            vram_free_gb = vram_free / (1024**3)
            vram_total_gb = torch.cuda.get_device_properties(target_gpu).total_memory / (1024**3)
            
            # Validation RTX 3090 (24GB VRAM attendus) - IND√âPENDAMMENT DU NOMBRE DE GPU
            if vram_total_gb < 20:  # RTX 3090 = ~24GB
                self.logger.error(f"üö´ ERREUR: GPU {target_gpu} a seulement {vram_total_gb:.1f}GB VRAM (RTX 3090 = 24GB attendus)")
                self.logger.error("üö´ S√âCURIT√â: Fallback CPU pour √©viter RTX 5060")
                return "cpu"  # Fallback CPU si mauvaise GPU
            
            if vram_free_gb < 2.0:
                self.logger.warning(f"VRAM insuffisante ({vram_free_gb:.1f}GB), fallback CPU")
                return "cpu"
                
            # Validation finale et confirmation RTX 3090
            if vram_total_gb >= 20:
                self.logger.info(f"‚úÖ RTX 3090 confirm√©e (GPU {target_gpu}): {vram_total_gb:.1f}GB VRAM totale, {vram_free_gb:.1f}GB disponible")
            else:
                self.logger.warning(f"‚ö†Ô∏è GPU {target_gpu} validation partielle : {vram_total_gb:.1f}GB VRAM totale, {vram_free_gb:.1f}GB disponible")
            
            return "cuda"
        else:
            self.logger.warning("CUDA non disponible, utilisation CPU")
            return "cpu"

    async def initialize(self):
        """Initialisation asynchrone avec chargement mod√®les"""
        self.logger.info(f"Initialisation RobustSTTManager sur {self.device}")
        
        for model_name in self.fallback_chain:
            try:
                await self._load_model(model_name)
            except Exception as e:
                self.logger.error(f"√âchec chargement {model_name}: {e}")
                if model_name == self.fallback_chain[-1]:
                    raise RuntimeError("Aucun mod√®le STT disponible")

    async def _load_model(self, model_size: str):
        """Chargement optimis√© avec faster-whisper"""
        start_time = time.time()
        self.logger.info(f"Chargement mod√®le '{model_size}' sur '{self.device}'...")
        
        try:
            self.models[model_size] = WhisperModel(
                model_size,
                device=self.device,
                compute_type=self.compute_type,
                num_workers=2,  # Parall√©lisation
                download_root=self.config.get("model_cache_dir", "./models")
            )
            
            load_time = time.time() - start_time
            self.logger.info(f"‚úÖ Mod√®le {model_size} charg√© en {load_time:.2f}s")
            
            # Test rapide du mod√®le
            test_audio = np.zeros(16000, dtype=np.float32)  # 1s silence
            self.models[model_size].transcribe(test_audio, language="fr")
            
        except Exception as e:
            self.logger.error(f"‚ùå √âchec chargement {model_size}: {e}")
            raise

    @circuit(failure_threshold=3, recovery_timeout=30)
    async def transcribe_audio(self, audio_data: bytes, language: str = "fr") -> Dict[str, Any]:
        """
        Transcription robuste avec circuit breaker et m√©triques
        
        Args:
            audio_data: Audio en bytes (WAV format)
            language: Code langue (d√©faut: fran√ßais)
            
        Returns:
            Dict avec transcription, temps de traitement et device utilis√©
        """
        start_time = time.time()
        self.metrics["transcriptions"] += 1
        stt_transcriptions_total.inc()
        
        try:
            # Conversion bytes ‚Üí numpy avec validation
            audio_array = self._bytes_to_audio_array(audio_data)
            
            # Segmentation VAD si disponible
            if self.vad_manager and self.config.get('use_vad', True):
                self.logger.debug("Segmentation VAD activ√©e")
                speech_segments = await self._process_with_vad(audio_array)
            else:
                speech_segments = [(0, len(audio_array), audio_array)]
            
            # Transcription avec fallback chain
            full_transcription = ""
            segments_info = []
            
            for idx, (start_ms, end_ms, segment) in enumerate(speech_segments):
                segment_text = await self._transcribe_segment(segment, language, idx)
                if segment_text:
                    full_transcription += segment_text + " "
                    segments_info.append({
                        "start": start_ms,
                        "end": end_ms,
                        "text": segment_text
                    })
            
            # Calcul m√©triques
            processing_time = time.time() - start_time
            self._update_latency_metrics(processing_time)
            stt_latency_seconds.observe(processing_time)
            
            # Monitoring VRAM si GPU - RTX 3090 uniquement
            if self.device == "cuda":
                # Surveiller la VRAM sur la GPU active (RTX 3090 si dual-GPU)
                current_device = torch.cuda.current_device()
                vram_used = torch.cuda.memory_allocated(current_device) / (1024**3)
                stt_vram_usage_bytes.set(vram_used * 1024**3)
                self.logger.debug(f"VRAM utilis√©e sur GPU {current_device}: {vram_used:.2f}GB")
            
            result = {
                "text": full_transcription.strip(),
                "segments": segments_info,
                "processing_time": processing_time,
                "device": self.device,
                "model_used": self._get_active_model(),
                "metrics": {
                    "audio_duration": len(audio_array) / 16000,
                    "rtf": processing_time / (len(audio_array) / 16000)  # Real-time factor
                }
            }
            
            self.logger.info(f"‚úÖ Transcription r√©ussie en {processing_time:.2f}s (RTF: {result['metrics']['rtf']:.2f})")
            return result
            
        except Exception as e:
            self.metrics["errors"] += 1
            stt_errors_total.inc()
            self.logger.error(f"‚ùå Erreur transcription: {e}", exc_info=True)
            
            # Clear cache GPU si erreur m√©moire
            if "out of memory" in str(e).lower() and self.device == "cuda":
                torch.cuda.empty_cache()
                self.logger.info("Cache GPU vid√© apr√®s erreur m√©moire")
            
            raise

    async def _process_with_vad(self, audio_array: np.ndarray) -> List[Tuple[int, int, np.ndarray]]:
        """Traitement VAD asynchrone avec timestamps"""
        loop = asyncio.get_event_loop()
        segments = await loop.run_in_executor(
            None, 
            self.vad_manager.process_audio, 
            audio_array
        )
        return segments

    async def _transcribe_segment(self, segment: np.ndarray, language: str, idx: int) -> str:
        """Transcription d'un segment avec fallback chain"""
        for model_name in self.fallback_chain:
            if model_name not in self.models:
                continue
                
            try:
                self.logger.debug(f"Tentative transcription segment {idx} avec {model_name}")
                
                # Transcription asynchrone
                loop = asyncio.get_event_loop()
                segments, info = await loop.run_in_executor(
                    None,
                    lambda: self.models[model_name].transcribe(
                        segment,
                        language=language,
                        beam_size=5,
                        best_of=5,
                        temperature=0.0,  # D√©terministe
                        condition_on_previous_text=True,
                        vad_filter=False  # VAD d√©j√† appliqu√©
                    )
                )
                
                text = " ".join([seg.text for seg in segments]).strip()
                if text:  # Succ√®s si texte non vide
                    return text
                    
            except Exception as e:
                self.logger.warning(f"√âchec {model_name} sur segment {idx}: {e}")
                continue
                
        self.logger.error(f"√âchec complet transcription segment {idx}")
        return ""

    def _bytes_to_audio_array(self, audio_data: bytes) -> np.ndarray:
        """Conversion robuste bytes ‚Üí numpy avec validation"""
        try:
            # Tentative 1: soundfile (plus robuste)
            audio_array, sample_rate = sf.read(io.BytesIO(audio_data))
            
            # Conversion mono si n√©cessaire
            if audio_array.ndim > 1:
                audio_array = np.mean(audio_array, axis=1)
            
            # Resampling √† 16kHz si n√©cessaire
            if sample_rate != 16000:
                audio_array = librosa.resample(
                    audio_array, 
                    orig_sr=sample_rate, 
                    target_sr=16000
                )
            
            # Normalisation [-1, 1]
            if audio_array.dtype != np.float32:
                audio_array = audio_array.astype(np.float32)
            
            max_val = np.max(np.abs(audio_array))
            if max_val > 0:
                audio_array = audio_array / max_val
                
            return audio_array
            
        except Exception as e:
            self.logger.error(f"Erreur conversion audio: {e}")
            raise ValueError(f"Format audio invalide: {e}")

    def _update_latency_metrics(self, latency: float):
        """Mise √† jour m√©triques de latence avec moyenne mobile"""
        self.metrics["total_latency"] += latency
        self.metrics["avg_latency"] = (
            self.metrics["total_latency"] / self.metrics["transcriptions"]
        )
        
        self.logger.debug(
            f"Latence: {latency:.3f}s | "
            f"Moyenne: {self.metrics['avg_latency']:.3f}s | "
            f"Total transcriptions: {self.metrics['transcriptions']}"
        )

    def _get_active_model(self) -> str:
        """Retourne le mod√®le actuellement charg√©"""
        return list(self.models.keys())[0] if self.models else "none"

    def get_metrics(self) -> Dict[str, Any]:
        """Export m√©triques pour monitoring"""
        return {
            **self.metrics,
            "device": self.device,
            "models_loaded": list(self.models.keys()),
            "vad_enabled": self.vad_manager is not None
        }

    async def cleanup(self):
        """Nettoyage ressources et mod√®les"""
        self.logger.info("Nettoyage RobustSTTManager...")
        
        # Lib√©ration mod√®les
        self.models.clear()
        
        # Clear cache GPU
        if self.device == "cuda":
            torch.cuda.empty_cache()
            
        self.logger.info("‚úÖ Nettoyage termin√©")

    def listen_and_transcribe(self, duration: float = 5.0, use_vad: bool = True) -> Dict[str, Any]:
        """
        Version synchrone pour compatibilit√© avec ancien handler
        Enregistre audio du microphone et transcrit
        """
        import sounddevice as sd
        
        self.logger.info(f"üé§ Enregistrement {duration}s en cours...")
        
        try:
            # Enregistrement
            audio_data = sd.rec(
                int(duration * 16000),
                samplerate=16000,
                channels=1,
                dtype='float32'
            )
            sd.wait()
            
            # Conversion en bytes WAV
            buffer = io.BytesIO()
            sf.write(buffer, audio_data.flatten(), 16000, format='WAV', subtype='PCM_16')
            audio_bytes = buffer.getvalue()
            
            # Transcription asynchrone
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(self.transcribe_audio(audio_bytes, "fr"))
            loop.close()
            
            if result['text']:
                self.logger.info(f"‚úÖ Transcription: '{result['text']}'")
                return {
                    "success": True,
                    "transcription": result['text'],
                    "latency_ms": result['processing_time'] * 1000,
                    "model_used": result['model_used'],
                    "device": result['device']
                }
            else:
                return {
                    "success": False,
                    "error": "Transcription vide",
                    "latency_ms": result['processing_time'] * 1000
                }
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur enregistrement/transcription: {e}")
            return {
                "success": False,
                "error": str(e),
                "latency_ms": 0
            } 