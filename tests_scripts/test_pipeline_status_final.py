#!/usr/bin/env python3
"""
Test Status Final SuperWhisper V6
√âvaluation compl√®te du pipeline avec r√©sum√© des probl√®mes
"""

import os
import sys
import pathlib
import asyncio
import logging
import time
import yaml

def _setup_portable_environment():
    current_file = pathlib.Path(__file__).resolve()
    
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    
    return project_root

_PROJECT_ROOT = _setup_portable_environment()

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(name)s] %(levelname)s: %(message)s')
logger = logging.getLogger("StatusFinal")

class PipelineStatusEvaluator:
    """√âvaluateur du status final du pipeline SuperWhisper V6"""
    
    def __init__(self):
        self.results = {
            'stt': {'status': 'unknown', 'details': [], 'score': 0},
            'llm': {'status': 'unknown', 'details': [], 'score': 0},
            'tts': {'status': 'unknown', 'details': [], 'score': 0},
            'integration': {'status': 'unknown', 'details': [], 'score': 0}
        }

    async def test_stt_component(self):
        """Test du composant STT"""
        logger.info("üé§ Test composant STT...")
        
        try:
            from STT.unified_stt_manager import UnifiedSTTManager
            
            stt_config = {
                'timeout_per_minute': 10.0,
                'cache_size_mb': 100,
                'fallback_chain': ['prism_primary']
            }
            
            stt_manager = UnifiedSTTManager(stt_config)
            
            # Test avec audio simul√©
            import numpy as np
            sample_rate = 16000
            duration = 2.0
            
            # G√©n√©rer signal test avec ton pur
            t = np.linspace(0, duration, int(sample_rate * duration))
            frequency = 440  # La note
            amplitude = 0.3
            audio_data = amplitude * np.sin(2 * np.pi * frequency * t)
            audio_bytes = (audio_data * 32767).astype(np.int16).tobytes()
            
            start_time = time.perf_counter()
            result = await stt_manager.transcribe_pcm(audio_bytes, sample_rate)
            stt_time = (time.perf_counter() - start_time) * 1000
            
            self.results['stt']['details'].append(f"Temps de traitement: {stt_time:.1f}ms")
            self.results['stt']['details'].append(f"Configuration GPU RTX 3090: OK")
            self.results['stt']['details'].append(f"Backend Prism initialis√©: OK")
            
            if stt_time < 3000:  # Moins de 3 secondes
                self.results['stt']['score'] += 50
                self.results['stt']['details'].append("Performance: Excellente")
            else:
                self.results['stt']['details'].append("Performance: Lente")
            
            self.results['stt']['score'] += 30  # Points pour l'initialisation
            self.results['stt']['status'] = 'functional'
            
            return True
            
        except Exception as e:
            self.results['stt']['status'] = 'error'
            self.results['stt']['details'].append(f"Erreur: {e}")
            return False

    async def test_llm_component(self):
        """Test du composant LLM"""
        logger.info("üß† Test composant LLM...")
        
        try:
            from LLM.llm_manager_enhanced import EnhancedLLMManager
            
            llm_config = {
                'model': 'nous-hermes-2-mistral-7b-dpo:latest',
                'use_ollama': True,
                'timeout': 15.0
            }
            
            llm_manager = EnhancedLLMManager(llm_config)
            await llm_manager.initialize()
            
            # Test simple
            test_question = "Quelle est la capitale de la France ?"
            start_time = time.perf_counter()
            response = await llm_manager.generate_response(test_question)
            llm_time = (time.perf_counter() - start_time) * 1000
            
            self.results['llm']['details'].append(f"Temps de r√©ponse: {llm_time:.1f}ms")
            self.results['llm']['details'].append(f"Question: {test_question}")
            self.results['llm']['details'].append(f"R√©ponse: {response[:100]}...")
            
            # V√©rifier si Ollama fonctionne
            if hasattr(llm_manager, 'use_ollama') and llm_manager.use_ollama:
                if "Paris" in response or "capitale" in response.lower():
                    self.results['llm']['score'] += 70
                    self.results['llm']['details'].append("Ollama: Fonctionnel avec vraie r√©ponse")
                    self.results['llm']['status'] = 'optimal'
                else:
                    self.results['llm']['score'] += 50
                    self.results['llm']['details'].append("Ollama: Connect√© mais r√©ponse g√©n√©rique")
                    self.results['llm']['status'] = 'functional'
            else:
                self.results['llm']['score'] += 20
                self.results['llm']['details'].append("Fallback: Utilis√© (Ollama non accessible)")
                self.results['llm']['status'] = 'fallback'
            
            return True
            
        except Exception as e:
            self.results['llm']['status'] = 'error'
            self.results['llm']['details'].append(f"Erreur: {e}")
            return False

    async def test_tts_component(self):
        """Test du composant TTS"""
        logger.info("üîä Test composant TTS...")
        
        try:
            with open('config/tts.yaml', 'r') as f:
                tts_config = yaml.safe_load(f)
            
            from TTS.tts_manager import UnifiedTTSManager
            tts_manager = UnifiedTTSManager(tts_config)
            
            test_text = "Test de synth√®se vocale fran√ßaise."
            start_time = time.perf_counter()
            result = await tts_manager.synthesize(text=test_text)
            tts_time = (time.perf_counter() - start_time) * 1000
            
            self.results['tts']['details'].append(f"Temps de synth√®se: {tts_time:.1f}ms")
            self.results['tts']['details'].append(f"Texte test√©: {test_text}")
            
            if result.success and result.audio_data:
                audio_size = len(result.audio_data)
                self.results['tts']['details'].append(f"Audio g√©n√©r√©: {audio_size} bytes")
                self.results['tts']['details'].append(f"Backend utilis√©: {result.backend_used}")
                
                if tts_time < 1500:
                    self.results['tts']['score'] += 70
                    self.results['tts']['details'].append("Performance: Excellente")
                elif tts_time < 3000:
                    self.results['tts']['score'] += 50
                    self.results['tts']['details'].append("Performance: Bonne")
                else:
                    self.results['tts']['score'] += 30
                    self.results['tts']['details'].append("Performance: Acceptable")
                
                # Test sauvegarde
                audio_file = "test_tts_status.wav"
                with open(audio_file, 'wb') as f:
                    f.write(result.audio_data)
                
                self.results['tts']['details'].append(f"Fichier audio: {audio_file}")
                self.results['tts']['score'] += 20
                self.results['tts']['status'] = 'functional'
                
                # Nettoyer
                try:
                    os.remove(audio_file)
                except:
                    pass
                
                return True
            else:
                self.results['tts']['details'].append(f"Erreur TTS: {result.error}")
                self.results['tts']['status'] = 'error'
                return False
                
        except Exception as e:
            self.results['tts']['status'] = 'error'
            self.results['tts']['details'].append(f"Erreur: {e}")
            return False

    async def test_integration(self):
        """Test d'int√©gration E2E simplifi√©"""
        logger.info("üîó Test int√©gration E2E...")
        
        try:
            # Test d'int√©gration STT ‚Üí LLM ‚Üí TTS
            question_test = "Bonjour, comment allez-vous ?"
            
            # STT simul√© (entr√©e texte)
            self.results['integration']['details'].append(f"Entr√©e simul√©e: {question_test}")
            
            # LLM
            from LLM.llm_manager_enhanced import EnhancedLLMManager
            llm_config = {'model': 'nous-hermes-2-mistral-7b-dpo:latest', 'use_ollama': True}
            llm_manager = EnhancedLLMManager(llm_config)
            await llm_manager.initialize()
            
            llm_response = await llm_manager.generate_response(question_test)
            self.results['integration']['details'].append(f"R√©ponse LLM: {llm_response[:50]}...")
            
            # TTS
            with open('config/tts.yaml', 'r') as f:
                tts_config = yaml.safe_load(f)
            
            from TTS.tts_manager import UnifiedTTSManager
            tts_manager = UnifiedTTSManager(tts_config)
            
            tts_result = await tts_manager.synthesize(text=llm_response)
            
            if tts_result.success:
                self.results['integration']['details'].append("Pipeline E2E: Fonctionnel")
                self.results['integration']['score'] += 80
                self.results['integration']['status'] = 'functional'
                return True
            else:
                self.results['integration']['details'].append("Pipeline E2E: √âchec TTS")
                self.results['integration']['score'] += 40
                self.results['integration']['status'] = 'partial'
                return False
                
        except Exception as e:
            self.results['integration']['status'] = 'error'
            self.results['integration']['details'].append(f"Erreur: {e}")
            return False

    def generate_final_report(self):
        """G√©n√®re le rapport final"""
        total_score = sum(comp['score'] for comp in self.results.values())
        max_score = 400  # 100 par composant max
        
        print("\n" + "="*60)
        print("üéØ RAPPORT FINAL - SuperWhisper V6 Pipeline Status")
        print("="*60)
        
        # Score global
        percentage = (total_score / max_score) * 100
        if percentage >= 90:
            status_global = "üü¢ EXCELLENT"
        elif percentage >= 70:
            status_global = "üü° BON"
        elif percentage >= 50:
            status_global = "üü† PARTIEL"
        else:
            status_global = "üî¥ CRITIQUE"
        
        print(f"üìä Score Global: {total_score}/{max_score} ({percentage:.1f}%) - {status_global}")
        
        # D√©tail par composant
        for component, data in self.results.items():
            component_name = {
                'stt': 'STT (Speech-to-Text)',
                'llm': 'LLM (Language Model)', 
                'tts': 'TTS (Text-to-Speech)',
                'integration': 'Int√©gration E2E'
            }[component]
            
            status_icon = {
                'optimal': 'üü¢',
                'functional': 'üü°', 
                'fallback': 'üü†',
                'partial': 'üü†',
                'error': 'üî¥',
                'unknown': '‚ö™'
            }[data['status']]
            
            print(f"\n{status_icon} {component_name}: {data['score']}/100 ({data['status'].upper()})")
            for detail in data['details']:
                print(f"   ‚Ä¢ {detail}")
        
        # Recommandations
        print(f"\nüîß RECOMMANDATIONS:")
        
        if self.results['llm']['status'] == 'fallback':
            print("   ‚Ä¢ R√©soudre connexion Ollama pour vraies r√©ponses LLM")
        
        if self.results['stt']['score'] < 70:
            print("   ‚Ä¢ Optimiser performance STT")
        
        if self.results['tts']['score'] < 70:
            print("   ‚Ä¢ Optimiser performance TTS")
        
        if percentage >= 70:
            print("   ‚Ä¢ Pipeline fonctionnel pour usage production")
        else:
            print("   ‚Ä¢ Corrections n√©cessaires avant usage production")
        
        print(f"\n‚ö° SYST√àME {'OP√âRATIONNEL' if percentage >= 70 else 'N√âCESSITE CORRECTIONS'}")
        print("="*60)

async def main():
    print("üîç SuperWhisper V6 - √âvaluation Status Final")
    print("="*60)
    
    evaluator = PipelineStatusEvaluator()
    
    try:
        # Tests s√©quentiels
        print("\nüé§ 1/4 Test STT...")
        await evaluator.test_stt_component()
        
        print("\nüß† 2/4 Test LLM...")
        await evaluator.test_llm_component()
        
        print("\nüîä 3/4 Test TTS...")
        await evaluator.test_tts_component()
        
        print("\nüîó 4/4 Test Int√©gration...")
        await evaluator.test_integration()
        
        # Rapport final
        evaluator.generate_final_report()
        
    except KeyboardInterrupt:
        print("\nüõë Test interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'√©valuation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())