# üõ†Ô∏è GUIDE D√âVELOPPEMENT GPU RTX 3090 - SUPERWHISPER V6
## Manuel Pratique pour D√©veloppeurs

---

**Projet :** SuperWhisper V6  
**Audience :** √âquipe D√©veloppement  
**Date :** 12/06/2025  
**Version :** 1.0 PRATIQUE  
**Pr√©requis :** [Standards GPU RTX 3090](docs/standards_gpu_rtx3090_definitifs.md)  

---

## üéØ OBJECTIF DE CE GUIDE

Ce guide vous accompagne **√©tape par √©tape** pour d√©velopper des scripts compatibles avec les standards GPU SuperWhisper V6. Apr√®s lecture, vous saurez :

‚úÖ **Int√©grer** la configuration GPU RTX 3090 dans vos scripts  
‚úÖ **Valider** votre code avec les outils fournis  
‚úÖ **√âviter** les erreurs communes  
‚úÖ **Optimiser** les performances GPU  
‚úÖ **Maintenir** la conformit√© standards  

---

## üöÄ D√âMARRAGE RAPIDE - 5 MINUTES

### üìã **Checklist Essentielle**
- [ ] 1. **Copier** le template de configuration GPU
- [ ] 2. **Ajouter** la validation RTX 3090  
- [ ] 3. **Utiliser** `cuda:0` dans votre code
- [ ] 4. **Tester** avec les validateurs fournis
- [ ] 5. **Valider** avant commit Git

### üìã **Template Minimal (Copier-Coller)**
```python
#!/usr/bin/env python3
"""
Votre script SuperWhisper V6
üö® CONFIGURATION GPU: RTX 3090 OBLIGATOIRE
"""

import os
import torch

# Configuration GPU RTX 3090 - OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

print("üéÆ GPU Configuration RTX 3090 activ√©e")

def validate_rtx3090_mandatory():
    """Validation RTX 3090 - OBLIGATOIRE"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
    
    if os.environ.get('CUDA_VISIBLE_DEVICES') != '1':
        raise RuntimeError("üö´ CUDA_VISIBLE_DEVICES incorrect")
    
    if os.environ.get('CUDA_DEVICE_ORDER') != 'PCI_BUS_ID':
        raise RuntimeError("üö´ CUDA_DEVICE_ORDER incorrect") 
    
    gpu_name = torch.cuda.get_device_name(0)
    if "RTX 3090" not in gpu_name:
        raise RuntimeError(f"üö´ GPU d√©tect√©: {gpu_name} - RTX 3090 requise")
    
    print(f"‚úÖ RTX 3090 valid√©e: {gpu_name}")

# Votre code ici
if __name__ == "__main__":
    validate_rtx3090_mandatory()
    
    # Utiliser cuda:0 qui pointe vers RTX 3090
    device = "cuda:0"
    
    # Votre code GPU ici...
    
    print("‚úÖ Script termin√© avec succ√®s")
```

---

## üìö WORKFLOW D√âVELOPPEMENT COMPLET

### üîÑ **√âtape 1 : Setup Initial**

#### 1.1 V√©rifier l'Environnement
```bash
# V√©rifier que RTX 3090 est disponible
nvidia-smi

# V√©rifier PyTorch CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

#### 1.2 R√©cup√©rer les Outils de Validation
```bash
# S'assurer d'avoir les scripts de validation
ls test_gpu_correct.py
ls test_validation_rtx3090_detection.py
ls memory_leak_v4.py
```

### üîÑ **√âtape 2 : D√©veloppement Script**

#### 2.1 Cr√©er le Fichier avec Template
```python
# Utiliser le template minimal ci-dessus
# Remplacer "Votre script SuperWhisper V6" par description r√©elle
# Ajouter vos imports sp√©cifiques apr√®s la configuration GPU
```

#### 2.2 Ajouter Votre Logique M√©tier
```python
# APR√àS validate_rtx3090_mandatory(), ajouter votre code

def votre_fonction_principale():
    """Votre logique m√©tier ici"""
    device = "cuda:0"  # RTX 3090 apr√®s mapping
    
    # Exemple : Chargement mod√®le
    model = torch.nn.Linear(1000, 100).to(device)
    
    # Exemple : Traitement donn√©es
    data = torch.randn(32, 1000, device=device)
    output = model(data)
    
    return output.cpu()  # Retourner sur CPU si besoin

if __name__ == "__main__":
    validate_rtx3090_mandatory()
    
    try:
        result = votre_fonction_principale()
        print(f"‚úÖ Traitement termin√©: {result.shape}")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        raise
```

### üîÑ **√âtape 3 : Tests et Validation**

#### 3.1 Test Initial Script
```bash
# Tester votre script
python votre_script.py

# R√©sultat attendu:
# üéÆ GPU Configuration RTX 3090 activ√©e
# ‚úÖ RTX 3090 valid√©e: NVIDIA GeForce RTX 3090
# ‚úÖ Traitement termin√©: torch.Size([32, 100])
# ‚úÖ Script termin√© avec succ√®s
```

#### 3.2 Validation avec Outils SuperWhisper V6
```bash
# Validation compl√®te
python test_gpu_correct.py

# Validation multi-scripts (inclut votre nouveau script)
python test_validation_rtx3090_detection.py

# Test performance (optionnel)
python test_benchmark_performance_rtx3090.py
```

### üîÑ **√âtape 4 : Optimisation Performance**

#### 4.1 Int√©gration Memory Leak V4.0 (Recommand√©)
```python
# Ajouter apr√®s la configuration GPU de base
try:
    from memory_leak_v4 import gpu_test_cleanup, configure_for_environment
    configure_for_environment("dev")
    memory_leak_protection = True
    print("‚úÖ Memory Leak Prevention activ√©")
except ImportError:
    memory_leak_protection = False
    gpu_test_cleanup = lambda name: lambda func: func

# Utiliser le d√©corateur pour vos fonctions GPU
@gpu_test_cleanup("votre_fonction_principale") if memory_leak_protection else lambda func: func
def votre_fonction_principale():
    # Votre code ici - cleanup automatique
    pass
```

#### 4.2 Monitoring Performance
```python
import time

def benchmark_votre_fonction():
    """Benchmark performance de votre fonction"""
    validate_rtx3090_mandatory()
    
    start_time = time.time()
    
    # Votre fonction
    result = votre_fonction_principale()
    
    duration = time.time() - start_time
    
    # RTX 3090 doit √™tre performante
    if duration > seuil_acceptable:
        print(f"‚ö†Ô∏è Performance potentiellement d√©grad√©e: {duration:.2f}s")
    else:
        print(f"‚úÖ Performance RTX 3090 OK: {duration:.2f}s")
    
    return result
```

### üîÑ **√âtape 5 : Finalisation et Commit**

#### 5.1 Tests Finaux
```bash
# Tests obligatoires avant commit
python votre_script.py
python test_gpu_correct.py
python test_validation_rtx3090_detection.py

# Tous doivent afficher "‚úÖ" pour RTX 3090
```

#### 5.2 Commit Git
```bash
# Commit avec message descriptif
git add votre_script.py
git commit -m "feat(gpu): Nouveau script avec standards RTX 3090 SuperWhisper V6

- Configuration GPU RTX 3090 compl√®te
- Validation obligatoire impl√©ment√©e  
- Tests validateurs r√©ussis
- Conformit√© standards SuperWhisper V6"
```

---

## üîß EXEMPLES PRATIQUES PAR CAS D'USAGE

### üìã **Cas 1 : Module STT (Speech-to-Text)**
```python
#!/usr/bin/env python3
"""
Module STT SuperWhisper V6 avec RTX 3090
üö® CONFIGURATION GPU: RTX 3090 OBLIGATOIRE
"""

import os
import torch
# Configuration GPU RTX 3090 - OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

try:
    import whisper  # ou faster-whisper
except ImportError:
    print("‚ö†Ô∏è Module Whisper non disponible")

def validate_rtx3090_mandatory():
    """Validation RTX 3090 pour STT"""
    # [Code validation standard ici]
    pass

class STTHandler:
    def __init__(self):
        validate_rtx3090_mandatory()
        
        # RTX 3090 apr√®s mapping
        self.device = "cuda:0"
        
        # Charger mod√®le sur RTX 3090
        self.model = whisper.load_model("large-v2", device=self.device)
        
        print(f"‚úÖ STT Handler initialis√© sur RTX 3090")
    
    def transcribe(self, audio_path):
        """Transcription audio sur RTX 3090"""
        result = self.model.transcribe(audio_path)
        return result["text"]

if __name__ == "__main__":
    stt = STTHandler()
    # Tests...
```

### üìã **Cas 2 : Module TTS (Text-to-Speech)**
```python
#!/usr/bin/env python3
"""
Module TTS SuperWhisper V6 avec RTX 3090
üö® CONFIGURATION GPU: RTX 3090 OBLIGATOIRE
"""

import os
import torch
# Configuration GPU RTX 3090 - OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

try:
    from TTS.api import TTS
except ImportError:
    print("‚ö†Ô∏è Module TTS non disponible")

def validate_rtx3090_mandatory():
    """Validation RTX 3090 pour TTS"""
    # [Code validation standard ici]
    pass

class TTSHandler:
    def __init__(self, model_name="tts_models/fr/css10/vits"):
        validate_rtx3090_mandatory()
        
        # RTX 3090 apr√®s mapping
        self.device = "cuda:0"
        
        # Initialiser TTS sur RTX 3090
        self.tts = TTS(model_name=model_name, gpu=True)
        self.tts.to(self.device)
        
        print(f"‚úÖ TTS Handler initialis√© sur RTX 3090")
    
    def synthesize(self, text, output_path="output.wav"):
        """Synth√®se vocale sur RTX 3090"""
        self.tts.tts_to_file(text=text, file_path=output_path)
        return output_path

if __name__ == "__main__":
    tts = TTSHandler()
    # Tests...
```

### üìã **Cas 3 : Module LLM (Large Language Model)**
```python
#!/usr/bin/env python3
"""
Module LLM SuperWhisper V6 avec RTX 3090
üö® CONFIGURATION GPU: RTX 3090 OBLIGATOIRE
"""

import os
import torch
# Configuration GPU RTX 3090 - OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
except ImportError:
    print("‚ö†Ô∏è Module Transformers non disponible")

def validate_rtx3090_mandatory():
    """Validation RTX 3090 pour LLM"""
    # [Code validation standard ici]
    pass

class LLMHandler:
    def __init__(self, model_name="microsoft/DialoGPT-medium"):
        validate_rtx3090_mandatory()
        
        # RTX 3090 apr√®s mapping
        self.device = "cuda:0"
        
        # Charger mod√®le sur RTX 3090
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.model.to(self.device)
        
        print(f"‚úÖ LLM Handler initialis√© sur RTX 3090")
    
    def generate_response(self, input_text, max_length=100):
        """G√©n√©ration r√©ponse sur RTX 3090"""
        inputs = self.tokenizer.encode(input_text, return_tensors='pt').to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                inputs, 
                max_length=max_length,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response

if __name__ == "__main__":
    llm = LLMHandler()
    # Tests...
```

### üìã **Cas 4 : Script de Test/Validation**
```python
#!/usr/bin/env python3
"""
Script de test SuperWhisper V6 avec RTX 3090
üö® CONFIGURATION GPU: RTX 3090 OBLIGATOIRE
"""

import os
import torch
import unittest
# Configuration GPU RTX 3090 - OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

def validate_rtx3090_mandatory():
    """Validation RTX 3090 pour tests"""
    # [Code validation standard ici]
    pass

class TestGPUConfiguration(unittest.TestCase):
    def setUp(self):
        """Setup tests avec validation RTX 3090"""
        validate_rtx3090_mandatory()
        self.device = "cuda:0"  # RTX 3090 apr√®s mapping
    
    def test_gpu_available(self):
        """Test GPU disponible"""
        self.assertTrue(torch.cuda.is_available())
    
    def test_rtx3090_detected(self):
        """Test RTX 3090 d√©tect√©e"""
        gpu_name = torch.cuda.get_device_name(0)
        self.assertIn("RTX 3090", gpu_name)
    
    def test_memory_sufficient(self):
        """Test m√©moire GPU suffisante"""
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        self.assertGreater(gpu_memory, 20)  # RTX 3090 = 24GB
    
    def test_tensor_operations(self):
        """Test op√©rations tensor sur RTX 3090"""
        x = torch.randn(100, 100, device=self.device)
        y = torch.matmul(x, x.t())
        self.assertEqual(y.device.type, 'cuda')

if __name__ == "__main__":
    print("üß™ Tests GPU SuperWhisper V6")
    unittest.main()
```

---

## ‚ö†Ô∏è R√âSOLUTION PROBL√àMES COURANTS

### üö® **Erreur : "RTX 3090 non d√©tect√©e"**

#### **Diagnostic**
```python
# Script diagnostic rapide
import torch
print(f"CUDA disponible: {torch.cuda.is_available()}")
print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
print(f"CUDA_DEVICE_ORDER: {os.environ.get('CUDA_DEVICE_ORDER')}")
if torch.cuda.is_available():
    print(f"GPU d√©tect√©e: {torch.cuda.get_device_name(0)}")
```

#### **Solutions**
1. **V√©rifier configuration environnement**
   ```python
   # Ajouter AVANT imports torch
   os.environ['CUDA_VISIBLE_DEVICES'] = '1'
   os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
   ```

2. **V√©rifier ordre dans le script**
   ```python
   # ‚úÖ CORRECT - Configuration AVANT import torch
   import os
   os.environ['CUDA_VISIBLE_DEVICES'] = '1'
   import torch
   
   # ‚ùå INCORRECT - Configuration APR√àS import torch
   import torch
   os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # Trop tard !
   ```

3. **Red√©marrer Python/Kernel**
   ```bash
   # Red√©marrer compl√®tement l'environnement Python
   exit()  # Quitter Python
   python votre_script.py  # Relancer
   ```

### üö® **Erreur : "CUDA out of memory"**

#### **Solutions**
1. **Optimiser allocation m√©moire**
   ```python
   # Ajouter optimisation m√©moire
   os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'
   
   # Cleanup p√©riodique
   torch.cuda.empty_cache()
   ```

2. **R√©duire taille batch**
   ```python
   # R√©duire taille des tensors
   batch_size = 16  # Au lieu de 32 ou 64
   ```

3. **Monitoring m√©moire**
   ```python
   def check_gpu_memory():
       if torch.cuda.is_available():
           allocated = torch.cuda.memory_allocated(0) / 1024**3
           cached = torch.cuda.memory_reserved(0) / 1024**3
           print(f"GPU M√©moire - Allou√©e: {allocated:.2f}GB, Cache: {cached:.2f}GB")
   ```

### üö® **Erreur : "Module not found"**

#### **Solutions**
1. **Imports conditionnels**
   ```python
   try:
       import whisper
   except ImportError:
       print("‚ö†Ô∏è Whisper non disponible - Tests limit√©s")
       whisper = None
   
   # Utiliser avec v√©rification
   if whisper is not None:
       model = whisper.load_model("base")
   ```

2. **Installation d√©pendances**
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   pip install openai-whisper
   pip install TTS
   ```

---

## üîç VALIDATION ET DEBUGGING

### üìã **Checklist Debug √âtape par √âtape**

#### **√âtape 1 : Configuration GPU**
- [ ] `CUDA_VISIBLE_DEVICES='1'` d√©fini AVANT import torch
- [ ] `CUDA_DEVICE_ORDER='PCI_BUS_ID'` d√©fini
- [ ] `torch.cuda.is_available()` retourne `True`
- [ ] `torch.cuda.get_device_name(0)` contient "RTX 3090"

#### **√âtape 2 : Code GPU**
- [ ] Utilisation `device = "cuda:0"` ou `device = "cuda"`
- [ ] Tensors/mod√®les bien transf√©r√©s sur GPU avec `.to(device)`
- [ ] Pas d'utilisation `cuda:1` apr√®s mapping

#### **√âtape 3 : Performance**
- [ ] Pas de memory leak d√©tect√©
- [ ] Performance conforme aux benchmarks RTX 3090
- [ ] Cleanup m√©moire apr√®s usage

#### **√âtape 4 : Tests**
- [ ] Script fonctionne en standalone
- [ ] Validation `test_gpu_correct.py` r√©ussie
- [ ] Validation `test_validation_rtx3090_detection.py` r√©ussie

### üìã **Scripts Debug Personnalis√©s**

#### **Script Debug Minimal**
```python
#!/usr/bin/env python3
"""Debug GPU Configuration SuperWhisper V6"""

import os
# Configuration GPU AVANT import torch
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

import torch

def debug_gpu_config():
    print("üîç DEBUG GPU CONFIGURATION")
    print("=" * 40)
    
    print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
    print(f"CUDA_DEVICE_ORDER: {os.environ.get('CUDA_DEVICE_ORDER')}")
    print(f"CUDA Available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"Device Count: {torch.cuda.device_count()}")
        print(f"Current Device: {torch.cuda.current_device()}")
        print(f"Device Name: {torch.cuda.get_device_name(0)}")
        
        props = torch.cuda.get_device_properties(0)
        print(f"Total Memory: {props.total_memory / 1024**3:.1f}GB")
        print(f"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.1f}GB")
        
        # Test simple
        x = torch.randn(100, 100, device="cuda:0")
        y = torch.matmul(x, x.t())
        print(f"Test Tensor OK: {y.shape}")
        
        if "RTX 3090" in torch.cuda.get_device_name(0):
            print("‚úÖ RTX 3090 D√âTECT√âE ET FONCTIONNELLE")
        else:
            print("‚ùå RTX 3090 NON D√âTECT√âE")
    else:
        print("‚ùå CUDA NON DISPONIBLE")

if __name__ == "__main__":
    debug_gpu_config()
```

---

## üìä BONNES PRATIQUES PERFORMANCE

### üéØ **Optimisations RTX 3090**

#### **1. Gestion M√©moire Optimale**
```python
# Configuration m√©moire optimis√©e pour RTX 3090
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

# Cleanup p√©riodique
def cleanup_gpu_memory():
    torch.cuda.empty_cache()
    torch.cuda.synchronize()

# Monitoring m√©moire
def monitor_gpu_memory(label=""):
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"{label} - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
```

#### **2. Batch Sizes Optimaux RTX 3090**
```python
# Tailles recommand√©es pour RTX 3090 (24GB)
OPTIMAL_BATCH_SIZES = {
    "whisper-large": 8,   # STT
    "tts-models": 16,     # TTS  
    "llm-7b": 4,          # LLM 7B
    "llm-13b": 2,         # LLM 13B
    "general": 32         # Op√©rations g√©n√©rales
}

def get_optimal_batch_size(task_type="general"):
    return OPTIMAL_BATCH_SIZES.get(task_type, 16)
```

#### **3. Context Managers pour S√©curit√©**
```python
from contextlib import contextmanager

@contextmanager
def rtx3090_context():
    """Context manager s√©curis√© pour RTX 3090"""
    validate_rtx3090_mandatory()
    
    initial_memory = torch.cuda.memory_allocated(0)
    
    try:
        yield "cuda:0"
    finally:
        # Cleanup automatique
        torch.cuda.empty_cache()
        
        final_memory = torch.cuda.memory_allocated(0)
        if final_memory > initial_memory + 100*1024*1024:  # 100MB tolerance
            print(f"‚ö†Ô∏è Possible memory leak: {(final_memory-initial_memory)/1024**3:.2f}GB")

# Utilisation
with rtx3090_context() as device:
    model = torch.nn.Linear(1000, 100).to(device)
    # Votre code ici
    # Cleanup automatique
```

---

## üìö RESSOURCES ET R√âF√âRENCES

### üìã **Documentation SuperWhisper V6**
- [Standards GPU RTX 3090 D√©finitifs](docs/standards_gpu_rtx3090_definitifs.md)
- [Journal Phase 4 Validation](docs/journal_phase4_validation.md)
- [Suivi Mission GPU](docs/suivi_mission_gpu.md)

### üìã **Scripts de Validation**
- `test_gpu_correct.py` - Validateur universel
- `test_validation_rtx3090_detection.py` - Validation multi-scripts
- `test_integration_gpu_rtx3090.py` - Tests int√©gration
- `test_workflow_stt_llm_tts_rtx3090.py` - Pipeline complet
- `test_benchmark_performance_rtx3090.py` - Benchmarks performance
- `memory_leak_v4.py` - Prevention memory leaks

### üìã **Commandes Utiles**
```bash
# Validation avant commit
python test_gpu_correct.py
python test_validation_rtx3090_detection.py

# Debug GPU
nvidia-smi
python -c "import torch; print(torch.cuda.get_device_name(0))"

# Monitoring en temps r√©el
watch -n 1 nvidia-smi
```

---

## üéØ CONCLUSION

Ce guide vous donne tous les outils pour d√©velopper efficacement avec les standards GPU SuperWhisper V6. **Points cl√©s √† retenir :**

‚úÖ **Toujours** utiliser le template de configuration GPU  
‚úÖ **Syst√©matiquement** valider RTX 3090 avec `validate_rtx3090_mandatory()`  
‚úÖ **Uniquement** utiliser `cuda:0` apr√®s mapping  
‚úÖ **Obligatoirement** tester avec les validateurs fournis  
‚úÖ **Maintenir** les performances RTX 3090  

**En cas de doute :** Consultez les exemples pratiques ou utilisez les scripts de debug fournis.

**Support :** Documentation compl√®te + scripts validation + √©quipe SuperWhisper V6

---

**üéØ AVEC CE GUIDE, VOUS MA√éTRISEZ LES STANDARDS GPU SUPERWHISPER V6 !**  
**üöÄ D√âVELOPPEMENT EFFICACE + PERFORMANCE RTX 3090 + CONFORMIT√â GARANTIE**

---

*Guide cr√©√© le 12/06/2025 par l'√©quipe Mission GPU SuperWhisper V6*  
*Version 1.0 PRATIQUE - Pour d√©veloppeurs* 