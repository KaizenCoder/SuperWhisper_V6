#!/usr/bin/env python3
"""
Script de validation LLM : nous-hermes-2-mistral-7b-dpo
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import asyncio
import httpx
import json
import time
from datetime import datetime

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

import torch

def validate_rtx3090_configuration():
    """Validation obligatoire de la configuration RTX 3090"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 = ~24GB
        raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
    
    print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")

class LLMValidator:
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.model_name = "nous-hermes-2-mistral-7b-dpo:latest"
        self.test_prompts = [
            "Salut ! Comment √ßa va ?",
            "Peux-tu me dire l'heure qu'il est ?",
            "Raconte-moi une blague courte.",
            "Quel est le sens de la vie ?",
            "Parle-moi de l'intelligence artificielle."
        ]
        
    async def test_model_availability(self):
        """V√©rifier que le mod√®le est disponible dans Ollama"""
        print(f"\nüîç V√©rification disponibilit√© mod√®le: {self.model_name}")
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                
                if response.status_code == 200:
                    models_data = response.json()
                    available_models = [model["name"] for model in models_data["models"]]
                    
                    print(f"üìã Mod√®les disponibles: {len(available_models)}")
                    for model in available_models:
                        print(f"  - {model}")
                    
                    if self.model_name in available_models:
                        print(f"‚úÖ Mod√®le {self.model_name} trouv√© !")
                        return True
                    else:
                        print(f"‚ùå Mod√®le {self.model_name} non trouv√©")
                        return False
                else:
                    print(f"‚ùå Erreur API tags: {response.status_code}")
                    return False
                    
        except Exception as e:
            print(f"‚ùå Erreur connexion Ollama: {e}")
            return False
    
    async def test_chat_generation(self, prompt: str):
        """Tester g√©n√©ration de r√©ponse pour un prompt"""
        print(f"\nüí¨ Test prompt: \"{prompt}\"")
        
        payload = {
            "model": self.model_name,
            "messages": [
                {
                    "role": "system", 
                    "content": "Tu es un assistant IA conversationnel en fran√ßais. R√©ponds de fa√ßon naturelle et concise."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 50,  # R√©ponses courtes pour tests
                "top_p": 0.9
            }
        }
        
        start_time = time.time()
        
        try:
            async with httpx.AsyncClient(timeout=45.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/chat",
                    json=payload
                )
                
                latency = (time.time() - start_time) * 1000
                
                if response.status_code == 200:
                    data = response.json()
                    generated_text = data["message"]["content"]
                    
                    # Calculer m√©triques
                    word_count = len(generated_text.split())
                    char_count = len(generated_text)
                    
                    print(f"‚úÖ R√©ponse g√©n√©r√©e ({latency:.1f}ms)")
                    print(f"üìù Contenu: \"{generated_text[:100]}{'...' if len(generated_text) > 100 else ''}\"")
                    print(f"üìä M√©triques: {word_count} mots, {char_count} caract√®res")
                    
                    # √âvaluation qualit√© simple
                    quality_score = self.evaluate_response_quality(prompt, generated_text)
                    print(f"‚≠ê Qualit√©: {quality_score}/10")
                    
                    return {
                        "success": True,
                        "response": generated_text,
                        "latency_ms": latency,
                        "word_count": word_count,
                        "char_count": char_count,
                        "quality_score": quality_score
                    }
                else:
                    print(f"‚ùå Erreur g√©n√©ration: {response.status_code}")
                    print(f"üìã D√©tails: {response.text}")
                    return {"success": False, "error": f"HTTP {response.status_code}"}
                    
        except Exception as e:
            latency = (time.time() - start_time) * 1000
            print(f"‚ùå Exception g√©n√©ration ({latency:.1f}ms): {e}")
            return {"success": False, "error": str(e), "latency_ms": latency}
    
    def evaluate_response_quality(self, prompt: str, response: str) -> int:
        """√âvaluation simple de la qualit√© de la r√©ponse (0-10)"""
        score = 5  # Score de base
        
        # Crit√®res positifs
        if len(response.strip()) > 10:  # R√©ponse non vide
            score += 1
        if any(word in response.lower() for word in ["bonjour", "salut", "merci", "oui", "non"]):
            score += 1
        if len(response.split()) >= 3:  # Au moins 3 mots
            score += 1
        if response.endswith(('.', '!', '?')):  # Ponctuation correcte
            score += 1
        if not response.isupper():  # Pas tout en majuscules
            score += 1
        
        # Crit√®res n√©gatifs
        if "sorry" in response.lower() or "d√©sol√©" in response.lower():
            score -= 1
        if len(response) < 5:  # R√©ponse trop courte
            score -= 2
        if response.count("?") > 2:  # Trop de questions
            score -= 1
            
        return max(0, min(10, score))
    
    async def run_validation(self):
        """Ex√©cuter validation compl√®te du mod√®le LLM"""
        print("üöÄ VALIDATION LLM HERMES-2-MISTRAL-7B-DPO")
        print("=" * 50)
        
        # V√©rifier mod√®le disponible
        if not await self.test_model_availability():
            print("üö´ Validation √©chou√©e: mod√®le non disponible")
            return False
        
        # Tests g√©n√©ration
        print(f"\nüß™ Tests g√©n√©ration sur {len(self.test_prompts)} prompts")
        results = []
        
        for i, prompt in enumerate(self.test_prompts, 1):
            print(f"\n--- Test {i}/{len(self.test_prompts)} ---")
            result = await self.test_chat_generation(prompt)
            results.append(result)
            
            # Pause entre tests
            await asyncio.sleep(1)
        
        # Rapport final
        self.generate_report(results)
        
        return True
    
    def generate_report(self, results):
        """G√©n√©rer rapport de validation"""
        print("\n" + "=" * 50)
        print("üìä RAPPORT VALIDATION LLM")
        print("=" * 50)
        
        successful_tests = [r for r in results if r.get("success", False)]
        failed_tests = [r for r in results if not r.get("success", False)]
        
        print(f"‚úÖ Tests r√©ussis: {len(successful_tests)}/{len(results)} ({len(successful_tests)/len(results)*100:.1f}%)")
        print(f"‚ùå Tests √©chou√©s: {len(failed_tests)}/{len(results)} ({len(failed_tests)/len(results)*100:.1f}%)")
        
        if successful_tests:
            latencies = [r["latency_ms"] for r in successful_tests]
            qualities = [r["quality_score"] for r in successful_tests]
            
            print(f"\nüìà M√©triques performance:")
            print(f"  - Latence moyenne: {sum(latencies)/len(latencies):.1f}ms")
            print(f"  - Latence P95: {sorted(latencies)[int(len(latencies)*0.95)]:.1f}ms")
            print(f"  - Qualit√© moyenne: {sum(qualities)/len(qualities):.1f}/10")
            
            # Objectif latence < 400ms pour total < 1.2s
            avg_latency = sum(latencies)/len(latencies)
            if avg_latency < 400:
                print(f"üéØ OBJECTIF LATENCE ATTEINT: {avg_latency:.1f}ms < 400ms")
            else:
                print(f"‚ö†Ô∏è OBJECTIF LATENCE MANQU√â: {avg_latency:.1f}ms > 400ms")
        
        if failed_tests:
            print(f"\n‚ö†Ô∏è Erreurs d√©tect√©es:")
            for i, test in enumerate(failed_tests):
                print(f"  - Test {i+1}: {test.get('error', 'Erreur inconnue')}")
        
        print(f"\nüìù Validation termin√©e: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

async def main():
    """Fonction principale"""
    try:
        # Validation GPU obligatoire
        validate_rtx3090_configuration()
        
        # Validation LLM
        validator = LLMValidator()
        success = await validator.run_validation()
        
        if success:
            print("\n‚úÖ VALIDATION LLM R√âUSSIE")
            return 0
        else:
            print("\n‚ùå VALIDATION LLM √âCHOU√âE")
            return 1
            
    except Exception as e:
        print(f"\nüö´ ERREUR VALIDATION: {e}")
        return 1

if __name__ == "__main__":
    # Validation RTX 3090 au d√©marrage
    validate_rtx3090_configuration()
    
    # Ex√©cution validation
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 