#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test de détection CUDA avec PyTorch
🚨 CONFIGURATION GPU: RTX 3090 via CUDA_VISIBLE_DEVICES='1'
🚨 RTX 5060 Ti (CUDA:0 physique) MASQUÉE
"""

import os
import torch

# =============================================================================
# 🚨 CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
# CUDA_VISIBLE_DEVICES='1' masque RTX 5060 Ti et rend RTX 3090 visible comme device 0
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU

print("🔒 CUDA_VISIBLE_DEVICES:", os.environ.get('CUDA_VISIBLE_DEVICES'))
print("🚨 RTX 5060 Ti MASQUÉE / RTX 3090 devient device 0 visible")
print("=== TEST RTX 3090 EXCLUSIF ===")
print(f"🎯 CUDA disponible: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    device_count = torch.cuda.device_count()
    print(f"🔥 Nombre de GPU visibles: {device_count}")
    
    for i in range(device_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"\n   GPU {i}: {gpu_name}")
        print(f"   Mémoire: {gpu_memory:.1f} GB")
        
        # Validation RTX 3090 exclusive
        if "RTX 3090" in gpu_name and gpu_memory >= 20:
            print(f"   ✅ RTX 3090 confirmée sur device {i}")
        elif "RTX 5060" in gpu_name:
            print(f"   🚫 RTX 5060 Ti détectée - DEVRAIT ÊTRE MASQUÉE!")
        
        # Test d'allocation sur RTX 3090
        if "RTX 3090" in gpu_name:
            try:
                torch.cuda.set_device(i)
                x = torch.randn(3000, 3000).cuda()  # Test 36MB sur RTX 3090
                print(f"   ✅ Allocation 36MB RTX 3090 réussie!")
                print(f"   📊 Tensor sur: {x.device}")
                del x
                torch.cuda.empty_cache()
            except Exception as e:
                print(f"   ❌ Erreur allocation RTX 3090: {e}")
    
    print(f"\n🎯 Version CUDA: {torch.version.cuda}")
    print(f"🎯 GPU courant: {torch.cuda.current_device()}")

else:
    print("❌ CUDA non disponible")

print("\n" + "="*50) 