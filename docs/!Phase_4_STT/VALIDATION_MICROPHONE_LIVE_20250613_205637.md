# üÜò **AIDE EXTERNE - VALIDATION MICROPHONE LIVE PHASE 4 STT - TRANSCRIPTION VIDE MALGR√â ARCHITECTURE FONCTIONNELLE**

**Date** : 13 June 2025 - 20:56  
**Probl√®me** : Validation Microphone Live Phase 4 STT - Transcription vide malgr√© architecture fonctionnelle  
**Urgence** : **CRITIQUE**  
**SuperWhisper V6** - Phase 4 STT  

---

## üéØ **CONTEXTE**

Architecture STT compl√®te et op√©rationnelle sur fichiers audio (148/138 mots, RTF 0.082), mais transcription vide lors tests microphone live malgr√© enregistrement audio correct. Backend prism_primary initialis√© mais √©chec transcription temps r√©el.

---

## üîß **CODE ESSENTIEL ACTUEL**


### **1. Script Validation - Point d'√âchec**

```python
# scripts/validation_microphone_live_equipe.py
#!/usr/bin/env python3
"""
"""
import os
import sys
import time
import json
from datetime import datetime
from pathlib import Path
def validate_rtx3090_validation():
    """Validation syst√©matique RTX 3090 pour √©quipe validation"""
    print("\nüîç VALIDATION CONFIGURATION RTX 3090")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA non disponible - RTX 3090 requise")
        return False
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        print(f"‚ùå CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    if "3090" not in gpu_name:
        print(f"‚ùå GPU d√©tect√©e: {gpu_name} - RTX 3090 requise")
        return False
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:
        print(f"‚ùå GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
        return False
    
    print(f"‚úÖ RTX 3090 valid√©e: {gpu_name} ({gpu_memory:.1f}GB)")
    return True

def test_microphone_setup():
    """Test setup microphone avec s√©lection automatique RODE NT-USB"""
    print("\nüé§ TEST SETUP MICROPHONE")
    print("=" * 30)
    
    try:
        # Lister devices audio
        devices = sd.query_devices()
        print("üìã Devices audio disponibles:")
        
        # Chercher TOUS les microphones RODE NT-USB
        rode_devices = []
        input_devices = []
        
        for i, device in enumerate(devices):
            if device['max_input_channels'] > 0:
                print(f"   {i}: {device['name']} (Input: {device['max_input_channels']} ch)")
                input_devices.append((i, device['name']))
                
                # D√©tecter TOUTES les instances RODE NT-USB
                if "RODE NT-USB" in device['name']:
                    rode_devices.append(i)
                    print(f"   üéØ RODE NT-USB d√©tect√©: Device {i}")
        
        # Tester chaque instance RODE NT-USB pour trouver celle qui fonctionne
        selected_device = None
        
        if rode_devices:
            print(f"\nüîç Test de {len(rode_devices)} instances RODE NT-USB...")
            
            for device_id in rode_devices:
                print(f"\nüß™ Test Device {device_id}...")
                try:
                    # Test rapide 1 seconde
                    test_audio = sd.rec(int(1 * 16000), samplerate=16000, channels=1, dtype=np.float32, device=device_id)
                    sd.wait()
                    
                    # V√©rifier si l'enregistrement a fonctionn√©
                    max_level = np.max(np.abs(test_audio))
                    if max_level > 0.001:  # Seuil tr√®s bas pour d√©tecter activit√©
                        print(f"‚úÖ Device {device_id} fonctionnel (niveau: {max_level:.6f})")
                        selected_device = device_id
                        break
                    else:
                        print(f"‚ö†Ô∏è Device {device_id} silencieux (niveau: {max_level:.6f})")
                        
                except Exception as e:
                    print(f"‚ùå Device {device_id} erreur: {e}")
                    continue
            
            if selected_device is None:
                print("‚ö†Ô∏è Aucune instance RODE NT-USB fonctionnelle trouv√©e")
                # Fallback sur le premier device RODE trouv√©
                selected_device = rode_devices[0]
                print(f"üîÑ Utilisation Device {selected_device} par d√©faut")
            else:
                print(f"\n‚úÖ S√©lection automatique: RODE NT-USB (Device {selected_device})")
                
        else:
            print(f"\n‚ö†Ô∏è RODE NT-USB non trouv√©, s√©lection manuelle requise")
            print("üìã Microphones d'entr√©e disponibles:")
            for i, (device_id, name) in enumerate(input_devices):
                print(f"   {i}: Device {device_id} - {name}")
            
            while True:
                try:
                    choice = int(input("üéØ S√©lectionnez le num√©ro du microphone √† utiliser: "))
                    if 0 <= choice < len(input_devices):
                        selected_device = input_devices[choice][0]
                        break
                    else:
                        print("‚ùå Num√©ro invalide")
                except ValueError:
                    print("‚ùå Veuillez entrer un num√©ro")
        
        print(f"üé§ Microphone s√©lectionn√©: Device {selected_device}")
        
        # Test enregistrement final avec microphone s√©lectionn√©
        print(f"\nüî¥ Test enregistrement 3 secondes avec Device {selected_device}...")
        print("   Parlez fort et clairement maintenant...")
        
        # Enregistrement avec device sp√©cifique
        audio = sd.rec(int(3 * 16000), samplerate=16000, channels=1, dtype=np.float32, device=selected_device)
        sd.wait()
        
        # V√©rifier niveau audio
        max_level = np.max(np.abs(audio))
        rms_level = n

    # ... (code tronqu√© pour lisibilit√©)
```

### **2. UnifiedSTTManager - Architecture Principale**

```python
# STT/unified_stt_manager.py
#!/usr/bin/env python3
"""
"""
import os
import sys
from typing import Dict, Any, Optional, List
import asyncio
import time
import hashlib
import numpy as np
import torch
from contextlib import asynccontextmanager
from dataclasses import dataclass
from collections import OrderedDict
def validate_rtx3090_mandatory():
    """Validation syst√©matique RTX 3090 pour STT"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise pour STT")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 = ~24GB
        raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
    
    gpu_name = torch.cuda.get_device_name(0)
    print(f"‚úÖ RTX 3090 valid√©e pour STT: {gpu_name} ({gpu_memory:.1f}GB)")

# Import des backends apr√®s configuration GPU
class STTResult:
    """R√©sultat de transcription STT"""
    text: str
    confidence: float
    segments: List[dict]
    processing_time: float
    device: str
    rtf: float
    backend_used: str
    success: bool
    cached: bool = False
    error: Optional[str] = None

class STTCache:
    """Cache LRU pour r√©sultats STT avec TTL"""
    
    def __init__(self, max_size: int = 200*1024*1024, ttl: int = 7200):
        """
        Initialise le cache LRU.
        
        Args:
            max_size: Taille maximale en bytes (d√©faut: 200MB)
            ttl: Dur√©e de vie des entr√©es en secondes (d√©faut: 2h)
        """
        self.max_size = max_size
        self.ttl = ttl
        self.cache = OrderedDict()  # {key: (value, timestamp, size)}
        self.current_size = 0
        self.hits = 0
        self.misses = 0
    
    def get(self, key: str) -> Optional[STTResult]:
        """R√©cup√®re une valeur du cache avec gestion TTL"""
        if key in self.cache:
            value, timestamp, _ = self.cache[key]
            
            # V√©rifier TTL
            if time.time() - timestamp > self.ttl:
                self._remove(key)
                self.misses += 1
                return None
            
            # Hit - d√©placer en fin de LRU
            self.cache.move_to_end(key)
            self.hits += 1
            return value
        
        self.misses += 1
        return None
    
    def put(self, key: str, value: STTResult):
        """Ajoute une valeur au cache avec √©viction LRU si n√©cessaire"""
        # Estimer taille (approximation avec s√©rialisation)
        estimated_size = len(str(value).encode('utf-8'))
        
        # V√©rifier si la valeur peut rentrer
        if estimated_size > self.max_size:
            return  # Trop grande pour le cache
        
        # √âviction LRU si n√©cessaire
        while self.current_size + estimated_size > self.max_size and self.cache:
            self._remove_lru()
        
        # Ajouter nouvelle entr√©e
        self.cache[key] = (value, time.time(), estimated_size)
        self.current_size += estimated_size
        self.cache.move_to_end(key)  # D√©placer en fin de LRU
    
    def _remove(self, key: str):
        """Supprime une entr√©e du cache"""
        if key in self.cache:
            _, _, size = self.cache[key]
            self.current_size -= size
            del self.cache[key]
    
    def _remove_lru(self):
        """Supprime l'entr√©e la moins r√©cemment utilis√©e"""
        if self.cache:
            key = next(iter(self.cache))  # Premier √©l√©ment (LRU)
            self._remove(key)

class CircuitBreaker:
    """Protection contre les √©checs en cascade"""
    
    def __init__(self, failure_threshold: int = 3, recovery_timeout: int = 30):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failures = 0
        self.last_failure_time = 0
        self.state = "closed"  # closed, open, half-open
    
    def record_failure(self):
        """Enregistre un √©chec"""
        self.failures += 1
        self.last_failure_time = time.time()
        
        if self.failures >= self.failure_threshold:
            self.state = "open"
    
    def record_success(self):
        """Enregistre un succ√®s"""
        self.failures = 0
        self.state = "closed"
    
    def is_open(self) -> bool:
        """V√©rifie si le circuit est ouvert"""
        if self.state == "open":
            # V√©rifier si on peut passer en half-open
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = "half-open"
                return False
            return True
        return False

class PrometheusSTTMetrics:
    """M√©triques Prometheus pour STT"""
    
    def __init__(self):
        try:
            self.transcriptions_total = Counter('stt_transcriptions_total', 'Total STT transcriptions', ['backend', 'status

    # ... (code tronqu√© pour lisibilit√©)
```

### **3. Backend Prism Stt Backend**

```python
# STT/backends/prism_stt_backend.py
#!/usr/bin/env python3
"""
"""
import os
import sys
import time
import asyncio
import numpy as np
import logging
from typing import Dict, Any, Optional
from pathlib import Path
class PrismSTTBackend(BaseSTTBackend):
    """
    Backend STT Prism_Whisper2 optimis√© RTX 3090 - SuperWhisper V6
    
    Bas√© sur l'analyse de Prism_Whisper2 avec optimisations SuperWhisper V6:
    - faster-whisper avec compute_type="float16" 
    - GPU Memory Optimizer int√©gr√©
    - Cache mod√®les intelligent
    - Performance cible < 400ms pour 5s audio
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialise le backend Prism STT
        
        Args:
            config: Configuration avec model_size, compute_type, etc.
        """
        super().__init__(config)
        
        # Configuration Prism
        self.model_size = config.get('model', 'large-v2')
        self.compute_type = config.get('compute_type', 'float16')
        self.language = config.get('language', 'fr')
        self.beam_size = config.get('beam_size', 5)
        self.vad_filter = config.get('vad_filter', True)  # üîß VAD avec param√®tres corrig√©s pour transcription compl√®te
        
        # Mod√®le Whisper
        self.model = None
        self.model_loaded = False
        
        # Optimisations m√©moire (inspir√© Prism_Whisper2)
        self.memory_optimizer = None
        self.pinned_buffers = []
        
        # M√©triques sp√©cifiques Prism
        self.model_load_time = 0.0
        self.warm_up_completed = False
        
        self.logger = self._setup_logging()
        
        # Initialisation
        self._initialize_prism_backend()
    
    def _setup_logging(self) -> logging.Logger:
        """Setup logging pour Prism backend"""
        logger = logging.getLogger(f'PrismSTTBackend_{self.model_size}')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - Prism - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _initialize_prism_backend(self):
        """Initialise le backend Prism avec optimisations RTX 3090"""
        try:
            self.logger.info(f"üöÄ Initialisation Prism STT {self.model_size} sur RTX 3090...")
            
            # Validation GPU obligatoire
            validate_rtx3090_mandatory()
            
            # Chargement du mod√®le depuis le pool partag√©
            start_time = time.time()
            self.model = model_pool.get_model(self.model_size, self.compute_type)
            
            if self.model is None:
                raise RuntimeError(f"Impossible de charger le mod√®le '{self.model_size}' depuis le pool.")

            self.model_load_time = time.time() - start_time
            self.model_loaded = True
            
            self.logger.info(f"‚úÖ Mod√®le '{self.model_size}' obtenu depuis le pool en {self.model_load_time:.2f}s")
            
            # Warm-up GPU avec audio test (inspir√© Prism_Whisper2)
            self._warm_up_model()
            
            # Initialiser optimiseur m√©moire
            self._initialize_memory_optimizer()
            
            self.logger.info("üé§ Backend Prism STT pr√™t sur RTX 3090")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation Prism: {e}")
            raise RuntimeError(f"√âchec initialisation PrismSTTBackend: {e}")
    
    def _warm_up_model(self):
        """Warm-up mod√®le avec audio test (inspir√© Prism_Whisper2)"""
        try:
            self.logger.info("üî• Warm-up mod√®le Prism...")
            
            # Audio test 3 secondes (comme dans Prism_Whisper2)
            dummy_audio = np.zeros(16000 * 3, dtype=np.float32)
            
            # 3 passes de warm-up
            for i in range(3):
                start_time = time.time()
                segments, _ = self.model.transcribe(
                    dummy_audio,
                    language=self.language,
                    beam_size=self.beam_size,
                    vad_filter=self.vad_filter
                )
                # Consommer les segments pour forcer l'ex√©cution
                list(segments)
                
                warm_up_time = time.time() - start_time
                self.logger.info(f"   Warm-up {i+1}/3: {warm_up_time:.3f}s")
            
            self.warm_up_completed = True
            self.logger.info("‚úÖ Warm-up Prism termin√©")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Warm-up √©chou√©: {e}")
    
    def _initialize_memory_optimizer(self):
        """Initialise optimiseur m√©moire (inspir√© Prism_Whisper2)"""
        try:
            # Pr√©-allocation buffers pinned pour audio
            buffer_sizes = [16000 * 1, 16000 * 3, 16000 * 5, 16000 * 10]  # 1s, 3s, 5s, 10s
            
            for size in buffer_sizes:
      

    # ... (code tronqu√© pour lisibilit√©)
```

---

## üîç **PROBL√àME IDENTIFI√â**

### **Zones Critiques**
1. **Architecture/Pipeline** : Analyse du flow de donn√©es
2. **Performance** : Goulots d'√©tranglement identifi√©s  
3. **Configuration** : Param√®tres optimaux manquants
4. **Int√©gration** : Probl√®mes de coordination modules

---

## üÜò **AIDE DEMAND√âE**

### **Solution Compl√®te Attendue**
- **Code fonctionnel imm√©diatement op√©rationnel**
- **Configuration optimale pour environnement**
- **Documentation int√©gration**
- **Plan r√©solution √©tape par √©tape**

### **Contraintes Techniques**
- **GPU** : RTX 3090 24GB exclusif (CUDA:1)
- **OS** : Windows 10 PowerShell 7
- **Python** : 3.12 avec d√©pendances existantes
- **Performance** : Maintenir niveau actuel

---

**üö® R√âPONSE EXHAUSTIVE DEMAND√âE AVEC CODE COMPLET !**
