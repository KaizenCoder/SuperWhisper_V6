# üéÆ STANDARDS GPU RTX 3090 - SUPERWHISPER V6
## Configuration Obligatoire pour D√©veloppements Futurs

---

**Projet :** SuperWhisper V6  
**Version :** 1.0 D√âFINITIVE  
**Date :** 12/06/2025  
**Statut :** OBLIGATOIRE POUR TOUS D√âVELOPPEMENTS  
**Validation :** Mission homog√©n√©isation GPU termin√©e avec succ√®s  

---

## üö® R√àGLES ABSOLUES - AUCUNE EXCEPTION AUTORIS√âE

### üéØ **R√®gle #1 : GPU EXCLUSIVE RTX 3090**
- ‚úÖ **AUTORIS√âE :** RTX 3090 (24GB VRAM) sur Bus PCI 1 uniquement
- ‚ùå **INTERDITE :** RTX 5060 Ti (16GB VRAM) sur Bus PCI 0 
- üéØ **Objectif :** Performance optimale + stabilit√© maximale

### üéØ **R√®gle #2 : Configuration GPU Compl√®te OBLIGATOIRE**
```python
# OBLIGATOIRE - √Ä COPIER DANS CHAQUE SCRIPT GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB sur Bus PCI 1
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Force l'ordre du bus physique
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire
```

### üéØ **R√®gle #3 : Validation RTX 3090 SYST√âMATIQUE**
```python
# OBLIGATOIRE - Fonction de validation dans chaque script
def validate_rtx3090_mandatory():
    """Validation syst√©matique RTX 3090 - AUCUNE EXCEPTION"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    cuda_order = os.environ.get('CUDA_DEVICE_ORDER', '')
    
    if cuda_devices != '1':
        raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
    
    if cuda_order != 'PCI_BUS_ID':
        raise RuntimeError(f"üö´ CUDA_DEVICE_ORDER='{cuda_order}' incorrect - doit √™tre 'PCI_BUS_ID'")
    
    gpu_name = torch.cuda.get_device_name(0)
    if "RTX 3090" not in gpu_name:
        raise RuntimeError(f"üö´ GPU d√©tect√©: {gpu_name} - RTX 3090 requise")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 ‚âà 24GB
        raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) insuffisante - RTX 3090 requise")
    
    print(f"‚úÖ RTX 3090 valid√©e: {gpu_name} ({gpu_memory:.1f}GB)")

# OBLIGATOIRE - Appel syst√©matique
if __name__ == "__main__":
    validate_rtx3090_mandatory()
```

---

## üìã TEMPLATE DE CODE OBLIGATOIRE V2.0

### üîß **Template Complet pour Nouveaux Scripts**
```python
#!/usr/bin/env python3
"""
[Description du script]
üö® CONFIGURATION GPU: RTX 3090 (CUDA:0 apr√®s mapping) OBLIGATOIRE
üîß Standards SuperWhisper V6 - Version 1.0 D√âFINITIVE
"""

import os
import sys

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
# RTX 5060 Ti (Bus PCI 0) = STRICTEMENT INTERDITE
# RTX 3090 (Bus PCI 1) = SEULE GPU AUTORIS√âE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB sur Bus PCI 1
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Force l'ordre du bus physique
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire

print("üéÆ SuperWhisper V6 - Configuration GPU RTX 3090 (CUDA:0 apr√®s mapping)")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
print(f"üîí CUDA_DEVICE_ORDER: {os.environ.get('CUDA_DEVICE_ORDER')}")

# =============================================================================
# üõ°Ô∏è MEMORY LEAK PREVENTION - OPTIONNEL MAIS RECOMMAND√â
# =============================================================================
try:
    from memory_leak_v4 import (
        configure_for_environment, 
        gpu_test_cleanup, 
        validate_no_memory_leak
    )
    configure_for_environment("dev")  # ou "ci" ou "production"
    print("‚úÖ Memory Leak Prevention V4.0 activ√©")
    memory_leak_protection = True
except ImportError:
    print("‚ö†Ô∏è Memory Leak V4.0 non disponible - Continuer sans protection")
    memory_leak_protection = False
    gpu_test_cleanup = lambda name: lambda func: func  # Fallback

# Maintenant imports normaux...
import torch
import time
import gc
# ... autres imports selon besoins

def validate_rtx3090_mandatory():
    """Validation syst√©matique RTX 3090 - OBLIGATOIRE SuperWhisper V6"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
    
    # CONTR√îLE 1: Variables environnement
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    cuda_order = os.environ.get('CUDA_DEVICE_ORDER', '')
    
    if cuda_devices != '1':
        raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
    
    if cuda_order != 'PCI_BUS_ID':
        raise RuntimeError(f"üö´ CUDA_DEVICE_ORDER='{cuda_order}' incorrect - doit √™tre 'PCI_BUS_ID'")
    
    # CONTR√îLE 2: GPU physique d√©tect√© (apr√®s mapping, cuda:0 = RTX 3090)
    gpu_name = torch.cuda.get_device_name(0)
    if "RTX 3090" not in gpu_name:
        raise RuntimeError(f"üö´ GPU d√©tect√©: {gpu_name} - RTX 3090 requise")
    
    # CONTR√îLE 3: M√©moire GPU
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 ‚âà 24GB
        raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) insuffisante - RTX 3090 requise")
    
    print(f"‚úÖ RTX 3090 valid√©e: {gpu_name} ({gpu_memory:.1f}GB)")

# EXEMPLE D'UTILISATION DANS LE CODE
@gpu_test_cleanup("nom_test_descriptif") if memory_leak_protection else lambda func: func
def votre_fonction_gpu():
    """Exemple fonction utilisant GPU avec protection memory leak"""
    # Utiliser cuda:0 qui pointe vers RTX 3090 apr√®s mapping
    device = "cuda:0"  # ou simplement "cuda"
    
    # Votre code GPU ici
    model = torch.randn(1000, 1000, device=device)
    result = torch.matmul(model, model.t())
    
    # Cleanup automatique via d√©corateur si Memory Leak V4 disponible
    return result.cpu()

# VALIDATION OBLIGATOIRE
if __name__ == "__main__":
    print("üöÄ SuperWhisper V6 - Validation Standards GPU")
    
    # √âTAPE 1: Validation RTX 3090
    validate_rtx3090_mandatory()
    
    # √âTAPE 2: Votre code principal
    try:
        # Votre code ici
        votre_fonction_gpu()
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        raise
    
    # √âTAPE 3: Validation memory leak (si protection activ√©e)
    if memory_leak_protection:
        validate_no_memory_leak()
    
    print("‚úÖ Script termin√© avec succ√®s - Standards SuperWhisper V6 respect√©s")
```

---

## üîç PROC√âDURES DE VALIDATION OBLIGATOIRES

### ‚úÖ **Validation Avant Commit Git**
```bash
# OBLIGATOIRE - Script de validation avant commit
python test_gpu_correct.py  # Validateur SuperWhisper V6
python test_validation_rtx3090_detection.py  # Validation multi-scripts

# R√âSULTAT ATTENDU:
# ‚úÖ RTX 3090 d√©tect√©e exclusivement
# ‚úÖ Aucun usage RTX 5060 Ti
# ‚úÖ Configuration GPU compl√®te sur tous scripts
```

### ‚úÖ **Validation Pendant D√©veloppement**
```python
# OBLIGATOIRE - Ajouter √† vos tests unitaires
def test_gpu_configuration_superwhisper_v6():
    """Test validation standards GPU SuperWhisper V6"""
    
    # Test configuration environnement
    assert os.environ.get('CUDA_VISIBLE_DEVICES') == '1'
    assert os.environ.get('CUDA_DEVICE_ORDER') == 'PCI_BUS_ID'
    
    # Test GPU d√©tect√©e
    gpu_name = torch.cuda.get_device_name(0)
    assert "RTX 3090" in gpu_name
    
    # Test m√©moire GPU
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    assert gpu_memory > 20  # RTX 3090 = 24GB
    
    print("‚úÖ Standards GPU SuperWhisper V6 valid√©s")
```

### ‚úÖ **Validation Performance Continue**
```python
# RECOMMAND√â - Monitoring performance RTX 3090
def benchmark_rtx3090_performance():
    """Benchmark RTX 3090 selon standards SuperWhisper V6"""
    
    # Configuration obligatoire
    validate_rtx3090_mandatory()
    
    # Test performance baseline
    device = "cuda:0"  # RTX 3090 apr√®s mapping
    
    start_time = time.time()
    
    # Test allocation m√©moire
    test_tensor = torch.randn(5000, 5000, device=device)
    
    # Test compute
    result = torch.matmul(test_tensor, test_tensor.t())
    
    # Test cleanup
    del test_tensor, result
    torch.cuda.empty_cache()
    
    duration = time.time() - start_time
    
    # RTX 3090 doit √™tre sous 2 secondes pour ce test
    assert duration < 2.0, f"Performance RTX 3090 d√©grad√©e: {duration:.2f}s"
    
    print(f"‚úÖ RTX 3090 Performance OK: {duration:.2f}s")
```

---

## üìä M√âTRIQUES PERFORMANCE RTX 3090

### üéØ **Benchmarks de R√©f√©rence (Valid√©s Scientifiquement)**
| üìà **M√©trique** | üéÆ **RTX 3090** | üéÆ **RTX 5060 Ti** | üìä **Avantage RTX 3090** |
|----------------|-----------------|-------------------|-------------------------|
| **VRAM Disponible** | 24GB | 16GB | **+8GB (50% plus)** |
| **Performance Compute** | 20,666 GFLOPS | ~12,400 GFLOPS | **67% plus rapide** |
| **Allocation M√©moire** | 3.8ms | ~6.3ms | **66% plus rapide** |
| **Cleanup M√©moire** | 2.7ms | ~4.5ms | **67% plus rapide** |
| **Ratio Performance** | 1.667x | 1.0x | **Facteur 1.67** |

### üîß **Seuils de Performance Attendus**
```python
# Standards performance RTX 3090 SuperWhisper V6
RTX3090_MIN_GFLOPS = 18000      # Minimum acceptable
RTX3090_MIN_VRAM_GB = 20        # Minimum 20GB (24GB nominal)
RTX3090_MAX_ALLOC_MS = 5.0      # Maximum 5ms allocation
RTX3090_MAX_CLEANUP_MS = 4.0    # Maximum 4ms cleanup
RTX3090_MIN_PERF_RATIO = 1.5    # Minimum 50% plus rapide que RTX 5060 Ti
```

---

## üõ†Ô∏è OUTILS DE VALIDATION DISPONIBLES

### üìã **Scripts de Validation SuperWhisper V6**
| üîß **Script** | üéØ **Fonction** | üí° **Utilisation** |
|---------------|----------------|-------------------|
| `test_gpu_correct.py` | Validateur universel 18 modules | `python test_gpu_correct.py` |
| `test_validation_rtx3090_detection.py` | Validation multi-scripts | `python test_validation_rtx3090_detection.py` |
| `test_integration_gpu_rtx3090.py` | Tests int√©gration syst√®me | `python test_integration_gpu_rtx3090.py` |
| `test_workflow_stt_llm_tts_rtx3090.py` | Pipeline STT‚ÜíLLM‚ÜíTTS | `python test_workflow_stt_llm_tts_rtx3090.py` |
| `test_benchmark_performance_rtx3090.py` | Benchmarks performance | `python test_benchmark_performance_rtx3090.py` |
| `test_stabilite_30min_rtx3090.py` | Tests stabilit√© endurance | `python test_stabilite_30min_rtx3090.py` |

### üìã **Scripts Auxiliaires**
| üîß **Script** | üéØ **Fonction** | üí° **Utilisation** |
|---------------|----------------|-------------------|
| `memory_leak_v4.py` | Prevention memory leaks | `from memory_leak_v4 import gpu_test_cleanup` |
| `test_diagnostic_rtx3090.py` | Diagnostic syst√®me complet | `python test_diagnostic_rtx3090.py` |

---

## ‚ö†Ô∏è ERREURS COMMUNES √Ä √âVITER

### ‚ùå **Configuration Incompl√®te**
```python
# ‚ùå INCORRECT - Manque CUDA_DEVICE_ORDER
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
# Sans CUDA_DEVICE_ORDER, ordre GPU impr√©visible !

# ‚úÖ CORRECT - Configuration compl√®te
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
```

### ‚ùå **Mapping GPU Incorrect**
```python
# ‚ùå INCORRECT - Essayer d'utiliser cuda:1 apr√®s mapping
device = "cuda:1"  # GPU inexistant apr√®s CUDA_VISIBLE_DEVICES='1'

# ‚úÖ CORRECT - Utiliser cuda:0 qui pointe vers RTX 3090
device = "cuda:0"  # ou simplement "cuda"
```

### ‚ùå **Validation Manquante**
```python
# ‚ùå INCORRECT - Assumer que GPU est correcte
# Pas de validation = risque utilisation RTX 5060 Ti

# ‚úÖ CORRECT - Validation syst√©matique
validate_rtx3090_mandatory()  # OBLIGATOIRE
```

### ‚ùå **Tests Insuffisants**
```python
# ‚ùå INCORRECT - Tester seulement si "√ßa marche"
if torch.cuda.is_available():
    print("GPU disponible")  # Peut √™tre RTX 5060 Ti !

# ‚úÖ CORRECT - Validation compl√®te
gpu_name = torch.cuda.get_device_name(0)
assert "RTX 3090" in gpu_name  # Validation factuelle
```

---

## üîß INT√âGRATION AVEC MEMORY LEAK V4.0

### üìã **Configuration Recommand√©e**
```python
# Import Memory Leak V4.0 (optionnel mais recommand√©)
try:
    from memory_leak_v4 import configure_for_environment, gpu_test_cleanup
    configure_for_environment("dev")  # ou "ci" ou "production"
    
    @gpu_test_cleanup("nom_fonction")
    def votre_fonction_gpu():
        # Cleanup automatique + monitoring m√©moire
        pass
        
except ImportError:
    # Fallback si Memory Leak V4.0 non disponible
    gpu_test_cleanup = lambda name: lambda func: func
```

### üìã **Environnements Support√©s**
- **DEV** : Logs JSON + monitoring d√©veloppement
- **CI** : Lock multiprocess + validation stricte  
- **PRODUCTION** : Logs JSON + lock multiprocess + m√©triques Prometheus

---

## üìà √âVOLUTION ET MAINTENANCE

### üîÑ **Mises √† Jour Standards**
- **Version 1.0** : Standards initiaux (12/06/2025)
- **Futures versions** : Adaptations selon √©volutions hardware/software
- **R√©trocompatibilit√©** : Standards V1.0 maintenus minimum 2 ans

### üîÑ **Processus de Modification**
1. **Proposition** : Issue GitHub avec justification technique
2. **Validation** : Tests performance + validation √©quipe
3. **Documentation** : Mise √† jour standards + migration guide
4. **D√©ploiement** : Rollout progressif avec monitoring

### üîÑ **Support et Questions**
- **Documentation** : Ce document + guides d√©veloppement
- **Validation** : Scripts fournis + exemples
- **Support** : √âquipe SuperWhisper V6

---

## üéØ CONFORMIT√â ET VALIDATION

### ‚úÖ **Checklist D√©veloppeur**
- [ ] Configuration GPU compl√®te (CUDA_VISIBLE_DEVICES + CUDA_DEVICE_ORDER)
- [ ] Fonction validate_rtx3090_mandatory() pr√©sente et appel√©e
- [ ] Code utilise cuda:0 ou "cuda" (jamais cuda:1 apr√®s mapping)
- [ ] Tests unitaires incluent validation GPU
- [ ] Script test√© avec validateurs SuperWhisper V6
- [ ] Performance conforme aux benchmarks RTX 3090
- [ ] Documentation √† jour

### ‚úÖ **Validation Continue**
- **Avant commit** : Scripts validation obligatoires
- **CI/CD** : Tests automatis√©s + validation GPU
- **Production** : Monitoring performance + m√©triques
- **Maintenance** : Review mensuel conformit√©

---

**üéØ CES STANDARDS SONT OBLIGATOIRES POUR TOUS LES D√âVELOPPEMENTS SUPERWHISPER V6**  
**üìä VALID√âS SCIENTIFIQUEMENT : RTX 3090 67% PLUS RAPIDE + 8GB VRAM SUPPL√âMENTAIRES**  
**üõ°Ô∏è S√âCURIT√â : 0% RISQUE UTILISATION ACCIDENTELLE RTX 5060 Ti**

---

*Document cr√©√© le 12/06/2025 par l'√©quipe Mission GPU SuperWhisper V6*  
*Statut : D√âFINITIF - Version 1.0* 