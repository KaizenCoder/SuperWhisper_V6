#!/usr/bin/env python3
"""
Test d'accÃ¨s RTX 3090
ðŸš¨ CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys

# =============================================================================
# ðŸš¨ CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
# RTX 5060 Ti (CUDA:0) = INTERDITE - RTX 3090 (CUDA:1) = OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation mÃ©moire

print("ðŸŽ® GPU Configuration: RTX 3090 (CUDA:1) forcÃ©e")
print(f"ðŸ”’ CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Maintenant imports normaux...
import torch

def validate_rtx3090_mandatory():
    """Validation obligatoire de la configuration RTX 3090"""
    if not torch.cuda.is_available():
        raise RuntimeError("ðŸš« CUDA non disponible - RTX 3090 requise")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"ðŸš« CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit Ãªtre '1'")
    
    cuda_order = os.environ.get('CUDA_DEVICE_ORDER', '')
    if cuda_order != 'PCI_BUS_ID':
        raise RuntimeError(f"ðŸš« CUDA_DEVICE_ORDER='{cuda_order}' incorrect - doit Ãªtre 'PCI_BUS_ID'")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 = ~24GB
        raise RuntimeError(f"ðŸš« GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
    
    print(f"âœ… RTX 3090 validÃ©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")

def test_rtx3090_access():
    """Test d'accÃ¨s et fonctionnalitÃ© RTX 3090"""
    print("=== TEST ACCÃˆS RTX 3090 ===")
    
    # Validation RTX 3090 obligatoire
    validate_rtx3090_mandatory()
    
    print(f"ðŸ”’ CUDA_VISIBLE_DEVICES = '{os.environ.get('CUDA_VISIBLE_DEVICES')}'")
    print(f"ðŸ”§ CUDA_DEVICE_ORDER = '{os.environ.get('CUDA_DEVICE_ORDER')}'")
    print(f"ðŸŽ¯ CUDA disponible: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"ðŸ”¢ Nombre de GPU visibles: {device_count}")
        
        gpu_name = torch.cuda.get_device_name(0)
        props = torch.cuda.get_device_properties(0)
        gpu_memory = props.total_memory / 1024**3
        
        print(f"ðŸŽ® GPU 0 (RTX 3090 mappÃ©e): {gpu_name}")
        print(f"ðŸ’¾ MÃ©moire: {gpu_memory:.1f} GB")
        print(f"ðŸ”§ Compute Capability: {props.major}.{props.minor}")
        
        # VÃ©rification RTX 3090
        if "RTX 3090" not in gpu_name:
            raise RuntimeError(f"GPU incorrecte dÃ©tectÃ©e: {gpu_name}")
        
        # Test crÃ©ation tensor simple
        print("\nðŸ§ª Test crÃ©ation tensor sur RTX 3090...")
        try:
            x = torch.tensor([1.0, 2.0, 3.0], device='cuda:0')
            print(f"   âœ… Tensor crÃ©Ã© sur: {x.device}")
            print(f"   ðŸ“Š Valeurs: {x.tolist()}")
            
            # Test calcul GPU
            y = x * 2
            print(f"   âœ… Calcul GPU rÃ©ussi: {y.tolist()}")
            
            # Test allocation plus importante
            z = torch.randn(1000, 1000, device='cuda:0')
            print(f"   âœ… Allocation 4MB RTX 3090 rÃ©ussie")
            
            # Test opÃ©ration matricielle
            w = torch.matmul(z, z.t())
            print(f"   âœ… Multiplication matricielle RTX 3090 rÃ©ussie")
            print(f"   ðŸ“ Taille rÃ©sultat: {w.shape}")
            
            # Cleanup
            del x, y, z, w
            torch.cuda.empty_cache()
            print(f"   âœ… Nettoyage mÃ©moire RTX 3090 effectuÃ©")
            
        except Exception as e:
            print(f"   âŒ Erreur: {e}")
            raise
        
        # Statistiques mÃ©moire
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"\nðŸ’¾ MÃ‰MOIRE RTX 3090 - AllouÃ©e: {allocated:.3f}GB, RÃ©servÃ©e: {reserved:.3f}GB")
        
    else:
        print("âŒ CUDA non disponible!")
        raise RuntimeError("CUDA non disponible")
    
    print("\n" + "="*40)
    print("âœ… TEST ACCÃˆS RTX 3090 RÃ‰USSI")
    print("   AccÃ¨s et fonctionnalitÃ© RTX 3090 validÃ©s")
    print("="*40)

# APPELER DANS TOUS LES SCRIPTS PRINCIPAUX
if __name__ == "__main__":
    validate_rtx3090_mandatory()
    test_rtx3090_access() 