#!/usr/bin/env python3
"""
VALIDATION FACTUELLE - mvp_settings.yaml
Test pour v√©rifier que la configuration utilise RTX 3090 (CUDA:0)
"""

import yaml
import torch
import os

def test_mvp_settings_config():
    """Test factuel de la configuration mvp_settings.yaml"""
    print("üîç VALIDATION - mvp_settings.yaml")
    print("="*40)
    
    # Test configuration
    config_path = "docs/Transmission_coordinateur/Transmission_coordinateur_20250610_1744/mvp_settings.yaml"
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"‚úÖ Fichier lu avec succ√®s: {config_path}")
        
        # V√©rifier configuration STT
        stt_device = config.get('stt', {}).get('gpu_device')
        print(f"   STT gpu_device: {stt_device}")
        
        if stt_device == "cuda:0":
            print("   ‚úÖ STT utilise CUDA:0 (RTX 3090)")
        else:
            print(f"   ‚ùå STT utilise {stt_device} (INCORRECT)")
            return False
        
        # V√©rifier configuration LLM
        llm_index = config.get('llm', {}).get('gpu_device_index')
        print(f"   LLM gpu_device_index: {llm_index}")
        
        if llm_index == 0:
            print("   ‚úÖ LLM utilise index 0 (RTX 3090)")
        else:
            print(f"   ‚ùå LLM utilise index {llm_index} (INCORRECT)")
            return False
            
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lecture config: {e}")
        return False

def test_gpu_allocation():
    """Test factuel d'allocation GPU selon config"""
    print("\nüéÆ TEST ALLOCATION GPU")
    print("="*30)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA non disponible")
        return False
    
    try:
        # Test CUDA:0 (RTX 3090 selon config)
        device = torch.device('cuda:0')
        test_tensor = torch.randn(100, 100).to(device)
        gpu_name = torch.cuda.get_device_name(device)
        memory_allocated = torch.cuda.memory_allocated(device) / 1024**2
        
        print(f"   Device test√©: cuda:0")
        print(f"   GPU utilis√©e: {gpu_name}")
        print(f"   M√©moire allou√©e: {memory_allocated:.1f}MB")
        
        is_rtx3090 = "3090" in gpu_name
        print(f"   RTX 3090 confirm√©e: {'‚úÖ OUI' if is_rtx3090 else '‚ùå NON'}")
        
        # Cleanup
        del test_tensor
        torch.cuda.empty_cache()
        
        return is_rtx3090
        
    except Exception as e:
        print(f"‚ùå Erreur allocation GPU: {e}")
        return False

if __name__ == "__main__":
    print("üö® VALIDATION mvp_settings.yaml - RTX 3090")
    print("="*50)
    
    # Test configuration
    config_valid = test_mvp_settings_config()
    
    # Test allocation GPU
    gpu_valid = test_gpu_allocation()
    
    # R√©sultat final
    print(f"\nüéØ R√âSULTAT FINAL:")
    print(f"   Configuration correcte: {'‚úÖ' if config_valid else '‚ùå'}")
    print(f"   RTX 3090 utilis√©e: {'‚úÖ' if gpu_valid else '‚ùå'}")
    
    overall_success = config_valid and gpu_valid
    print(f"   Validation globale: {'‚úÖ R√âUSSIE' if overall_success else '‚ùå √âCHEC'}")
    
    if overall_success:
        print("   ‚úÖ mvp_settings.yaml utilise correctement RTX 3090")
    else:
        print("   ‚ùå mvp_settings.yaml n√©cessite correction") 