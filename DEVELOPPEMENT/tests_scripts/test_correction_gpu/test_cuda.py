#!/usr/bin/env python3
"""
Test de d√©tection CUDA avec PyTorch
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import torch

def validate_rtx3090_mandatory():
    """Validation obligatoire de la configuration RTX 3090"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
    
    cuda_order = os.environ.get('CUDA_DEVICE_ORDER', '')
    if cuda_order != 'PCI_BUS_ID':
        raise RuntimeError(f"üö´ CUDA_DEVICE_ORDER='{cuda_order}' incorrect - doit √™tre 'PCI_BUS_ID'")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 = ~24GB
        raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
    
    print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")

def test_cuda_rtx3090():
    """Test CUDA RTX 3090 avec validation compl√®te"""
    print("üö® RTX 5060 Ti MASQU√âE / RTX 3090 devient device 0 visible")
    print("=== TEST RTX 3090 EXCLUSIF ===")
    print(f"üéØ CUDA disponible: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"üî• Nombre de GPU visibles: {device_count}")
        
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"\n   GPU {i}: {gpu_name}")
            print(f"   M√©moire: {gpu_memory:.1f} GB")
            
            # Validation RTX 3090 exclusive
            if "RTX 3090" in gpu_name and gpu_memory >= 20:
                print(f"   ‚úÖ RTX 3090 confirm√©e sur device {i}")
            elif "RTX 5060" in gpu_name:
                print(f"   üö´ RTX 5060 Ti d√©tect√©e - DEVRAIT √äTRE MASQU√âE!")
                raise RuntimeError("RTX 5060 Ti non masqu√©e - configuration GPU incorrecte")
            
            # Test d'allocation sur RTX 3090
            if "RTX 3090" in gpu_name:
                try:
                    torch.cuda.set_device(i)
                    x = torch.randn(3000, 3000, device=f'cuda:{i}')  # Test 36MB sur RTX 3090
                    print(f"   ‚úÖ Allocation 36MB RTX 3090 r√©ussie!")
                    print(f"   üìä Tensor sur: {x.device}")
                    
                    # Test calcul GPU
                    y = torch.matmul(x, x.t())
                    print(f"   ‚úÖ Calcul matriciel RTX 3090 r√©ussi")
                    
                    # Cleanup m√©moire
                    del x, y
                    torch.cuda.empty_cache()
                    print(f"   ‚úÖ Nettoyage m√©moire RTX 3090 effectu√©")
                    
                except Exception as e:
                    print(f"   ‚ùå Erreur allocation RTX 3090: {e}")
                    raise
        
        print(f"\nüéØ Version CUDA: {torch.version.cuda}")
        print(f"üéØ GPU courant: {torch.cuda.current_device()}")
        
        # Statistiques m√©moire finales
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"üíæ M√©moire RTX 3090 - Allou√©e: {allocated:.3f}GB, R√©serv√©e: {reserved:.3f}GB")

    else:
        print("‚ùå CUDA non disponible")
        raise RuntimeError("CUDA non disponible")

    print("\n" + "="*50)
    print("‚úÖ Test CUDA RTX 3090 termin√© avec succ√®s")

# APPELER DANS TOUS LES SCRIPTS PRINCIPAUX
if __name__ == "__main__":
    validate_rtx3090_mandatory()
    test_cuda_rtx3090() 