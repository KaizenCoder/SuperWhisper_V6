# üîß GUIDE R√âSOLUTION OLLAMA HTTP 404 - SuperWhisper V6

**Probl√®me** : Erreur HTTP 404 lors des appels √† l'API Ollama  
**Impact** : LLM utilise toujours le fallback au lieu d'Ollama  
**Solution** : Diagnostic automatique + correction API  

---

## üöÄ R√âSOLUTION RAPIDE (5 MINUTES)

### √âtape 1: Diagnostic Automatique
```powershell
# Depuis PowerShell Windows (pas WSL)
PS C:\Dev\SuperWhisper_V6> python diagnostic_ollama_fix.py
```

**Ce script va :**
- ‚úÖ D√©marrer Ollama automatiquement
- ‚úÖ Tester tous les endpoints API disponibles
- ‚úÖ Identifier le format API fonctionnel
- ‚úÖ G√©n√©rer un rapport de diagnostic

### √âtape 2: Correction Automatique
```powershell
PS C:\Dev\SuperWhisper_V6> python fix_llm_manager_ollama.py
```

**Ce script va :**
- ‚úÖ Sauvegarder le fichier original
- ‚úÖ Corriger l'API Ollama dans `LLM/llm_manager_enhanced.py`
- ‚úÖ V√©rifier que la correction est appliqu√©e
- ‚úÖ Cr√©er un script de test

### √âtape 3: Test de la Correction
```powershell
PS C:\Dev\SuperWhisper_V6> python test_ollama_corrected.py
```

**R√©sultat attendu :**
```
‚úÖ Test r√©ussi: Paris est la capitale de la France.
```

### √âtape 4: Test Pipeline Complet
```powershell
PS C:\Dev\SuperWhisper_V6> python test_pipeline_microphone_reel.py
```

---

## üîç DIAGNOSTIC D√âTAILL√â

### Probl√®me Identifi√©

L'erreur HTTP 404 est caus√©e par :

1. **Format API incorrect** : Le code utilise un m√©lange d'API OpenAI + Ollama
2. **Endpoint erron√©** : `/v1/chat/completions` au lieu de `/api/generate`
3. **Structure de donn√©es incorrecte** : `messages` au lieu de `prompt`

### Correction Appliqu√©e

**AVANT** (Code d√©faillant) :
```python
# ‚ùå Format hybride incorrect
data = {
    "model": self.config.get('model', 'nous-hermes'),
    "messages": messages,  # Format OpenAI
    "max_tokens": max_tokens,
    "temperature": temperature,
    "stream": False
}

response = await client.post(
    'http://127.0.0.1:11434/v1/chat/completions',  # ‚ùå Endpoint OpenAI
    json=data
)
```

**APR√àS** (Code corrig√©) :
```python
# ‚úÖ Format API native Ollama
data = {
    "model": actual_model,
    "prompt": f"{self.system_prompt}\n\nUser: {user_input}\nAssistant:",  # Format Ollama
    "stream": False,
    "options": {
        "temperature": temperature,
        "num_predict": max_tokens,
        "top_p": 0.9,
        "repeat_penalty": 1.1,
        "stop": ["User:", "\n\n"]
    }
}

response = await client.post(
    'http://127.0.0.1:11434/api/generate',  # ‚úÖ Endpoint Ollama natif
    json=data
)
```

---

## üìä FORMATS API OLLAMA SUPPORT√âS

### 1. API Native Ollama (Recommand√©e)
- **Endpoint** : `/api/generate`
- **Format** : `{"model": "...", "prompt": "...", "options": {...}}`
- **Avantages** : Plus rapide, support complet des options

### 2. API Chat Ollama
- **Endpoint** : `/api/chat`
- **Format** : `{"model": "...", "messages": [...], "options": {...}}`
- **Avantages** : Format conversationnel

### 3. API OpenAI Compatible
- **Endpoint** : `/v1/chat/completions`
- **Format** : `{"model": "...", "messages": [...], "max_tokens": ...}`
- **Avantages** : Compatibilit√© avec code OpenAI existant

---

## üõ†Ô∏è D√âPANNAGE AVANC√â

### Si le diagnostic √©choue :

1. **V√©rifier Ollama install√©** :
   ```powershell
   ollama --version
   ```

2. **D√©marrer Ollama manuellement** :
   ```powershell
   # Terminal 1
   set OLLAMA_MODELS=D:\modeles_llm
   ollama serve
   
   # Terminal 2 - Tester
   ollama list
   ```

3. **V√©rifier le mod√®le** :
   ```powershell
   ollama pull nous-hermes-2-mistral-7b-dpo
   ollama run nous-hermes-2-mistral-7b-dpo "Bonjour"
   ```

4. **V√©rifier le port** :
   ```powershell
   netstat -an | findstr 11434
   ```

### Si la correction √©choue :

1. **Restaurer la sauvegarde** :
   ```powershell
   # Les sauvegardes sont cr√©√©es automatiquement
   # Format: LLM/llm_manager_enhanced.py.backup_YYYYMMDD_HHMMSS
   copy "LLM\llm_manager_enhanced.py.backup_*" "LLM\llm_manager_enhanced.py"
   ```

2. **Correction manuelle** :
   - Ouvrir `LLM/llm_manager_enhanced.py`
   - Localiser la m√©thode `_generate_ollama`
   - Remplacer par le code corrig√© (voir section "Correction Appliqu√©e")

---

## üéØ VALIDATION FINALE

Une fois la correction appliqu√©e, le pipeline doit :

1. **STT** : Transcrire votre voix ‚Üí ‚úÖ (d√©j√† fonctionnel)
2. **LLM** : G√©n√©rer vraies r√©ponses via Ollama ‚Üí ‚úÖ (corrig√©)
3. **TTS** : Synth√©tiser la r√©ponse ‚Üí ‚úÖ (d√©j√† fonctionnel)

**Test de validation** :
```
Vous : "Quelle est la capitale de la France ?"
Pipeline : "Paris est la capitale de la France."
```

**Performance attendue** :
- STT : ~1000ms
- LLM : ~2000-5000ms (avec Ollama)
- TTS : ~800ms
- **Total** : ~4-7 secondes

---

## üìÅ FICHIERS CR√â√âS

- `diagnostic_ollama_fix.py` - Script diagnostic automatique
- `fix_llm_manager_ollama.py` - Script correction automatique
- `test_ollama_corrected.py` - Script test post-correction
- `diagnostic_ollama_rapport.json` - Rapport diagnostic d√©taill√©
- `LLM/llm_manager_enhanced.py.backup_*` - Sauvegarde automatique

---

## ‚ö° R√âSUM√â EX√âCUTION

```powershell
# 1. Diagnostic (2 min)
python diagnostic_ollama_fix.py

# 2. Correction (30 sec)
python fix_llm_manager_ollama.py

# 3. Test (30 sec)
python test_ollama_corrected.py

# 4. Pipeline complet (test final)
python test_pipeline_microphone_reel.py
```

**R√©sultat** : Pipeline voix-√†-voix 100% fonctionnel avec vraies r√©ponses LLM Ollama !

---

*Guide SuperWhisper V6 - R√©solution HTTP 404 Ollama*  
*Temps de r√©solution : ~5 minutes* 