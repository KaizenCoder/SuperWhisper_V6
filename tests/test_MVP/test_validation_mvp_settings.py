#!/usr/bin/env python3
"""
VALIDATION FACTUELLE - mvp_settings.yaml
Test pour v√©rifier que la configuration utilise RTX 3090 (CUDA:0)

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import yaml
import torch
import os

def test_mvp_settings_config():
    """Test factuel de la configuration mvp_settings.yaml"""
    print("üîç VALIDATION - mvp_settings.yaml")
    print("="*40)
    
    # Test configuration
    config_path = "docs/Transmission_coordinateur/Transmission_coordinateur_20250610_1744/mvp_settings.yaml"
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"‚úÖ Fichier lu avec succ√®s: {config_path}")
        
        # V√©rifier configuration STT
        stt_device = config.get('stt', {}).get('gpu_device')
        print(f"   STT gpu_device: {stt_device}")
        
        if stt_device == "cuda:0":
            print("   ‚úÖ STT utilise CUDA:0 (RTX 3090)")
        else:
            print(f"   ‚ùå STT utilise {stt_device} (INCORRECT)")
            return False
        
        # V√©rifier configuration LLM
        llm_index = config.get('llm', {}).get('gpu_device_index')
        print(f"   LLM gpu_device_index: {llm_index}")
        
        if llm_index == 0:
            print("   ‚úÖ LLM utilise index 0 (RTX 3090)")
        else:
            print(f"   ‚ùå LLM utilise index {llm_index} (INCORRECT)")
            return False
            
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lecture config: {e}")
        return False

def test_gpu_allocation():
    """Test factuel d'allocation GPU selon config"""
    print("\nüéÆ TEST ALLOCATION GPU")
    print("="*30)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA non disponible")
        return False
    
    try:
        # Test CUDA:0 (RTX 3090 selon config)
        device = torch.device('cuda:0')
        test_tensor = torch.randn(100, 100).to(device)
        gpu_name = torch.cuda.get_device_name(device)
        memory_allocated = torch.cuda.memory_allocated(device) / 1024**2
        
        print(f"   Device test√©: cuda:0")
        print(f"   GPU utilis√©e: {gpu_name}")
        print(f"   M√©moire allou√©e: {memory_allocated:.1f}MB")
        
        is_rtx3090 = "3090" in gpu_name
        print(f"   RTX 3090 confirm√©e: {'‚úÖ OUI' if is_rtx3090 else '‚ùå NON'}")
        
        # Cleanup
        del test_tensor
        torch.cuda.empty_cache()
        
        return is_rtx3090
        
    except Exception as e:
        print(f"‚ùå Erreur allocation GPU: {e}")
        return False

if __name__ == "__main__":
    print("üö® VALIDATION mvp_settings.yaml - RTX 3090")
    print("="*50)
    
    # Test configuration
    config_valid = test_mvp_settings_config()
    
    # Test allocation GPU
    gpu_valid = test_gpu_allocation()
    
    # R√©sultat final
    print(f"\nüéØ R√âSULTAT FINAL:")
    print(f"   Configuration correcte: {'‚úÖ' if config_valid else '‚ùå'}")
    print(f"   RTX 3090 utilis√©e: {'‚úÖ' if gpu_valid else '‚ùå'}")
    
    overall_success = config_valid and gpu_valid
    print(f"   Validation globale: {'‚úÖ R√âUSSIE' if overall_success else '‚ùå √âCHEC'}")
    
    if overall_success:
        print("   ‚úÖ mvp_settings.yaml utilise correctement RTX 3090")
    else:
        print("   ‚ùå mvp_settings.yaml n√©cessite correction") 