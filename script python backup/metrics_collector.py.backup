#!/usr/bin/env python3
"""
Collecteur M√©triques Pipeline - Task 18.7
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

Collecteur m√©triques Prometheus pour monitoring pipeline voix-√†-voix :
- Latence end-to-end et par composant
- Throughput et taux d'erreur
- M√©triques GPU et VRAM
- Dashboard Grafana compatible
"""

import os
import sys
import time
import threading
import psutil
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import logging

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

print("üéÆ Metrics Collector: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

try:
    from prometheus_client import (
        Counter, Histogram, Gauge, Info, Enum,
        start_http_server, generate_latest, CONTENT_TYPE_LATEST
    )
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False
    print("‚ö†Ô∏è Prometheus client non disponible - m√©triques d√©sactiv√©es")

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("‚ö†Ô∏è PyTorch non disponible - m√©triques GPU d√©sactiv√©es")

# =============================================================================
# CONFIGURATION LOGGING
# =============================================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s ‚Äì %(levelname)s ‚Äì %(name)s ‚Äì %(message)s",
)
LOGGER = logging.getLogger("MetricsCollector")

# =============================================================================
# M√âTRIQUES PROMETHEUS
# =============================================================================

class PipelineMetricsCollector:
    """Collecteur m√©triques Prometheus pour pipeline SuperWhisper V6"""
    
    def __init__(self, enabled: bool = True, port: int = 9091):
        self.enabled = enabled and PROMETHEUS_AVAILABLE
        self.port = port
        self._server_started = False
        self._collection_thread: Optional[threading.Thread] = None
        self._running = threading.Event()
        
        if not self.enabled:
            LOGGER.warning("M√©triques d√©sactiv√©es - Prometheus non disponible")
            return
            
        self._init_metrics()
        LOGGER.info("‚úÖ Collecteur m√©triques initialis√©")
    
    def _init_metrics(self):
        """Initialiser les m√©triques Prometheus"""
        if not self.enabled:
            return
            
        # =================================================================
        # M√âTRIQUES PIPELINE PRINCIPAL
        # =================================================================
        
        # Latence end-to-end
        self.pipeline_latency = Histogram(
            'superwhisper_pipeline_latency_seconds',
            'Latence totale pipeline voix-√†-voix',
            buckets=[0.1, 0.2, 0.5, 1.0, 1.2, 2.0, 5.0, 10.0]
        )
        
        # Latence par composant
        self.stt_latency = Histogram(
            'superwhisper_stt_latency_seconds',
            'Latence transcription STT',
            buckets=[0.05, 0.1, 0.2, 0.4, 0.8, 1.0, 2.0]
        )
        
        self.llm_latency = Histogram(
            'superwhisper_llm_latency_seconds', 
            'Latence g√©n√©ration LLM',
            buckets=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]
        )
        
        self.tts_latency = Histogram(
            'superwhisper_tts_latency_seconds',
            'Latence synth√®se TTS', 
            buckets=[0.05, 0.1, 0.2, 0.5, 1.0, 2.0]
        )
        
        # Compteurs requ√™tes
        self.requests_total = Counter(
            'superwhisper_requests_total',
            'Nombre total de requ√™tes pipeline',
            ['status']  # success, error, timeout
        )
        
        self.conversations_total = Counter(
            'superwhisper_conversations_total',
            'Nombre total de tours de conversation'
        )
        
        # M√©triques erreurs
        self.errors_total = Counter(
            'superwhisper_errors_total',
            'Nombre total d\'erreurs',
            ['component', 'error_type']  # stt/llm/tts, timeout/network/processing
        )
        
        # =================================================================
        # M√âTRIQUES SYST√àME ET GPU
        # =================================================================
        
        # Utilisation CPU/RAM
        self.cpu_usage = Gauge(
            'superwhisper_cpu_usage_percent',
            'Utilisation CPU syst√®me'
        )
        
        self.memory_usage = Gauge(
            'superwhisper_memory_usage_bytes',
            'Utilisation m√©moire RAM'
        )
        
        # M√©triques GPU RTX 3090
        if TORCH_AVAILABLE:
            self.gpu_memory_used = Gauge(
                'superwhisper_gpu_memory_used_bytes',
                'M√©moire GPU utilis√©e RTX 3090'
            )
            
            self.gpu_memory_total = Gauge(
                'superwhisper_gpu_memory_total_bytes',
                'M√©moire GPU totale RTX 3090'
            )
            
            self.gpu_utilization = Gauge(
                'superwhisper_gpu_utilization_percent',
                'Utilisation GPU RTX 3090'
            )
        
        # =================================================================
        # M√âTRIQUES QUALIT√â ET PERFORMANCE
        # =================================================================
        
        # Throughput
        self.throughput_conversations_per_minute = Gauge(
            'superwhisper_throughput_conversations_per_minute',
            'D√©bit conversations par minute'
        )
        
        # Queue sizes
        self.queue_size = Gauge(
            'superwhisper_queue_size',
            'Taille des queues pipeline',
            ['queue_type']  # text_queue, response_queue
        )
        
        # Uptime
        self.uptime_seconds = Gauge(
            'superwhisper_uptime_seconds',
            'Temps de fonctionnement pipeline'
        )
        
        # Info syst√®me
        self.system_info = Info(
            'superwhisper_system_info',
            'Informations syst√®me'
        )
        
        # √âtat pipeline
        self.pipeline_status = Enum(
            'superwhisper_pipeline_status',
            '√âtat du pipeline',
            states=['starting', 'running', 'stopping', 'error']
        )
        
        # Initialiser infos syst√®me
        self._update_system_info()
        
        LOGGER.info("‚úÖ M√©triques Prometheus initialis√©es")
    
    def _update_system_info(self):
        """Mettre √† jour les informations syst√®me"""
        if not self.enabled:
            return
            
        info = {
            'version': 'SuperWhisper_V6',
            'python_version': f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
            'platform': sys.platform,
            'cpu_count': str(psutil.cpu_count()),
            'memory_total_gb': f"{psutil.virtual_memory().total / 1024**3:.1f}",
        }
        
        if TORCH_AVAILABLE and torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            info.update({
                'gpu_name': gpu_props.name,
                'gpu_memory_gb': f"{gpu_props.total_memory / 1024**3:.1f}",
                'cuda_version': torch.version.cuda or 'unknown'
            })
        
        self.system_info.info(info)
    
    def start_server(self):
        """D√©marrer le serveur HTTP Prometheus"""
        if not self.enabled or self._server_started:
            return
            
        try:
            start_http_server(self.port)
            self._server_started = True
            LOGGER.info(f"‚úÖ Serveur m√©triques Prometheus d√©marr√© sur port {self.port}")
            
            # D√©marrer collecte automatique
            self._start_collection()
            
        except Exception as e:
            LOGGER.error(f"‚ùå Erreur d√©marrage serveur m√©triques: {e}")
    
    def _start_collection(self):
        """D√©marrer la collecte automatique des m√©triques syst√®me"""
        if not self.enabled or self._collection_thread:
            return
            
        self._running.set()
        self._collection_thread = threading.Thread(
            target=self._collection_loop,
            daemon=True,
            name="MetricsCollection"
        )
        self._collection_thread.start()
        LOGGER.info("‚úÖ Collecte automatique m√©triques d√©marr√©e")
    
    def _collection_loop(self):
        """Boucle collecte m√©triques syst√®me"""
        start_time = time.time()
        
        while self._running.is_set():
            try:
                # M√©triques syst√®me
                self.cpu_usage.set(psutil.cpu_percent())
                self.memory_usage.set(psutil.virtual_memory().used)
                
                # M√©triques GPU
                if TORCH_AVAILABLE and torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated(0)
                    memory_total = torch.cuda.get_device_properties(0).total_memory
                    
                    self.gpu_memory_used.set(memory_used)
                    self.gpu_memory_total.set(memory_total)
                    
                    # Utilisation GPU (approximation via m√©moire)
                    gpu_util = (memory_used / memory_total) * 100
                    self.gpu_utilization.set(gpu_util)
                
                # Uptime
                uptime = time.time() - start_time
                self.uptime_seconds.set(uptime)
                
                # Attendre 5 secondes
                time.sleep(5)
                
            except Exception as e:
                LOGGER.error(f"Erreur collecte m√©triques: {e}")
                time.sleep(10)
    
    def stop(self):
        """Arr√™ter la collecte m√©triques"""
        if self._running.is_set():
            self._running.clear()
            LOGGER.info("üõë Collecte m√©triques arr√™t√©e")
        
        if self._collection_thread and self._collection_thread.is_alive():
            self._collection_thread.join(timeout=2)
    
    # =================================================================
    # M√âTHODES ENREGISTREMENT M√âTRIQUES PIPELINE
    # =================================================================
    
    def record_pipeline_latency(self, latency_seconds: float):
        """Enregistrer latence pipeline end-to-end"""
        if self.enabled:
            self.pipeline_latency.observe(latency_seconds)
    
    def record_stt_latency(self, latency_seconds: float):
        """Enregistrer latence STT"""
        if self.enabled:
            self.stt_latency.observe(latency_seconds)
    
    def record_llm_latency(self, latency_seconds: float):
        """Enregistrer latence LLM"""
        if self.enabled:
            self.llm_latency.observe(latency_seconds)
    
    def record_tts_latency(self, latency_seconds: float):
        """Enregistrer latence TTS"""
        if self.enabled:
            self.tts_latency.observe(latency_seconds)
    
    def record_request(self, status: str = 'success'):
        """Enregistrer requ√™te pipeline"""
        if self.enabled:
            self.requests_total.labels(status=status).inc()
    
    def record_conversation(self):
        """Enregistrer tour de conversation"""
        if self.enabled:
            self.conversations_total.inc()
    
    def record_error(self, component: str, error_type: str):
        """Enregistrer erreur"""
        if self.enabled:
            self.errors_total.labels(component=component, error_type=error_type).inc()
    
    def set_queue_size(self, queue_type: str, size: int):
        """Mettre √† jour taille queue"""
        if self.enabled:
            self.queue_size.labels(queue_type=queue_type).set(size)
    
    def set_pipeline_status(self, status: str):
        """Mettre √† jour statut pipeline"""
        if self.enabled:
            self.pipeline_status.state(status)
    
    def update_throughput(self, conversations_count: int, time_window_minutes: float):
        """Mettre √† jour throughput"""
        if self.enabled and time_window_minutes > 0:
            throughput = conversations_count / time_window_minutes
            self.throughput_conversations_per_minute.set(throughput)

# =================================================================
# DASHBOARD GRAFANA CONFIGURATION
# =================================================================

GRAFANA_DASHBOARD_JSON = """
{
  "dashboard": {
    "id": null,
    "title": "SuperWhisper V6 Pipeline Monitoring",
    "tags": ["superwhisper", "pipeline", "voice"],
    "timezone": "browser",
    "panels": [
      {
        "title": "Pipeline Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, superwhisper_pipeline_latency_seconds)",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, superwhisper_pipeline_latency_seconds)", 
            "legendFormat": "Median"
          }
        ],
        "yAxes": [{"unit": "s", "max": 2.0}],
        "alert": {
          "conditions": [
            {
              "query": {"queryType": "", "refId": "A"},
              "reducer": {"type": "last", "params": []},
              "evaluator": {"params": [1.2], "type": "gt"}
            }
          ],
          "executionErrorState": "alerting",
          "for": "1m",
          "frequency": "10s",
          "handler": 1,
          "name": "Pipeline Latency Alert",
          "noDataState": "no_data",
          "notifications": []
        }
      },
      {
        "title": "Component Latencies",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, superwhisper_stt_latency_seconds)",
            "legendFormat": "STT"
          },
          {
            "expr": "histogram_quantile(0.95, superwhisper_llm_latency_seconds)",
            "legendFormat": "LLM"
          },
          {
            "expr": "histogram_quantile(0.95, superwhisper_tts_latency_seconds)",
            "legendFormat": "TTS"
          }
        ]
      },
      {
        "title": "GPU Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "superwhisper_gpu_memory_used_bytes / 1024^3",
            "legendFormat": "Used GB"
          },
          {
            "expr": "superwhisper_gpu_memory_total_bytes / 1024^3",
            "legendFormat": "Total GB"
          }
        ]
      },
      {
        "title": "Throughput",
        "type": "singlestat",
        "targets": [
          {
            "expr": "superwhisper_throughput_conversations_per_minute",
            "legendFormat": "Conversations/min"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(superwhisper_errors_total[5m])",
            "legendFormat": "Errors/sec"
          }
        ]
      }
    ],
    "time": {"from": "now-1h", "to": "now"},
    "refresh": "5s"
  }
}
"""

def save_grafana_dashboard(output_path: str = "PIPELINE/monitoring/grafana_dashboard.json"):
    """Sauvegarder configuration dashboard Grafana"""
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(GRAFANA_DASHBOARD_JSON)
        LOGGER.info(f"‚úÖ Dashboard Grafana sauvegard√©: {output_path}")
    except Exception as e:
        LOGGER.error(f"‚ùå Erreur sauvegarde dashboard: {e}")

# =================================================================
# EXEMPLE UTILISATION
# =================================================================

if __name__ == "__main__":
    # Cr√©er collecteur
    collector = PipelineMetricsCollector(enabled=True, port=9091)
    
    # D√©marrer serveur
    collector.start_server()
    
    # Sauvegarder dashboard Grafana
    save_grafana_dashboard()
    
    # Simuler quelques m√©triques
    import random
    
    try:
        print("üöÄ Collecteur m√©triques d√©marr√© - http://localhost:9091/metrics")
        print("üìä Dashboard Grafana: PIPELINE/monitoring/grafana_dashboard.json")
        print("Ctrl+C pour arr√™ter...")
        
        collector.set_pipeline_status('running')
        
        # Simulation m√©triques
        for i in range(100):
            # Latences simul√©es
            collector.record_stt_latency(random.uniform(0.1, 0.4))
            collector.record_llm_latency(random.uniform(0.2, 0.8))
            collector.record_tts_latency(random.uniform(0.1, 0.3))
            
            # Latence totale
            total_latency = random.uniform(0.5, 1.1)
            collector.record_pipeline_latency(total_latency)
            
            # Requ√™te r√©ussie
            collector.record_request('success')
            collector.record_conversation()
            
            # Quelques erreurs
            if random.random() < 0.05:
                collector.record_error('llm', 'timeout')
                collector.record_request('error')
            
            # Tailles queues
            collector.set_queue_size('text_queue', random.randint(0, 5))
            collector.set_queue_size('response_queue', random.randint(0, 3))
            
            time.sleep(2)
            
    except KeyboardInterrupt:
        print("\nüõë Arr√™t collecteur m√©triques...")
        collector.set_pipeline_status('stopping')
        collector.stop()
        print("‚úÖ Collecteur arr√™t√©") 