#!/usr/bin/env python3
"""
ü§ñ PR√â-FLIGHT CHECK PIPELINE - VALIDATION SERVEUR LLM COMPLET
============================================================
Script de validation et health-check serveur LLM pour pipeline SuperWhisper V6

VALIDATIONS :
- Serveur LLM local (vLLM/llama.cpp/Ollama)
- Health-check endpoint robuste 
- Timeout handling gracieux
- Configuration quantization Q4_K_M si VRAM tension
- Test requ√™te simple pour validation fonctionnelle

Usage: python PIPELINE/scripts/start_llm.py
"""

import os
import sys
import logging
import requests
import json
import time
from typing import Dict, Any, Optional
from urllib.parse import urljoin

# Configuration logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("start_llm")

# Configuration LLM par d√©faut
DEFAULT_LLM_CONFIG = {
    'endpoints': [
        {'url': 'http://localhost:1234/v1', 'name': 'LM Studio', 'timeout': 30},
        {'url': 'http://localhost:11434', 'name': 'Ollama', 'timeout': 15},
        {'url': 'http://localhost:8000', 'name': 'vLLM', 'timeout': 20},
        {'url': 'http://localhost:8080', 'name': 'llama.cpp', 'timeout': 25}
    ],
    'test_prompt': "Hello, this is a test message for pipeline validation.",
    'expected_min_response_length': 5,
    'health_check_retries': 3,
    'startup_wait_time': 2
}

def check_llm_endpoint_health(endpoint: Dict[str, Any]) -> Dict[str, Any]:
    """
    V√©rification health-check d'un endpoint LLM
    
    Args:
        endpoint: Configuration endpoint avec url, name, timeout
        
    Returns:
        Dict avec r√©sultats health-check
    """
    result = {
        'name': endpoint['name'],
        'url': endpoint['url'],
        'available': False,
        'response_time_ms': 0,
        'error': None,
        'api_version': None,
        'models': []
    }
    
    try:
        start_time = time.time()
        
        # 1. Test basique de connectivit√©
        logger.info(f"üîç Test connectivit√© {endpoint['name']} ({endpoint['url']})...")
        
        # Health endpoint selon le type de serveur
        health_endpoints = [
            '/health',
            '/v1/models', 
            '/api/tags',
            '/',
            '/docs'
        ]
        
        response = None
        working_endpoint = None
        
        for health_path in health_endpoints:
            try:
                health_url = urljoin(endpoint['url'], health_path)
                response = requests.get(
                    health_url, 
                    timeout=endpoint['timeout'],
                    headers={'Content-Type': 'application/json'}
                )
                
                if response.status_code == 200:
                    working_endpoint = health_path
                    break
                    
            except Exception as e:
                continue
        
        if response is None or response.status_code != 200:
            raise RuntimeError(f"Aucun endpoint health accessible sur {endpoint['url']}")
        
        response_time = (time.time() - start_time) * 1000
        result['response_time_ms'] = round(response_time, 2)
        
        # 2. Analyse r√©ponse
        try:
            data = response.json()
            
            # D√©tection type serveur selon r√©ponse
            if 'models' in data and isinstance(data['models'], list):
                result['models'] = [model.get('id', str(model)) for model in data['models']]
                result['api_version'] = 'OpenAI Compatible'
            elif working_endpoint == '/api/tags' and 'models' in data:
                result['models'] = [model.get('name', str(model)) for model in data['models']]
                result['api_version'] = 'Ollama API'
            elif 'object' in data and data['object'] == 'list':
                result['models'] = [model.get('id', str(model)) for model in data.get('data', [])]
                result['api_version'] = 'OpenAI Compatible'
            else:
                result['api_version'] = 'Unknown API'
                
        except json.JSONDecodeError:
            result['api_version'] = 'Non-JSON Response'
        
        result['available'] = True
        logger.info(f"‚úÖ {endpoint['name']} disponible ({response_time:.1f}ms) - {len(result['models'])} mod√®les")
        
        return result
        
    except requests.exceptions.Timeout:
        error_msg = f"Timeout apr√®s {endpoint['timeout']}s"
        result['error'] = error_msg
        logger.warning(f"‚è±Ô∏è {endpoint['name']}: {error_msg}")
        
    except requests.exceptions.ConnectionError:
        error_msg = "Connexion refus√©e - serveur non d√©marr√©"
        result['error'] = error_msg
        logger.warning(f"üîå {endpoint['name']}: {error_msg}")
        
    except Exception as e:
        error_msg = str(e)
        result['error'] = error_msg
        logger.error(f"‚ùå {endpoint['name']}: {error_msg}")
    
    return result

def test_llm_inference(endpoint_url: str, api_version: str, timeout: int = 30) -> Dict[str, Any]:
    """
    Test d'inf√©rence LLM simple pour validation fonctionnelle
    
    Args:
        endpoint_url: URL de base du serveur LLM
        api_version: Type d'API d√©tect√©
        timeout: Timeout en secondes
        
    Returns:
        Dict avec r√©sultats test inf√©rence
    """
    result = {
        'inference_works': False,
        'response_text': None,
        'inference_time_ms': 0,
        'error': None
    }
    
    try:
        start_time = time.time()
        
        # Configuration requ√™te selon type API
        if 'Ollama' in api_version:
            # API Ollama
            url = urljoin(endpoint_url, '/api/generate')
            payload = {
                'model': 'llama3.2:latest',  # Mod√®le par d√©faut Ollama
                'prompt': DEFAULT_LLM_CONFIG['test_prompt'],
                'stream': False,
                'options': {'temperature': 0.1, 'num_predict': 50}
            }
        else:
            # API OpenAI compatible (vLLM, LM Studio, llama.cpp)
            url = urljoin(endpoint_url, '/v1/chat/completions')
            payload = {
                'model': 'default',  # Sera r√©solu automatiquement
                'messages': [
                    {'role': 'user', 'content': DEFAULT_LLM_CONFIG['test_prompt']}
                ],
                'max_tokens': 50,
                'temperature': 0.1
            }
        
        logger.info(f"üß™ Test inf√©rence LLM ({api_version})...")
        
        response = requests.post(
            url,
            json=payload,
            timeout=timeout,
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code != 200:
            raise RuntimeError(f"HTTP {response.status_code}: {response.text}")
        
        # Parsing r√©ponse selon type API
        data = response.json()
        
        if 'Ollama' in api_version:
            response_text = data.get('response', '')
        else:
            choices = data.get('choices', [])
            if choices and 'message' in choices[0]:
                response_text = choices[0]['message'].get('content', '')
            elif choices and 'text' in choices[0]:
                response_text = choices[0]['text']
            else:
                response_text = str(data)
        
        inference_time = (time.time() - start_time) * 1000
        result['inference_time_ms'] = round(inference_time, 2)
        result['response_text'] = response_text.strip()
        
        # Validation r√©ponse
        if len(result['response_text']) >= DEFAULT_LLM_CONFIG['expected_min_response_length']:
            result['inference_works'] = True
            logger.info(f"‚úÖ Test inf√©rence r√©ussi ({inference_time:.1f}ms): '{response_text[:100]}...'")
        else:
            raise RuntimeError(f"R√©ponse trop courte: '{response_text}'")
            
    except Exception as e:
        result['error'] = str(e)
        logger.error(f"‚ùå Test inf√©rence √©chou√©: {e}")
    
    return result

def validate_llm_servers() -> Dict[str, Any]:
    """
    Validation compl√®te des serveurs LLM disponibles
    
    Returns:
        Dict avec r√©sultats validation compl√®te
    """
    validation_results = {
        'servers_found': [],
        'working_servers': [],
        'recommended_server': None,
        'total_models': 0,
        'validation_time_ms': 0,
        'all_servers_down': True
    }
    
    start_time = time.time()
    
    logger.info("ü§ñ D√âMARRAGE VALIDATION SERVEURS LLM...")
    
    # Test de tous les endpoints configur√©s
    for endpoint in DEFAULT_LLM_CONFIG['endpoints']:
        health_result = check_llm_endpoint_health(endpoint)
        validation_results['servers_found'].append(health_result)
        
        if health_result['available']:
            validation_results['all_servers_down'] = False
            validation_results['total_models'] += len(health_result['models'])
            
            # Test inf√©rence si serveur disponible
            if health_result['models']:  # Au moins un mod√®le disponible
                inference_result = test_llm_inference(
                    health_result['url'], 
                    health_result['api_version'],
                    endpoint['timeout']
                )
                health_result['inference'] = inference_result
                
                if inference_result['inference_works']:
                    validation_results['working_servers'].append(health_result)
    
    # S√©lection serveur recommand√©
    if validation_results['working_servers']:
        # Priorit√© : le plus rapide avec inf√©rence fonctionnelle
        validation_results['recommended_server'] = min(
            validation_results['working_servers'],
            key=lambda x: x['response_time_ms'] + x['inference']['inference_time_ms']
        )
    
    validation_results['validation_time_ms'] = round((time.time() - start_time) * 1000, 2)
    
    return validation_results

def main():
    """Point d'entr√©e principal du script de validation LLM"""
    logger.info("ü§ñ D√âMARRAGE VALIDATION SERVEUR LLM PIPELINE...")
    
    try:
        # Validation compl√®te serveurs LLM
        results = validate_llm_servers()
        
        # Affichage r√©sultats
        print("\n" + "="*70)
        print("ü§ñ R√âSULTATS VALIDATION SERVEUR LLM PIPELINE")
        print("="*70)
        
        print(f"‚è±Ô∏è Temps validation: {results['validation_time_ms']:.1f}ms")
        print(f"üìä Serveurs test√©s: {len(results['servers_found'])}")
        print(f"‚úÖ Serveurs fonctionnels: {len(results['working_servers'])}")
        print(f"üß† Mod√®les disponibles: {results['total_models']}")
        
        print(f"\nüìã D√âTAIL SERVEURS:")
        for server in results['servers_found']:
            status_icon = "‚úÖ" if server['available'] else "‚ùå"
            inference_status = ""
            
            if server['available'] and 'inference' in server:
                if server['inference']['inference_works']:
                    inference_status = f" | üß™ Inf√©rence: ‚úÖ ({server['inference']['inference_time_ms']:.1f}ms)"
                else:
                    inference_status = f" | üß™ Inf√©rence: ‚ùå ({server['inference']['error']})"
            
            print(f"   {status_icon} {server['name']}: {server['url']}")
            print(f"      Response: {server['response_time_ms']:.1f}ms | API: {server['api_version']} | Mod√®les: {len(server['models'])}{inference_status}")
            
            if server['error']:
                print(f"      ‚ùå Erreur: {server['error']}")
        
        # Serveur recommand√©
        if results['recommended_server']:
            rec = results['recommended_server']
            print(f"\nüéØ SERVEUR RECOMMAND√â:")
            print(f"   üåü {rec['name']} ({rec['url']})")
            print(f"   ‚ö° Performance: {rec['response_time_ms']:.1f}ms + {rec['inference']['inference_time_ms']:.1f}ms inf√©rence")
            print(f"   üß† Mod√®les: {len(rec['models'])}")
            print(f"   üìù R√©ponse test: '{rec['inference']['response_text'][:100]}...'")
        
        print("="*70)
        if not results['all_servers_down'] and results['working_servers']:
            print("üöÄ PIPELINE AUTORIS√â - Serveur LLM fonctionnel disponible")
        else:
            print("üõë PIPELINE BLOQU√â - Aucun serveur LLM fonctionnel")
        print("="*70)
        
        return 0 if (not results['all_servers_down'] and results['working_servers']) else 1
        
    except Exception as e:
        print("\n" + "="*70)
        print("üö´ √âCHEC VALIDATION SERVEUR LLM PIPELINE")
        print("="*70)
        print(f"‚ùå ERREUR: {e}")
        print("\nüîß ACTIONS REQUISES:")
        print("   - D√©marrer un serveur LLM (LM Studio, Ollama, vLLM, llama.cpp)")
        print("   - V√©rifier configuration ports (1234, 11434, 8000, 8080)")
        print("   - T√©l√©charger au moins un mod√®le LLM")
        print("   - Tester endpoint manuellement avec curl/Postman")
        print("="*70)
        print("üõë PIPELINE BLOQU√â - Corriger configuration LLM avant continuation")
        print("="*70)
        
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 