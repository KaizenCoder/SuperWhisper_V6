#!/usr/bin/env python3
"""
Test simple int√©gration faster-whisper - RTX 3090
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import torch
import numpy as np
import time
from dataclasses import dataclass
from typing import Optional

def validate_rtx3090_mandatory():
    """Validation obligatoire RTX 3090"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 = ~24GB
        raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
    
    print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")

@dataclass
class STTResult:
    """R√©sultat de transcription STT"""
    text: str
    confidence: float
    processing_time: float
    rtf: float  # Real-Time Factor
    model_used: str
    audio_duration: float
    error: Optional[str] = None

def test_faster_whisper_import():
    """Test import faster-whisper"""
    try:
        from faster_whisper import WhisperModel
        print("‚úÖ faster-whisper import√© avec succ√®s")
        return True
    except ImportError as e:
        print(f"‚ùå Erreur import faster-whisper: {e}")
        return False

def test_faster_whisper_model():
    """Test chargement mod√®le faster-whisper"""
    try:
        from faster_whisper import WhisperModel
        
        print("üîÑ Chargement mod√®le tiny...")
        start_time = time.time()
        
        model = WhisperModel(
            "tiny",
            device="cuda",
            compute_type="float16"
        )
        
        load_time = time.time() - start_time
        print(f"‚úÖ Mod√®le tiny charg√© en {load_time:.2f}s")
        
        # Test GPU memory
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            print(f"üìä GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
        
        return model
    except Exception as e:
        print(f"‚ùå Erreur chargement mod√®le: {e}")
        return None

def generate_test_audio(duration=3.0, sample_rate=16000):
    """G√©n√®re un audio de test synth√©tique"""
    # G√©n√©ration d'un signal sinuso√Ødal simple
    t = np.linspace(0, duration, int(sample_rate * duration), False)
    frequency = 440  # La note A4
    audio = np.sin(2 * np.pi * frequency * t).astype(np.float32)
    
    print(f"üéµ Audio test g√©n√©r√©: {duration}s, {sample_rate}Hz, {len(audio)} samples")
    return audio

def test_transcription(model, audio, sample_rate=16000):
    """Test transcription avec faster-whisper"""
    try:
        print("üé§ D√©but transcription...")
        start_time = time.time()
        
        # Transcription
        segments, info = model.transcribe(
            audio,
            beam_size=1,
            language="fr",
            condition_on_previous_text=False
        )
        
        # Collecte des segments
        text_segments = []
        for segment in segments:
            text_segments.append(segment.text)
        
        processing_time = time.time() - start_time
        audio_duration = len(audio) / sample_rate
        rtf = processing_time / audio_duration
        
        result = STTResult(
            text=" ".join(text_segments).strip(),
            confidence=info.language_probability,
            processing_time=processing_time,
            rtf=rtf,
            model_used="tiny",
            audio_duration=audio_duration
        )
        
        print(f"‚úÖ Transcription termin√©e:")
        print(f"   üìù Texte: '{result.text}'")
        print(f"   ‚è±Ô∏è Temps: {result.processing_time:.3f}s")
        print(f"   üéØ RTF: {result.rtf:.3f}")
        print(f"   üîä Dur√©e audio: {result.audio_duration:.1f}s")
        print(f"   üìä Confiance: {result.confidence:.3f}")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Erreur transcription: {e}")
        return STTResult(
            text="",
            confidence=0.0,
            processing_time=0.0,
            rtf=0.0,
            model_used="tiny",
            audio_duration=0.0,
            error=str(e)
        )

def main():
    """Test principal"""
    print("üöÄ D√©marrage test int√©gration faster-whisper RTX 3090")
    print("=" * 60)
    
    try:
        # 1. Validation RTX 3090
        print("\n1Ô∏è‚É£ Validation RTX 3090...")
        validate_rtx3090_mandatory()
        
        # 2. Test import
        print("\n2Ô∏è‚É£ Test import faster-whisper...")
        if not test_faster_whisper_import():
            return False
        
        # 3. Test chargement mod√®le
        print("\n3Ô∏è‚É£ Test chargement mod√®le...")
        model = test_faster_whisper_model()
        if model is None:
            return False
        
        # 4. Test transcription
        print("\n4Ô∏è‚É£ Test transcription...")
        audio = generate_test_audio(duration=3.0)
        result = test_transcription(model, audio)
        
        if result.error:
            print(f"‚ùå Erreur transcription: {result.error}")
            return False
        
        # 5. Validation performance
        print("\n5Ô∏è‚É£ Validation performance...")
        if result.rtf < 0.5:  # Objectif RTF < 0.5 pour tiny
            print(f"‚úÖ Performance RTF excellente: {result.rtf:.3f} < 0.5")
        else:
            print(f"‚ö†Ô∏è Performance RTF acceptable: {result.rtf:.3f}")
        
        print("\nüéâ Test int√©gration faster-whisper R√âUSSI!")
        print("=" * 60)
        return True
        
    except Exception as e:
        print(f"\n‚ùå Erreur test: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 