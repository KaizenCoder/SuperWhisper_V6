#!/usr/bin/env python3
"""
Test Status Final SuperWhisper V6
Ã‰valuation complÃ¨te du pipeline avec rÃ©sumÃ© des problÃ¨mes
"""

import os
import sys
import pathlib
import asyncio
import logging
import time
import yaml

def _setup_portable_environment():
    current_file = pathlib.Path(__file__).resolve()
    
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    
    return project_root

_PROJECT_ROOT = _setup_portable_environment()

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(name)s] %(levelname)s: %(message)s')
logger = logging.getLogger("StatusFinal")

class PipelineStatusEvaluator:
    """Ã‰valuateur du status final du pipeline SuperWhisper V6"""
    
    def __init__(self):
        self.results = {
            'stt': {'status': 'unknown', 'details': [], 'score': 0},
            'llm': {'status': 'unknown', 'details': [], 'score': 0},
            'tts': {'status': 'unknown', 'details': [], 'score': 0},
            'integration': {'status': 'unknown', 'details': [], 'score': 0}
        }

    async def test_stt_component(self):
        """Test du composant STT"""
        logger.info("ğŸ¤ Test composant STT...")
        
        try:
            from STT.unified_stt_manager import UnifiedSTTManager
            
            stt_config = {
                'timeout_per_minute': 10.0,
                'cache_size_mb': 100,
                'fallback_chain': ['prism_primary']
            }
            
            stt_manager = UnifiedSTTManager(stt_config)
            
            # Test avec audio simulÃ©
            import numpy as np
            sample_rate = 16000
            duration = 2.0
            
            # GÃ©nÃ©rer signal test avec ton pur
            t = np.linspace(0, duration, int(sample_rate * duration))
            frequency = 440  # La note
            amplitude = 0.3
            audio_data = amplitude * np.sin(2 * np.pi * frequency * t)
            audio_bytes = (audio_data * 32767).astype(np.int16).tobytes()
            
            start_time = time.perf_counter()
            result = await stt_manager.transcribe_pcm(audio_bytes, sample_rate)
            stt_time = (time.perf_counter() - start_time) * 1000
            
            self.results['stt']['details'].append(f"Temps de traitement: {stt_time:.1f}ms")
            self.results['stt']['details'].append(f"Configuration GPU RTX 3090: OK")
            self.results['stt']['details'].append(f"Backend Prism initialisÃ©: OK")
            
            if stt_time < 3000:  # Moins de 3 secondes
                self.results['stt']['score'] += 50
                self.results['stt']['details'].append("Performance: Excellente")
            else:
                self.results['stt']['details'].append("Performance: Lente")
            
            self.results['stt']['score'] += 30  # Points pour l'initialisation
            self.results['stt']['status'] = 'functional'
            
            return True
            
        except Exception as e:
            self.results['stt']['status'] = 'error'
            self.results['stt']['details'].append(f"Erreur: {e}")
            return False

    async def test_llm_component(self):
        """Test du composant LLM"""
        logger.info("ğŸ§  Test composant LLM...")
        
        try:
            from LLM.llm_manager_enhanced import EnhancedLLMManager
            
            llm_config = {
                'model': 'nous-hermes-2-mistral-7b-dpo:latest',
                'use_ollama': True,
                'timeout': 15.0
            }
            
            llm_manager = EnhancedLLMManager(llm_config)
            await llm_manager.initialize()
            
            # Test simple
            test_question = "Quelle est la capitale de la France ?"
            start_time = time.perf_counter()
            response = await llm_manager.generate_response(test_question)
            llm_time = (time.perf_counter() - start_time) * 1000
            
            self.results['llm']['details'].append(f"Temps de rÃ©ponse: {llm_time:.1f}ms")
            self.results['llm']['details'].append(f"Question: {test_question}")
            self.results['llm']['details'].append(f"RÃ©ponse: {response[:100]}...")
            
            # VÃ©rifier si Ollama fonctionne
            if hasattr(llm_manager, 'use_ollama') and llm_manager.use_ollama:
                if "Paris" in response or "capitale" in response.lower():
                    self.results['llm']['score'] += 70
                    self.results['llm']['details'].append("Ollama: Fonctionnel avec vraie rÃ©ponse")
                    self.results['llm']['status'] = 'optimal'
                else:
                    self.results['llm']['score'] += 50
                    self.results['llm']['details'].append("Ollama: ConnectÃ© mais rÃ©ponse gÃ©nÃ©rique")
                    self.results['llm']['status'] = 'functional'
            else:
                self.results['llm']['score'] += 20
                self.results['llm']['details'].append("Fallback: UtilisÃ© (Ollama non accessible)")
                self.results['llm']['status'] = 'fallback'
            
            return True
            
        except Exception as e:
            self.results['llm']['status'] = 'error'
            self.results['llm']['details'].append(f"Erreur: {e}")
            return False

    async def test_tts_component(self):
        """Test du composant TTS"""
        logger.info("ğŸ”Š Test composant TTS...")
        
        try:
            with open('config/tts.yaml', 'r') as f:
                tts_config = yaml.safe_load(f)
            
            from TTS.tts_manager import UnifiedTTSManager
            tts_manager = UnifiedTTSManager(tts_config)
            
            test_text = "Test de synthÃ¨se vocale franÃ§aise."
            start_time = time.perf_counter()
            result = await tts_manager.synthesize(text=test_text)
            tts_time = (time.perf_counter() - start_time) * 1000
            
            self.results['tts']['details'].append(f"Temps de synthÃ¨se: {tts_time:.1f}ms")
            self.results['tts']['details'].append(f"Texte testÃ©: {test_text}")
            
            if result.success and result.audio_data:
                audio_size = len(result.audio_data)
                self.results['tts']['details'].append(f"Audio gÃ©nÃ©rÃ©: {audio_size} bytes")
                self.results['tts']['details'].append(f"Backend utilisÃ©: {result.backend_used}")
                
                if tts_time < 1500:
                    self.results['tts']['score'] += 70
                    self.results['tts']['details'].append("Performance: Excellente")
                elif tts_time < 3000:
                    self.results['tts']['score'] += 50
                    self.results['tts']['details'].append("Performance: Bonne")
                else:
                    self.results['tts']['score'] += 30
                    self.results['tts']['details'].append("Performance: Acceptable")
                
                # Test sauvegarde
                audio_file = "test_tts_status.wav"
                with open(audio_file, 'wb') as f:
                    f.write(result.audio_data)
                
                self.results['tts']['details'].append(f"Fichier audio: {audio_file}")
                self.results['tts']['score'] += 20
                self.results['tts']['status'] = 'functional'
                
                # Nettoyer
                try:
                    os.remove(audio_file)
                except:
                    pass
                
                return True
            else:
                self.results['tts']['details'].append(f"Erreur TTS: {result.error}")
                self.results['tts']['status'] = 'error'
                return False
                
        except Exception as e:
            self.results['tts']['status'] = 'error'
            self.results['tts']['details'].append(f"Erreur: {e}")
            return False

    async def test_integration(self):
        """Test d'intÃ©gration E2E simplifiÃ©"""
        logger.info("ğŸ”— Test intÃ©gration E2E...")
        
        try:
            # Test d'intÃ©gration STT â†’ LLM â†’ TTS
            question_test = "Bonjour, comment allez-vous ?"
            
            # STT simulÃ© (entrÃ©e texte)
            self.results['integration']['details'].append(f"EntrÃ©e simulÃ©e: {question_test}")
            
            # LLM
            from LLM.llm_manager_enhanced import EnhancedLLMManager
            llm_config = {'model': 'nous-hermes-2-mistral-7b-dpo:latest', 'use_ollama': True}
            llm_manager = EnhancedLLMManager(llm_config)
            await llm_manager.initialize()
            
            llm_response = await llm_manager.generate_response(question_test)
            self.results['integration']['details'].append(f"RÃ©ponse LLM: {llm_response[:50]}...")
            
            # TTS
            with open('config/tts.yaml', 'r') as f:
                tts_config = yaml.safe_load(f)
            
            from TTS.tts_manager import UnifiedTTSManager
            tts_manager = UnifiedTTSManager(tts_config)
            
            tts_result = await tts_manager.synthesize(text=llm_response)
            
            if tts_result.success:
                self.results['integration']['details'].append("Pipeline E2E: Fonctionnel")
                self.results['integration']['score'] += 80
                self.results['integration']['status'] = 'functional'
                return True
            else:
                self.results['integration']['details'].append("Pipeline E2E: Ã‰chec TTS")
                self.results['integration']['score'] += 40
                self.results['integration']['status'] = 'partial'
                return False
                
        except Exception as e:
            self.results['integration']['status'] = 'error'
            self.results['integration']['details'].append(f"Erreur: {e}")
            return False

    def generate_final_report(self):
        """GÃ©nÃ¨re le rapport final"""
        total_score = sum(comp['score'] for comp in self.results.values())
        max_score = 400  # 100 par composant max
        
        print("\n" + "="*60)
        print("ğŸ¯ RAPPORT FINAL - SuperWhisper V6 Pipeline Status")
        print("="*60)
        
        # Score global
        percentage = (total_score / max_score) * 100
        if percentage >= 90:
            status_global = "ğŸŸ¢ EXCELLENT"
        elif percentage >= 70:
            status_global = "ğŸŸ¡ BON"
        elif percentage >= 50:
            status_global = "ğŸŸ  PARTIEL"
        else:
            status_global = "ğŸ”´ CRITIQUE"
        
        print(f"ğŸ“Š Score Global: {total_score}/{max_score} ({percentage:.1f}%) - {status_global}")
        
        # DÃ©tail par composant
        for component, data in self.results.items():
            component_name = {
                'stt': 'STT (Speech-to-Text)',
                'llm': 'LLM (Language Model)', 
                'tts': 'TTS (Text-to-Speech)',
                'integration': 'IntÃ©gration E2E'
            }[component]
            
            status_icon = {
                'optimal': 'ğŸŸ¢',
                'functional': 'ğŸŸ¡', 
                'fallback': 'ğŸŸ ',
                'partial': 'ğŸŸ ',
                'error': 'ğŸ”´',
                'unknown': 'âšª'
            }[data['status']]
            
            print(f"\n{status_icon} {component_name}: {data['score']}/100 ({data['status'].upper()})")
            for detail in data['details']:
                print(f"   â€¢ {detail}")
        
        # Recommandations
        print(f"\nğŸ”§ RECOMMANDATIONS:")
        
        if self.results['llm']['status'] == 'fallback':
            print("   â€¢ RÃ©soudre connexion Ollama pour vraies rÃ©ponses LLM")
        
        if self.results['stt']['score'] < 70:
            print("   â€¢ Optimiser performance STT")
        
        if self.results['tts']['score'] < 70:
            print("   â€¢ Optimiser performance TTS")
        
        if percentage >= 70:
            print("   â€¢ Pipeline fonctionnel pour usage production")
        else:
            print("   â€¢ Corrections nÃ©cessaires avant usage production")
        
        print(f"\nâš¡ SYSTÃˆME {'OPÃ‰RATIONNEL' if percentage >= 70 else 'NÃ‰CESSITE CORRECTIONS'}")
        print("="*60)

async def main():
    print("ğŸ” SuperWhisper V6 - Ã‰valuation Status Final")
    print("="*60)
    
    evaluator = PipelineStatusEvaluator()
    
    try:
        # Tests sÃ©quentiels
        print("\nğŸ¤ 1/4 Test STT...")
        await evaluator.test_stt_component()
        
        print("\nğŸ§  2/4 Test LLM...")
        await evaluator.test_llm_component()
        
        print("\nğŸ”Š 3/4 Test TTS...")
        await evaluator.test_tts_component()
        
        print("\nğŸ”— 4/4 Test IntÃ©gration...")
        await evaluator.test_integration()
        
        # Rapport final
        evaluator.generate_final_report()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ Test interrompu par l'utilisateur")
    except Exception as e:
        print(f"\nâŒ Erreur lors de l'Ã©valuation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())