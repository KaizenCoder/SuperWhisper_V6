#!/usr/bin/env python3
"""
Script d'installation des d√©pendances Prism STT - SuperWhisper V6
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import subprocess
import platform
from pathlib import Path

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire

print("üîß Installation Prism Dependencies - Configuration GPU RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

def run_command(command, description=""):
    """
    Ex√©cute une commande avec gestion d'erreur.
    
    Args:
        command: Commande √† ex√©cuter
        description: Description de l'op√©ration
    
    Returns:
        bool: True si succ√®s, False sinon
    """
    print(f"\nüîÑ {description}")
    print(f"   Commande: {command}")
    
    try:
        result = subprocess.run(
            command,
            shell=True,
            check=True,
            capture_output=True,
            text=True
        )
        
        if result.stdout:
            print(f"‚úÖ Sortie: {result.stdout.strip()}")
        
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Erreur: {e}")
        if e.stdout:
            print(f"   Stdout: {e.stdout}")
        if e.stderr:
            print(f"   Stderr: {e.stderr}")
        return False

def check_python_version():
    """V√©rifie la version Python"""
    version = sys.version_info
    print(f"üêç Python version: {version.major}.{version.minor}.{version.micro}")
    
    if version.major < 3 or (version.major == 3 and version.minor < 8):
        print("‚ùå Python 3.8+ requis")
        return False
    
    print("‚úÖ Version Python compatible")
    return True

def check_cuda_availability():
    """V√©rifie la disponibilit√© CUDA"""
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("‚ùå CUDA non disponible")
            return False
        
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"üéÆ GPU d√©tect√©e: {gpu_name}")
        print(f"üíæ VRAM: {gpu_memory:.1f}GB")
        
        if "3090" not in gpu_name:
            print(f"‚ö†Ô∏è GPU non optimale: {gpu_name} (RTX 3090 recommand√©e)")
        
        if gpu_memory < 8:
            print(f"‚ö†Ô∏è VRAM faible: {gpu_memory:.1f}GB (8GB+ recommand√©)")
        
        print("‚úÖ CUDA disponible")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è PyTorch non install√© - sera install√© avec les d√©pendances")
        return True

def install_base_dependencies():
    """Installe les d√©pendances de base"""
    print("\nüì¶ Installation des d√©pendances de base...")
    
    base_packages = [
        "torch",
        "torchaudio", 
        "numpy",
        "scipy",
        "librosa",
        "soundfile",
        "pyaudio",
        "asyncio",
        "aiofiles"
    ]
    
    for package in base_packages:
        success = run_command(
            f"pip install {package}",
            f"Installation {package}"
        )
        if not success:
            print(f"‚ö†Ô∏è √âchec installation {package} - continuons...")
    
    return True

def install_faster_whisper():
    """Installe faster-whisper"""
    print("\nüé§ Installation faster-whisper...")
    
    # Installation faster-whisper
    success = run_command(
        "pip install faster-whisper",
        "Installation faster-whisper"
    )
    
    if not success:
        print("‚ö†Ô∏è Tentative installation depuis GitHub...")
        success = run_command(
            "pip install git+https://github.com/guillaumekln/faster-whisper.git",
            "Installation faster-whisper depuis GitHub"
        )
    
    return success

def install_ctranslate2():
    """Installe CTranslate2 pour faster-whisper"""
    print("\n‚ö° Installation CTranslate2...")
    
    # Installation CTranslate2
    success = run_command(
        "pip install ctranslate2",
        "Installation CTranslate2"
    )
    
    return success

def install_audio_dependencies():
    """Installe les d√©pendances audio"""
    print("\nüîä Installation d√©pendances audio...")
    
    audio_packages = [
        "ffmpeg-python",
        "pydub",
        "webrtcvad",
        "noisereduce"
    ]
    
    for package in audio_packages:
        success = run_command(
            f"pip install {package}",
            f"Installation {package}"
        )
        if not success:
            print(f"‚ö†Ô∏è √âchec installation {package} - continuons...")
    
    return True

def install_monitoring_dependencies():
    """Installe les d√©pendances de monitoring"""
    print("\nüìä Installation d√©pendances monitoring...")
    
    monitoring_packages = [
        "prometheus-client",
        "psutil",
        "GPUtil"
    ]
    
    for package in monitoring_packages:
        success = run_command(
            f"pip install {package}",
            f"Installation {package}"
        )
        if not success:
            print(f"‚ö†Ô∏è √âchec installation {package} - continuons...")
    
    return True

def download_whisper_models():
    """T√©l√©charge les mod√®les Whisper"""
    print("\nüì• T√©l√©chargement mod√®les Whisper...")
    
    # Cr√©er r√©pertoire mod√®les
    models_dir = Path("./models/whisper")
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # Script de t√©l√©chargement
    download_script = """
import os
from faster_whisper import WhisperModel

# Configuration GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '1'

print("üì• T√©l√©chargement mod√®le tiny...")
model_tiny = WhisperModel("tiny", device="cuda", download_root="./models/whisper")
print("‚úÖ Mod√®le tiny t√©l√©charg√©")

print("üì• T√©l√©chargement mod√®le large-v2...")
model_large = WhisperModel("large-v2", device="cuda", download_root="./models/whisper")
print("‚úÖ Mod√®le large-v2 t√©l√©charg√©")

print("üéØ Mod√®les pr√™ts pour utilisation")
"""
    
    # Sauvegarder script temporaire
    script_path = Path("temp_download_models.py")
    script_path.write_text(download_script)
    
    try:
        success = run_command(
            f"python {script_path}",
            "T√©l√©chargement mod√®les Whisper"
        )
        
        # Nettoyage
        script_path.unlink()
        
        return success
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur t√©l√©chargement mod√®les: {e}")
        if script_path.exists():
            script_path.unlink()
        return False

def create_requirements_file():
    """Cr√©e un fichier requirements.txt pour Prism STT"""
    print("\nüìù Cr√©ation requirements_prism_stt.txt...")
    
    requirements = """# D√©pendances Prism STT - SuperWhisper V6
# Configuration RTX 3090 (CUDA:1) obligatoire

# Core ML/Audio
torch>=2.0.0
torchaudio>=2.0.0
numpy>=1.21.0
scipy>=1.7.0

# STT Engine
faster-whisper>=0.9.0
ctranslate2>=3.20.0

# Audio Processing
librosa>=0.9.0
soundfile>=0.12.0
pyaudio>=0.2.11
ffmpeg-python>=0.2.0
pydub>=0.25.0
webrtcvad>=2.0.10
noisereduce>=3.0.0

# Async/Performance
asyncio
aiofiles>=23.0.0

# Monitoring
prometheus-client>=0.17.0
psutil>=5.9.0
GPUtil>=1.4.0

# Testing
pytest>=7.0.0
pytest-asyncio>=0.21.0

# Development
black>=23.0.0
flake8>=6.0.0
mypy>=1.0.0
"""
    
    requirements_path = Path("requirements_prism_stt.txt")
    requirements_path.write_text(requirements)
    
    print(f"‚úÖ Fichier cr√©√©: {requirements_path}")
    return True

def test_installation():
    """Test l'installation"""
    print("\nüß™ Test de l'installation...")
    
    test_script = """
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '1'

try:
    import torch
    print(f"‚úÖ PyTorch: {torch.__version__}")
    print(f"‚úÖ CUDA disponible: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    import faster_whisper
    print(f"‚úÖ faster-whisper import√©")
    
    import ctranslate2
    print(f"‚úÖ ctranslate2: {ctranslate2.__version__}")
    
    import numpy as np
    print(f"‚úÖ numpy: {np.__version__}")
    
    import librosa
    print(f"‚úÖ librosa: {librosa.__version__}")
    
    print("üéØ Installation valid√©e avec succ√®s!")
    
except ImportError as e:
    print(f"‚ùå Erreur import: {e}")
    exit(1)
"""
    
    # Sauvegarder script temporaire
    script_path = Path("temp_test_installation.py")
    script_path.write_text(test_script)
    
    try:
        success = run_command(
            f"python {script_path}",
            "Test installation"
        )
        
        # Nettoyage
        script_path.unlink()
        
        return success
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur test installation: {e}")
        if script_path.exists():
            script_path.unlink()
        return False

def main():
    """Installation compl√®te des d√©pendances Prism STT"""
    print("üöÄ Installation des d√©pendances Prism STT - SuperWhisper V6")
    print("=" * 60)
    
    # V√©rifications pr√©alables
    if not check_python_version():
        return False
    
    # Installation √©tape par √©tape
    steps = [
        ("D√©pendances de base", install_base_dependencies),
        ("CTranslate2", install_ctranslate2),
        ("faster-whisper", install_faster_whisper),
        ("D√©pendances audio", install_audio_dependencies),
        ("D√©pendances monitoring", install_monitoring_dependencies),
        ("Fichier requirements", create_requirements_file),
        ("V√©rification CUDA", check_cuda_availability),
        ("Test installation", test_installation)
    ]
    
    success_count = 0
    total_steps = len(steps)
    
    for step_name, step_func in steps:
        print(f"\n{'='*20} {step_name} {'='*20}")
        
        try:
            if step_func():
                success_count += 1
                print(f"‚úÖ {step_name} - Succ√®s")
            else:
                print(f"‚ö†Ô∏è {step_name} - √âchec partiel")
        except Exception as e:
            print(f"‚ùå {step_name} - Erreur: {e}")
    
    # R√©sum√© final
    print("\n" + "="*60)
    print("üìä R√âSUM√â INSTALLATION")
    print("="*60)
    print(f"‚úÖ √âtapes r√©ussies: {success_count}/{total_steps}")
    
    if success_count >= total_steps - 1:  # Tol√©rance 1 √©chec
        print("üéØ Installation R√âUSSIE!")
        print("\nüìã Prochaines √©tapes:")
        print("   1. Tester: python tests/test_prism_integration.py")
        print("   2. Lancer: python STT/backends/prism_stt_backend.py")
        print("   3. Int√©grer dans UnifiedSTTManager")
        return True
    else:
        print("‚ö†Ô∏è Installation PARTIELLE - V√©rifiez les erreurs ci-dessus")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 