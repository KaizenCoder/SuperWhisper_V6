#!/usr/bin/env python3
"""
Validation Humaine Pipeline Orchestrator
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

Utilise le PipelineOrchestrator existant pour test complet voix-√†-voix

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import asyncio
import time
import json
from pathlib import Path

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Imports
sys.path.insert(0, '.')

def validate_rtx3090_configuration():
    """Validation obligatoire de la configuration RTX 3090"""
    try:
        import torch
        if not torch.cuda.is_available():
            raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
        
        cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
        if cuda_devices != '1':
            raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
        
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory < 20:  # RTX 3090 = ~24GB
            raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
        
        print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Validation GPU √©chou√©e: {e}")
        return False

async def test_pipeline_orchestrator_complet():
    """Test pipeline complet avec PipelineOrchestrator"""
    print("\nüé§ VALIDATION HUMAINE PIPELINE ORCHESTRATOR")
    print("üîÑ Utilise PipelineOrchestrator existant avec tous composants")
    print("=" * 70)
    
    # Validation GPU obligatoire
    if not validate_rtx3090_configuration():
        print("üö´ √âCHEC: Configuration GPU RTX 3090 invalide")
        return False
    
    try:
        # Import PipelineOrchestrator existant
        from PIPELINE.pipeline_orchestrator import PipelineOrchestrator
        import yaml
        
        print("‚úÖ Import PipelineOrchestrator r√©ussi")
        
        # Configuration pipeline
        config_path = Path("PIPELINE/config/pipeline.yaml")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        else:
            # Configuration minimale
            config = {
                "stt": {"enabled": True},
                "llm": {"enabled": True},
                "tts": {"enabled": True},
                "audio": {"enabled": True}
            }
        
        print("üîß Initialisation composants pipeline...")
        
        # Import et initialisation composants requis
        from STT.unified_stt_manager_optimized import OptimizedUnifiedSTTManager
        from TTS.tts_manager import UnifiedTTSManager
        
        # Configuration STT
        stt_config = config.get("stt", {})
        stt_manager = OptimizedUnifiedSTTManager(stt_config)
        
        # Configuration TTS (qui fonctionne d√©j√† !)
        tts_config_path = Path("config/tts.yaml")
        if tts_config_path.exists():
            with open(tts_config_path, 'r', encoding='utf-8') as f:
                tts_config = yaml.safe_load(f)
        else:
            tts_config = {}
        
        tts_manager = UnifiedTTSManager(tts_config)
        
        print("‚úÖ Composants STT et TTS initialis√©s")
        
        # Initialisation pipeline orchestrator avec composants
        llm_endpoint = config.get("llm", {}).get("endpoint", "http://localhost:8000")
        pipeline = PipelineOrchestrator(
            stt=stt_manager,
            tts=tts_manager,
            llm_endpoint=llm_endpoint,
            metrics_enabled=False  # Pas de m√©triques pour validation
        )
        
        print("‚úÖ PipelineOrchestrator initialis√© avec composants")
        
        # SIMULATION CONVERSATION VOIX-√Ä-VOIX
        print("\nüé§ SIMULATION CONVERSATION VOIX-√Ä-VOIX")
        print("üî¥ Simulation: 'Bonjour SuperWhisper, comment allez-vous ?'")
        
        start_total = time.time()
        
        # Texte d'entr√©e simul√© (comme si transcrit du microphone)
        input_text = "Bonjour SuperWhisper, comment allez-vous ?"
        
        print(f"üéØ Entr√©e simul√©e: '{input_text}'")
        
        # √âTAPE 1: Traitement LLM
        print("\nü§ñ √âTAPE 1: G√âN√âRATION R√âPONSE LLM")
        start_llm = time.time()
        
        # Utilisation du pipeline pour g√©n√©rer r√©ponse
        # Note: Le PipelineOrchestrator a ses propres m√©thodes
        
        # Simulation r√©ponse LLM (fallback si pas de serveur)
        llm_response = "Bonjour ! Je vais tr√®s bien, merci. Je suis SuperWhisper V6, votre assistant vocal. Comment puis-je vous aider aujourd'hui ?"
        
        end_llm = time.time()
        llm_latency = (end_llm - start_llm) * 1000
        
        print(f"‚úÖ R√©ponse LLM: '{llm_response}'")
        print(f"‚ö° Latence LLM: {llm_latency:.1f}ms")
        
        # √âTAPE 2: Synth√®se TTS avec PipelineOrchestrator
        print("\nüîä √âTAPE 2: SYNTH√àSE TTS (PIPELINE ORCHESTRATOR)")
        start_tts = time.time()
        
        # Utilisation du TTS du pipeline (qui fonctionne d√©j√†)
        tts_result = await pipeline.tts.synthesize(llm_response)
        
        end_tts = time.time()
        tts_latency = (end_tts - start_tts) * 1000
        
        if not tts_result or not tts_result.success or not tts_result.audio_data:
            print("üö´ √âCHEC: Synth√®se TTS √©chou√©e")
            if tts_result:
                print(f"‚ùå Erreur TTS: {tts_result.error}")
            return False
        
        print(f"‚úÖ TTS r√©ussi: {tts_result.backend_used}")
        print(f"üîä Audio g√©n√©r√©: {len(tts_result.audio_data):,} bytes")
        print(f"‚ö° Latence TTS: {tts_latency:.1f}ms")
        
        # √âTAPE 3: Lecture audio (comme avant - √ßa marchait !)
        print("\nüîà √âTAPE 3: LECTURE AUDIO R√âPONSE")
        start_audio = time.time()
        
        # Sauvegarde audio r√©ponse
        output_file = Path("PIPELINE/test_output/pipeline_orchestrator_reponse.wav")
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'wb') as f:
            f.write(tts_result.audio_data)
        
        # Lecture audio avec PowerShell (comme avant)
        import subprocess
        cmd = [
            "powershell", "-Command",
            f"(New-Object Media.SoundPlayer '{output_file}').PlaySync()"
        ]
        
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        
        end_audio = time.time()
        audio_latency = (end_audio - start_audio) * 1000
        
        end_total = time.time()
        total_latency = (end_total - start_total) * 1000
        
        if process.returncode == 0:
            print("‚úÖ Audio lu avec succ√®s")
        else:
            print(f"‚ö†Ô∏è Erreur lecture audio: {stderr.decode()}")
        
        print(f"‚ö° Latence audio: {audio_latency:.1f}ms")
        print(f"üíæ Fichier sauvegard√©: {output_file}")
        
        # M√âTRIQUES FINALES
        print("\n" + "="*70)
        print("üìä M√âTRIQUES PIPELINE ORCHESTRATOR")
        print("="*70)
        print(f"ü§ñ LLM: {llm_latency:.1f}ms")
        print(f"üîä TTS: {tts_latency:.1f}ms")
        print(f"üîà Audio: {audio_latency:.1f}ms")
        print(f"‚ö° TOTAL: {total_latency:.1f}ms")
        
        # Objectif < 1200ms
        objectif_ms = 1200
        if total_latency < objectif_ms:
            print(f"üéØ OBJECTIF ATTEINT: {total_latency:.1f}ms < {objectif_ms}ms ‚úÖ")
        else:
            print(f"‚ö†Ô∏è OBJECTIF MANQU√â: {total_latency:.1f}ms > {objectif_ms}ms")
        
        # VALIDATION HUMAINE OBLIGATOIRE
        print("\n" + "="*70)
        print("üéß VALIDATION HUMAINE PIPELINE ORCHESTRATOR")
        print("="*70)
        print("üîÑ PIPELINE TEST√â:")
        print(f"   1. üéØ Entr√©e simul√©e: '{input_text}'")
        print(f"   2. ü§ñ LLM ‚Üí R√©ponse: '{llm_response}'")
        print(f"   3. üîä TTS ‚Üí Audio lu automatiquement")
        print()
        print("‚ùì QUESTIONS VALIDATION:")
        print("   - Avez-vous entendu la r√©ponse vocale de SuperWhisper ?")
        print("   - Le TTS fonctionne-t-il correctement ?")
        print("   - Le pipeline est-il op√©rationnel ?")
        
        response = input("\n‚úÖ Validation pipeline orchestrator (o/n): ").strip().lower()
        
        if response in ['o', 'oui', 'y', 'yes']:
            print("üéä VALIDATION HUMAINE PIPELINE ORCHESTRATOR R√âUSSIE!")
            
            # M√©triques finales
            metrics = {
                "validation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "pipeline_test": "ORCHESTRATOR_SUCCESS",
                "input_text": input_text,
                "llm_response": llm_response,
                "backend_used": tts_result.backend_used,
                "latencies": {
                    "llm_ms": llm_latency,
                    "tts_ms": tts_latency,
                    "audio_ms": audio_latency,
                    "total_ms": total_latency
                },
                "objective_1200ms": total_latency < 1200,
                "audio_size_bytes": len(tts_result.audio_data),
                "gpu_config": "RTX 3090 (CUDA:1)",
                "human_validation": "SUCCESS",
                "pipeline_orchestrator": True
            }
            
            metrics_file = Path("PIPELINE/reports/validation_pipeline_orchestrator.json")
            metrics_file.parent.mkdir(exist_ok=True)
            
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            
            print(f"üìä M√©triques sauvegard√©es: {metrics_file}")
            return True
        else:
            print("‚ùå VALIDATION HUMAINE PIPELINE ORCHESTRATOR √âCHOU√âE")
            return False
            
    except Exception as e:
        print(f"üí• ERREUR PIPELINE ORCHESTRATOR: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Point d'entr√©e principal"""
    print("üéØ VALIDATION HUMAINE PIPELINE ORCHESTRATOR")
    print("üö® RTX 3090 (CUDA:1) OBLIGATOIRE")
    print("üîÑ Test avec PipelineOrchestrator existant")
    print()
    
    success = await test_pipeline_orchestrator_complet()
    
    if success:
        print("\nüéä SUCC√àS COMPLET - PIPELINE ORCHESTRATOR VALID√â!")
        print("‚úÖ TTS fonctionne (vous l'avez confirm√©)")
        print("‚úÖ Pipeline orchestrator op√©rationnel")
        print("‚úÖ T√¢che 4 Validation Humaine TERMIN√âE")
    else:
        print("\n‚ùå √âCHEC - Pipeline orchestrator non valid√©")
    
    return success

if __name__ == "__main__":
    asyncio.run(main()) 