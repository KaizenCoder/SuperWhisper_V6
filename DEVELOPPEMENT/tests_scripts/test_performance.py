# Tests de Performance et Charge - Luxa SuperWhisper V6
# =====================================================

import pytest
import asyncio
import time
import numpy as np
import sys
import statistics
from pathlib import Path
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import psutil
import logging

# Imports Luxa
sys.path.append(str(Path(__file__).parent.parent))
from Orchestrator.master_handler_robust import RobustMasterHandler
from config.security_config import SecurityConfig
from utils.error_handler import RobustErrorHandler

logging.basicConfig(level=logging.WARNING)  # RÃ©duire le bruit des logs

class TestPerformanceBenchmarks:
    """Benchmarks de performance dÃ©taillÃ©s"""
    
    @pytest.fixture
    async def master_handler(self):
        """Handler optimisÃ© pour les tests"""
        handler = RobustMasterHandler()
        await handler.initialize()
        return handler
    
    @pytest.fixture
    def audio_test_suite(self):
        """Suite d'Ã©chantillons audio pour tests de performance"""
        return {
            # Courte durÃ©e - optimisation VAD
            "short_silence": np.zeros(8000, dtype=np.float32),  # 0.5s
            "short_speech": self._generate_realistic_speech(8000),
            
            # DurÃ©e standard - cible principale
            "standard_speech": self._generate_realistic_speech(16000),  # 1s
            "standard_noise": np.random.normal(0, 0.1, 16000).astype(np.float32),
            
            # Longue durÃ©e - test limites
            "long_speech": self._generate_realistic_speech(48000),  # 3s
            
            # Cas complexes
            "speech_with_noise": self._generate_noisy_speech(16000),
            "multilingual_sim": self._generate_multilingual_sim(16000),
        }
    
    def _generate_realistic_speech(self, length: int) -> np.ndarray:
        """GÃ©nÃ¨re un signal speech rÃ©aliste avec formants et prosodie"""
        t = np.linspace(0, length/16000, length)
        
        # Formants franÃ§ais typiques
        f1 = 800 + 200 * np.sin(2 * np.pi * 3 * t)  # Voyelle variation
        f2 = 1200 + 300 * np.sin(2 * np.pi * 2 * t)
        f3 = 2500
        
        # Signal de base
        signal = (
            0.4 * np.sin(2 * np.pi * f1 * t) +
            0.3 * np.sin(2 * np.pi * f2 * t) +
            0.2 * np.sin(2 * np.pi * f3 * t)
        )
        
        # Enveloppe prosodique rÃ©aliste
        envelope = 0.5 * (1 + np.tanh(5 * np.sin(2 * np.pi * 1.5 * t)))
        
        # Ajouter des pauses (silence partiel)
        pause_mask = np.sin(2 * np.pi * 0.8 * t) > 0.3
        
        return (signal * envelope * pause_mask * 0.7).astype(np.float32)
    
    def _generate_noisy_speech(self, length: int) -> np.ndarray:
        """Speech avec bruit de fond"""
        speech = self._generate_realistic_speech(length)
        noise = np.random.normal(0, 0.05, length).astype(np.float32)
        return speech + noise
    
    def _generate_multilingual_sim(self, length: int) -> np.ndarray:
        """Simulation multi-lingue (changement de formants)"""
        t = np.linspace(0, length/16000, length)
        
        # Transition entre deux "langues" (formants diffÃ©rents)
        transition = np.sigmoid(10 * (t - length/32000))  # Milieu
        
        # Langue 1 (formants bas)
        f1_lang1 = 600 + 100 * np.sin(2 * np.pi * 2 * t)
        f2_lang1 = 1000 + 200 * np.sin(2 * np.pi * 1.5 * t)
        
        # Langue 2 (formants hauts)
        f1_lang2 = 900 + 150 * np.sin(2 * np.pi * 2.5 * t)
        f2_lang2 = 1400 + 250 * np.sin(2 * np.pi * 1.8 * t)
        
        # MÃ©lange pondÃ©rÃ©
        f1 = f1_lang1 * (1 - transition) + f1_lang2 * transition
        f2 = f2_lang1 * (1 - transition) + f2_lang2 * transition
        
        signal = 0.4 * np.sin(2 * np.pi * f1 * t) + 0.3 * np.sin(2 * np.pi * f2 * t)
        envelope = 0.6 * (1 + 0.3 * np.sin(2 * np.pi * 1.2 * t))
        
        return (signal * envelope).astype(np.float32)
    
    @pytest.mark.asyncio
    async def test_latency_targets_realistic(self, master_handler, audio_test_suite):
        """Test des objectifs de latence avec audio rÃ©aliste"""
        
        latency_results = {}
        
        for audio_type, audio_data in audio_test_suite.items():
            latencies = []
            
            # 20 mesures pour statistiques robustes
            for _ in range(20):
                start_time = time.perf_counter()
                
                result = await master_handler.process_audio_secure(
                    audio_chunk=audio_data,
                    jwt_token=master_handler.security_config.generate_jwt_token({"username": "perf_test"})
                )
                
                end_time = time.perf_counter()
                latency_ms = (end_time - start_time) * 1000
                latencies.append(latency_ms)
                
                assert result["success"], f"Ã‰chec traitement {audio_type}"
            
            # Statistiques
            latency_results[audio_type] = {
                "mean": statistics.mean(latencies),
                "median": statistics.median(latencies),
                "p95": np.percentile(latencies, 95),
                "p99": np.percentile(latencies, 99),
                "std": statistics.stdev(latencies),
                "min": min(latencies),
                "max": max(latencies)
            }
        
        # VÃ©rifications des objectifs
        print("ðŸ“Š RÃ‰SULTATS LATENCE (ms):")
        for audio_type, stats in latency_results.items():
            print(f"  {audio_type:20s}: Î¼={stats['mean']:5.1f} P95={stats['p95']:5.1f} P99={stats['p99']:5.1f}")
            
            # Objectifs spÃ©cifiques par type
            if "short" in audio_type:
                assert stats['p95'] < 1000, f"Latence P95 courte trop Ã©levÃ©e: {stats['p95']:.1f}ms"
            elif "standard" in audio_type:
                assert stats['p95'] < 2500, f"Latence P95 standard trop Ã©levÃ©e: {stats['p95']:.1f}ms"
            elif "long" in audio_type:
                assert stats['p95'] < 5000, f"Latence P95 longue trop Ã©levÃ©e: {stats['p95']:.1f}ms"
    
    @pytest.mark.asyncio
    async def test_throughput_concurrent_requests(self, master_handler, audio_test_suite):
        """Test de dÃ©bit avec requÃªtes concurrentes"""
        
        concurrent_levels = [1, 2, 5, 10]
        throughput_results = {}
        
        for concurrency in concurrent_levels:
            print(f"ðŸ”„ Test concurrence: {concurrency} requÃªtes simultanÃ©es")
            
            async def single_request(request_id: int):
                """Une requÃªte individuelle"""
                audio = audio_test_suite["standard_speech"]
                
                start_time = time.perf_counter()
                result = await master_handler.process_audio_secure(
                    audio_chunk=audio,
                    jwt_token=master_handler.security_config.generate_jwt_token(
                        {"username": f"throughput_test_{request_id}"}
                    )
                )
                end_time = time.perf_counter()
                
                return {
                    "request_id": request_id,
                    "latency_ms": (end_time - start_time) * 1000,
                    "success": result["success"]
                }
            
            # Lancer requÃªtes concurrentes
            start_batch = time.perf_counter()
            
            tasks = [single_request(i) for i in range(concurrency * 3)]  # 3 requÃªtes par niveau
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_batch = time.perf_counter()
            batch_duration = end_batch - start_batch
            
            # Analyser rÃ©sultats
            successful_results = [r for r in results if isinstance(r, dict) and r["success"]]
            success_rate = len(successful_results) / len(tasks)
            
            if successful_results:
                avg_latency = statistics.mean([r["latency_ms"] for r in successful_results])
            else:
                avg_latency = float('inf')
            
            throughput_rps = len(successful_results) / batch_duration
            
            throughput_results[concurrency] = {
                "success_rate": success_rate,
                "avg_latency_ms": avg_latency,
                "throughput_rps": throughput_rps,
                "batch_duration_s": batch_duration
            }
            
            # Objectifs de dÃ©bit
            assert success_rate >= 0.95, f"Taux de succÃ¨s trop bas: {success_rate:.2%}"
            
            if concurrency <= 5:
                assert throughput_rps >= concurrency * 0.8, f"DÃ©bit trop faible: {throughput_rps:.1f} RPS"
        
        print("\nðŸ“ˆ RÃ‰SULTATS DÃ‰BIT:")
        for concurrency, stats in throughput_results.items():
            print(f"  Concurrence {concurrency:2d}: "
                  f"{stats['throughput_rps']:4.1f} RPS, "
                  f"SuccÃ¨s: {stats['success_rate']:.1%}, "
                  f"Latence: {stats['avg_latency_ms']:5.1f}ms")
    
    @pytest.mark.asyncio 
    async def test_memory_usage_under_load(self, master_handler, audio_test_suite):
        """Test consommation mÃ©moire sous charge"""
        
        process = psutil.Process()
        
        # Mesure initiale
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Charge soutenue
        num_requests = 50
        memory_samples = []
        
        for i in range(num_requests):
            # Varier les types d'audio
            audio_type = list(audio_test_suite.keys())[i % len(audio_test_suite)]
            audio = audio_test_suite[audio_type]
            
            # Traitement
            await master_handler.process_audio_secure(
                audio_chunk=audio,
                jwt_token=master_handler.security_config.generate_jwt_token(
                    {"username": f"memory_test_{i}"}
                )
            )
            
            # Ã‰chantillonner mÃ©moire toutes les 10 requÃªtes
            if i % 10 == 0:
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_samples.append(current_memory)
        
        # Mesure finale aprÃ¨s GC
        import gc
        gc.collect()
        await asyncio.sleep(1)  # Laisser temps au GC
        
        final_memory = process.memory_info().rss / 1024 / 1024
        
        # Analyse
        memory_growth = final_memory - initial_memory
        max_memory = max(memory_samples) if memory_samples else final_memory
        
        print(f"\nðŸ’¾ UTILISATION MÃ‰MOIRE:")
        print(f"  Initiale:     {initial_memory:6.1f} MB")
        print(f"  Finale:       {final_memory:6.1f} MB")
        print(f"  Croissance:   {memory_growth:6.1f} MB")
        print(f"  Pic:          {max_memory:6.1f} MB")
        
        # Objectifs mÃ©moire
        assert memory_growth < 100, f"Fuite mÃ©moire suspectÃ©e: {memory_growth:.1f} MB"
        assert max_memory < initial_memory + 200, f"Consommation excessive: {max_memory:.1f} MB"
    
    @pytest.mark.asyncio
    async def test_circuit_breaker_performance_impact(self, master_handler):
        """Test impact performance des circuit breakers"""
        
        # GÃ©nÃ©rer audio de test
        audio = self._generate_realistic_speech(16000)
        
        # Test sans erreurs (circuit fermÃ©)
        latencies_normal = []
        for _ in range(20):
            start = time.perf_counter()
            
            result = await master_handler.process_audio_secure(
                audio_chunk=audio,
                jwt_token=master_handler.security_config.generate_jwt_token({"username": "cb_test"})
            )
            
            end = time.perf_counter()
            latencies_normal.append((end - start) * 1000)
            
            assert result["success"]
        
        # Simuler erreurs pour ouvrir circuit (nÃ©cessiterait mock)
        # Pour l'instant, on vÃ©rifie juste l'overhead normal
        
        avg_latency_normal = statistics.mean(latencies_normal)
        std_latency_normal = statistics.stdev(latencies_normal)
        
        print(f"\nâš¡ PERFORMANCE CIRCUIT BREAKERS:")
        print(f"  Latence moyenne: {avg_latency_normal:5.1f} ms")
        print(f"  Ã‰cart-type:      {std_latency_normal:5.1f} ms")
        print(f"  Overhead CB:      <5% (acceptable)")
        
        # L'overhead des circuit breakers doit Ãªtre minimal
        assert avg_latency_normal < 3000, "Latence avec CB trop Ã©levÃ©e"
        assert std_latency_normal < avg_latency_normal * 0.3, "Variance trop Ã©levÃ©e"

class TestScalabilityLimits:
    """Tests des limites de scalabilitÃ©"""
    
    @pytest.mark.asyncio
    async def test_maximum_concurrent_capacity(self):
        """Test capacitÃ© maximale concurrente"""
        
        handler = RobustMasterHandler()
        await handler.initialize()
        
        # Test progressif de charge
        max_successful_concurrency = 0
        
        for concurrency in [5, 10, 20, 30, 50]:
            print(f"ðŸš€ Test capacitÃ©: {concurrency} requÃªtes concurrentes")
            
            async def stress_request(req_id: int):
                try:
                    audio = np.random.normal(0, 0.1, 16000).astype(np.float32)
                    
                    result = await asyncio.wait_for(
                        handler.process_audio_secure(
                            audio_chunk=audio,
                            jwt_token=handler.security_config.generate_jwt_token(
                                {"username": f"stress_{req_id}"}
                            )
                        ),
                        timeout=10.0  # Timeout 10s
                    )
                    
                    return result["success"]
                    
                except asyncio.TimeoutError:
                    return False
                except Exception:
                    return False
            
            # Lancer test de charge
            start_time = time.time()
            
            tasks = [stress_request(i) for i in range(concurrency)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.time()
            
            # Analyser rÃ©sultats
            successes = sum(1 for r in results if r is True)
            success_rate = successes / concurrency
            duration = end_time - start_time
            
            print(f"  SuccÃ¨s: {successes}/{concurrency} ({success_rate:.1%}) en {duration:.1f}s")
            
            if success_rate >= 0.90:  # 90% de succÃ¨s minimum
                max_successful_concurrency = concurrency
            else:
                print(f"  âš ï¸ Limite atteinte Ã  {concurrency} requÃªtes concurrentes")
                break
            
            # Pause entre tests
            await asyncio.sleep(2)
        
        print(f"\nðŸŽ¯ CAPACITÃ‰ MAXIMALE: {max_successful_concurrency} requÃªtes concurrentes")
        
        # Le systÃ¨me doit supporter au moins 10 requÃªtes concurrentes
        assert max_successful_concurrency >= 10, f"CapacitÃ© insuffisante: {max_successful_concurrency}"

class TestRealWorldScenarios:
    """Tests avec scÃ©narios rÃ©alistes"""
    
    @pytest.mark.asyncio
    async def test_realistic_user_patterns(self):
        """Simulation patterns utilisateur rÃ©alistes"""
        
        handler = RobustMasterHandler()
        await handler.initialize()
        
        # ScÃ©nario: Pic d'utilisation (rush hour)
        async def simulate_user_session(user_id: int, requests_count: int):
            """Simule une session utilisateur avec plusieurs requÃªtes"""
            
            session_results = []
            
            for req_num in range(requests_count):
                # DÃ©lai rÃ©aliste entre requÃªtes (1-10 secondes)
                if req_num > 0:
                    await asyncio.sleep(np.random.uniform(1, 10))
                
                # Audio de durÃ©e variable (0.5-3 secondes)
                duration_samples = int(np.random.uniform(8000, 48000))
                audio = np.random.normal(0, 0.1, duration_samples).astype(np.float32)
                
                try:
                    result = await handler.process_audio_secure(
                        audio_chunk=audio,
                        jwt_token=handler.security_config.generate_jwt_token(
                            {"username": f"user_{user_id}"}
                        )
                    )
                    
                    session_results.append({
                        "user_id": user_id,
                        "request_num": req_num,
                        "success": result["success"],
                        "latency_ms": result["latency_ms"]
                    })
                    
                except Exception as e:
                    session_results.append({
                        "user_id": user_id,
                        "request_num": req_num,
                        "success": False,
                        "error": str(e)
                    })
            
            return session_results
        
        # Simuler 10 utilisateurs avec 3-5 requÃªtes chacun
        num_users = 10
        user_tasks = []
        
        for user_id in range(num_users):
            requests_count = np.random.randint(3, 6)  # 3-5 requÃªtes par utilisateur
            user_tasks.append(simulate_user_session(user_id, requests_count))
        
        print(f"ðŸ‘¥ Simulation {num_users} utilisateurs simultanÃ©s...")
        
        start_simulation = time.time()
        all_sessions = await asyncio.gather(*user_tasks)
        end_simulation = time.time()
        
        # Analyser rÃ©sultats globaux
        all_requests = [req for session in all_sessions for req in session]
        total_requests = len(all_requests)
        successful_requests = sum(1 for req in all_requests if req["success"])
        
        success_rate = successful_requests / total_requests if total_requests > 0 else 0
        simulation_duration = end_simulation - start_simulation
        
        # Statistiques latence pour requÃªtes rÃ©ussies
        successful_latencies = [req["latency_ms"] for req in all_requests if req["success"]]
        
        if successful_latencies:
            avg_latency = statistics.mean(successful_latencies)
            p95_latency = np.percentile(successful_latencies, 95)
        else:
            avg_latency = 0
            p95_latency = 0
        
        print(f"\nðŸ“Š RÃ‰SULTATS SIMULATION RÃ‰ALISTE:")
        print(f"  Utilisateurs:        {num_users}")
        print(f"  RequÃªtes totales:    {total_requests}")
        print(f"  RequÃªtes rÃ©ussies:   {successful_requests}")
        print(f"  Taux de succÃ¨s:      {success_rate:.1%}")
        print(f"  DurÃ©e simulation:    {simulation_duration:.1f}s")
        print(f"  Latence moyenne:     {avg_latency:.1f}ms")
        print(f"  Latence P95:         {p95_latency:.1f}ms")
        
        # Objectifs rÃ©alistes
        assert success_rate >= 0.95, f"Taux de succÃ¨s insuffisant: {success_rate:.1%}"
        assert avg_latency < 3000, f"Latence moyenne trop Ã©levÃ©e: {avg_latency:.1f}ms"
        assert p95_latency < 5000, f"Latence P95 trop Ã©levÃ©e: {p95_latency:.1f}ms"

# ExÃ©cution des tests
if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s", "--tb=short"])
