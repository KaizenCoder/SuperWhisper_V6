#!/usr/bin/env python3
"""
ü¶ô VALIDATION NOUS-HERMES-2-MISTRAL-7B-DPO - SUPERWHISPER V6
===========================================================
Script validation compl√®te pour Nous-Hermes-2-Mistral-7B-DPO

MOD√àLE CIBLE:
- Nom: nous-hermes-2-mistral-7b-dpo
- Fichier: Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S.gguf (3.9GB)
- Avantages: DPO-tuned, conversation excellente, fran√ßais natif

MISSION:
- Validation latence < 600ms pour pipeline < 2.5s
- Tests conversation fran√ßaise approfondie
- √âvaluation qualit√© contextuelle
- S√©lection param√®tres optimaux

Usage: python scripts/test_nous_hermes_validation.py

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import asyncio
import time
import logging
import statistics
from typing import Dict, Any, List
import httpx

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
# RTX 5060 (CUDA:0) = INTERDITE - RTX 3090 (CUDA:1) = OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Configuration logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("nous_hermes_validation")

class NousHermesValidator:
    """Validateur complet pour Nous-Hermes-2-Mistral-7B-DPO"""
    
    def __init__(self):
        self.model_name = "nous-hermes-2-mistral-7b-dpo:latest"
        self.ollama_url = "http://localhost:11434"
        self.target_latency = 600  # ms - objectif pour pipeline < 2.5s
        
        # Prompts de test conversation fran√ßaise
        self.test_prompts = [
            {
                "category": "greeting",
                "prompt": "Bonjour ! Comment puis-je vous aider aujourd'hui ?",
                "expected_keywords": ["bonjour", "aide", "service"]
            },
            {
                "category": "time_question", 
                "prompt": "Quelle heure est-il maintenant ?",
                "expected_keywords": ["heure", "temps", "maintenant"]
            },
            {
                "category": "gratitude",
                "prompt": "Merci beaucoup pour votre aide pr√©cieuse !",
                "expected_keywords": ["merci", "plaisir", "service"]
            },
            {
                "category": "weather",
                "prompt": "Quel temps fait-il dehors ?",
                "expected_keywords": ["temps", "m√©t√©o", "information"]
            },
            {
                "category": "conversation",
                "prompt": "Racontez-moi une histoire courte et amusante.",
                "expected_keywords": ["histoire", "fois", "alors"]
            },
            {
                "category": "help_request",
                "prompt": "J'ai besoin d'aide pour organiser ma journ√©e.",
                "expected_keywords": ["aide", "organiser", "planifier"]
            },
            {
                "category": "casual_chat",
                "prompt": "Comment √ßa va de votre c√¥t√© ?",
                "expected_keywords": ["bien", "merci", "assistant"]
            }
        ]
        
        # Configurations de test
        self.test_configs = [
            {
                "name": "optimal",
                "temperature": 0.7,
                "max_tokens": 50,
                "top_p": 0.9
            },
            {
                "name": "fast",
                "temperature": 0.3,
                "max_tokens": 30,
                "top_p": 0.8
            },
            {
                "name": "creative",
                "temperature": 0.9,
                "max_tokens": 80,
                "top_p": 0.95
            }
        ]
    
    async def validate_model_available(self):
        """Valider que le mod√®le est disponible dans Ollama"""
        logger.info(f"üîç Validation disponibilit√© mod√®le {self.model_name}...")
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                
                if response.status_code == 200:
                    data = response.json()
                    models = [model['name'] for model in data.get('models', [])]
                    
                    if self.model_name in models:
                        logger.info(f"‚úÖ Mod√®le {self.model_name} disponible")
                        return True
                    else:
                        logger.error(f"‚ùå Mod√®le {self.model_name} non trouv√©")
                        logger.info(f"üìã Mod√®les disponibles: {models}")
                        return False
                else:
                    logger.error(f"‚ùå Erreur Ollama: {response.status_code}")
                    return False
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur validation: {e}")
            return False
    
    async def test_single_prompt(self, prompt_data: Dict, config: Dict):
        """Tester un prompt avec une configuration donn√©e"""
        prompt = prompt_data["prompt"]
        category = prompt_data["category"]
        
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": config["temperature"],
                    "num_predict": config["max_tokens"],
                    "top_p": config["top_p"]
                }
            }
            
            start_time = time.time()
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(f"{self.ollama_url}/api/generate", json=payload)
            
            latency = (time.time() - start_time) * 1000
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get("response", "").strip()
                
                # √âvaluation qualit√©
                quality_score = self.evaluate_response_quality(
                    response_text, 
                    prompt_data["expected_keywords"],
                    category
                )
                
                return {
                    "success": True,
                    "latency": latency,
                    "response": response_text,
                    "quality_score": quality_score,
                    "category": category,
                    "config": config["name"]
                }
            else:
                logger.error(f"‚ùå Erreur HTTP {response.status_code}")
                return {"success": False, "latency": 0, "quality_score": 0}
                
        except Exception as e:
            logger.error(f"‚ùå Erreur test: {e}")
            return {"success": False, "latency": 0, "quality_score": 0}
    
    def evaluate_response_quality(self, response: str, expected_keywords: List[str], category: str) -> float:
        """√âvaluer la qualit√© d'une r√©ponse (0-10)"""
        if not response or len(response) < 5:
            return 0.0
        
        score = 5.0  # Score de base
        
        # V√©rification longueur appropri√©e
        if 10 <= len(response) <= 200:
            score += 1.0
        elif len(response) < 10:
            score -= 2.0
        
        # V√©rification fran√ßais
        french_indicators = ["je", "vous", "est", "pour", "avec", "dans", "sur", "une", "des", "les"]
        french_count = sum(1 for word in french_indicators if word in response.lower())
        score += min(french_count * 0.2, 1.5)
        
        # V√©rification mots-cl√©s contextuels
        keyword_matches = sum(1 for keyword in expected_keywords if keyword.lower() in response.lower())
        if keyword_matches > 0:
            score += keyword_matches * 0.5
        
        # V√©rification coh√©rence conversationnelle
        if category == "greeting" and any(word in response.lower() for word in ["bonjour", "salut", "aide"]):
            score += 1.0
        elif category == "gratitude" and any(word in response.lower() for word in ["plaisir", "rien", "service"]):
            score += 1.0
        elif category == "time_question" and any(word in response.lower() for word in ["heure", "temps", "information"]):
            score += 1.0
        
        # P√©nalit√©s
        if "je suis pr√™t" in response.lower():
            score -= 3.0  # R√©ponse r√©p√©titive
        
        if response.count(".") == 0 and len(response) > 20:
            score -= 0.5  # Manque de ponctuation
        
        return max(0.0, min(10.0, score))
    
    async def run_comprehensive_validation(self):
        """Validation compl√®te du mod√®le"""
        logger.info("üöÄ D√âMARRAGE VALIDATION NOUS-HERMES-2-MISTRAL-7B-DPO")
        
        # 1. Validation disponibilit√©
        if not await self.validate_model_available():
            return False
        
        all_results = []
        
        # 2. Tests avec diff√©rentes configurations
        for config in self.test_configs:
            logger.info(f"üîß Test configuration '{config['name']}'...")
            
            config_results = []
            
            for prompt_data in self.test_prompts:
                result = await self.test_single_prompt(prompt_data, config)
                
                if result["success"]:
                    config_results.append(result)
                    logger.info(f"‚úÖ {prompt_data['category']}: {result['latency']:.1f}ms, qualit√©: {result['quality_score']:.1f}/10")
                else:
                    logger.error(f"‚ùå {prompt_data['category']}: √©chec")
            
            all_results.extend(config_results)
        
        # 3. Analyse r√©sultats
        return self.analyze_results(all_results)
    
    def analyze_results(self, results: List[Dict]) -> bool:
        """Analyser les r√©sultats de validation"""
        if not results:
            logger.error("‚ùå Aucun r√©sultat √† analyser")
            return False
        
        successful_results = [r for r in results if r["success"]]
        
        if not successful_results:
            logger.error("‚ùå Aucun test r√©ussi")
            return False
        
        # Calculs statistiques
        latencies = [r["latency"] for r in successful_results]
        quality_scores = [r["quality_score"] for r in successful_results]
        
        avg_latency = statistics.mean(latencies)
        min_latency = min(latencies)
        max_latency = max(latencies)
        avg_quality = statistics.mean(quality_scores)
        
        success_rate = len(successful_results) / len(results) * 100
        
        # Rapport final
        logger.info("\n" + "="*60)
        logger.info("üìä RAPPORT VALIDATION NOUS-HERMES-2-MISTRAL-7B-DPO")
        logger.info("="*60)
        logger.info(f"‚úÖ Taux de succ√®s: {success_rate:.1f}% ({len(successful_results)}/{len(results)})")
        logger.info(f"‚ö° Latence moyenne: {avg_latency:.1f}ms")
        logger.info(f"‚ö° Latence min/max: {min_latency:.1f}ms / {max_latency:.1f}ms")
        logger.info(f"üéØ Qualit√© moyenne: {avg_quality:.1f}/10")
        
        # √âvaluation objectif latence
        if avg_latency <= self.target_latency:
            logger.info(f"‚úÖ Objectif latence < {self.target_latency}ms: ATTEINT")
            latency_ok = True
        else:
            logger.warning(f"‚ö†Ô∏è Objectif latence < {self.target_latency}ms: D√âPASS√â (+{avg_latency - self.target_latency:.1f}ms)")
            latency_ok = False
        
        # √âvaluation qualit√©
        quality_ok = avg_quality >= 7.0
        if quality_ok:
            logger.info(f"‚úÖ Objectif qualit√© ‚â• 7.0/10: ATTEINT")
        else:
            logger.warning(f"‚ö†Ô∏è Objectif qualit√© ‚â• 7.0/10: NON ATTEINT")
        
        # Projection pipeline complet
        stt_latency = 833  # ms (valid√© pr√©c√©demment)
        tts_latency = 975  # ms (valid√© pr√©c√©demment)
        total_pipeline = stt_latency + avg_latency + tts_latency
        
        logger.info("\nüìä PROJECTION PIPELINE COMPLET:")
        logger.info(f"üé§ STT: {stt_latency}ms")
        logger.info(f"ü§ñ LLM: {avg_latency:.1f}ms")
        logger.info(f"üîä TTS: {tts_latency}ms")
        logger.info(f"üìä TOTAL: {total_pipeline:.1f}ms ({total_pipeline/1000:.2f}s)")
        
        pipeline_ok = total_pipeline <= 2500  # 2.5s objectif r√©aliste
        if pipeline_ok:
            logger.info("‚úÖ Objectif pipeline < 2.5s: ATTEINT")
        else:
            logger.warning(f"‚ö†Ô∏è Objectif pipeline < 2.5s: D√âPASS√â (+{(total_pipeline-2500)/1000:.2f}s)")
        
        # Verdict final
        overall_success = success_rate >= 90 and quality_ok and pipeline_ok
        
        if overall_success:
            logger.info("\nüéä VALIDATION NOUS-HERMES-2-MISTRAL-7B-DPO: R√âUSSIE")
            logger.info("üöÄ Mod√®le APPROUV√â pour SuperWhisper V6")
        else:
            logger.warning("\n‚ö†Ô∏è VALIDATION NOUS-HERMES-2-MISTRAL-7B-DPO: CONDITIONNELLE")
            logger.info("üîß Optimisations recommand√©es")
        
        # Sauvegarde rapport
        self.save_validation_report({
            "model_name": self.model_name,
            "success_rate": success_rate,
            "avg_latency": avg_latency,
            "avg_quality": avg_quality,
            "total_pipeline": total_pipeline,
            "approved": overall_success,
            "results": successful_results
        })
        
        return overall_success
    
    def save_validation_report(self, report_data: Dict):
        """Sauvegarder le rapport de validation"""
        import json
        
        report_path = "docs/VALIDATION_NOUS_HERMES_REPORT.md"
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("# ü¶ô RAPPORT VALIDATION NOUS-HERMES-2-MISTRAL-7B-DPO\n\n")
                f.write("## üìä R√âSULTATS VALIDATION\n\n")
                f.write(f"- **Mod√®le**: {report_data['model_name']}\n")
                f.write(f"- **Taux de succ√®s**: {report_data['success_rate']:.1f}%\n")
                f.write(f"- **Latence moyenne**: {report_data['avg_latency']:.1f}ms\n")
                f.write(f"- **Qualit√© moyenne**: {report_data['avg_quality']:.1f}/10\n")
                f.write(f"- **Pipeline total**: {report_data['total_pipeline']:.1f}ms\n")
                f.write(f"- **Statut**: {'‚úÖ APPROUV√â' if report_data['approved'] else '‚ö†Ô∏è CONDITIONNEL'}\n\n")
                
                f.write("## üîç D√âTAILS TESTS\n\n")
                for result in report_data['results'][:5]:  # Top 5 r√©sultats
                    f.write(f"### {result['category'].title()} ({result['config']})\n")
                    f.write(f"- **Latence**: {result['latency']:.1f}ms\n")
                    f.write(f"- **Qualit√©**: {result['quality_score']:.1f}/10\n")
                    f.write(f"- **R√©ponse**: \"{result['response'][:100]}...\"\n\n")
            
            logger.info(f"üìÑ Rapport sauvegard√©: {report_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde rapport: {e}")

async def main():
    """Fonction principale de validation"""
    try:
        validator = NousHermesValidator()
        success = await validator.run_comprehensive_validation()
        
        if success:
            print("\nüéä VALIDATION TERMIN√âE AVEC SUCC√àS")
            print("üöÄ Nous-Hermes-2-Mistral-7B-DPO approuv√© pour SuperWhisper V6")
            return 0
        else:
            print("\n‚ö†Ô∏è VALIDATION TERMIN√âE AVEC R√âSERVES")
            print("üîß Optimisations recommand√©es avant production")
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Erreur validation: {e}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 