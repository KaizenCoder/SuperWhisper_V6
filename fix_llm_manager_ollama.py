#!/usr/bin/env python3
"""
üîß CORRECTION AUTOMATIQUE LLM MANAGER
Corrige automatiquement l'API Ollama dans EnhancedLLMManager
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib
import shutil
from datetime import datetime

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    os.chdir(project_root)
    print(f"üìÅ Project Root: {project_root}")
    return project_root

_PROJECT_ROOT = _setup_portable_environment()

def create_backup(file_path):
    """Cr√©e une sauvegarde du fichier"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = f"{file_path}.backup_{timestamp}"
    shutil.copy2(file_path, backup_path)
    print(f"üíæ Sauvegarde cr√©√©e: {backup_path}")
    return backup_path

def fix_llm_manager():
    """Corrige le LLM Manager pour utiliser la bonne API Ollama"""
    
    llm_manager_path = "LLM/llm_manager_enhanced.py"
    
    if not os.path.exists(llm_manager_path):
        print(f"‚ùå Fichier non trouv√©: {llm_manager_path}")
        return False
    
    # Cr√©er sauvegarde
    backup_path = create_backup(llm_manager_path)
    
    try:
        # Lire le fichier actuel
        with open(llm_manager_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Nouvelle m√©thode _generate_ollama corrig√©e
        new_generate_ollama = '''    async def _generate_ollama(self, user_input: str, max_tokens: int, temperature: float) -> str:
        """G√©n√©ration via Ollama API - VERSION CORRIG√âE"""
        try:
            import httpx
            
            # ‚úÖ CORRECTION: Utiliser l'API native Ollama avec le bon format
            actual_model = getattr(self, 'actual_model_name', self.config.get('model', 'nous-hermes'))
            
            # Format correct pour l'API native Ollama
            data = {
                "model": actual_model,
                "prompt": f"{self.system_prompt}\\n\\nUser: {user_input}\\nAssistant:",
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1,
                    "stop": ["User:", "\\n\\n"]
                }
            }
            
            self.logger.info(f"üß† Requ√™te Ollama: model={actual_model}, tokens={max_tokens}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    'http://127.0.0.1:11434/api/generate',
                    json=data
                )
                
                if response.status_code == 200:
                    result = response.json()
                    response_text = result.get('response', '').strip()
                    
                    if response_text:
                        self.logger.info(f"‚úÖ Ollama r√©ponse: {response_text[:50]}...")
                        return response_text
                    else:
                        self.logger.warning("‚ö†Ô∏è Ollama r√©ponse vide")
                        return None
                else:
                    self.logger.error(f"‚ùå Ollama API error: {response.status_code} - {response.text}")
                    return None
                    
        except Exception as e:
            self.logger.error(f"‚ùå Erreur Ollama: {e}")
            return None'''
        
        # Trouver et remplacer la m√©thode _generate_ollama
        start_marker = "    async def _generate_ollama(self, user_input: str, max_tokens: int, temperature: float) -> str:"
        end_marker = "            return None"
        
        start_index = content.find(start_marker)
        if start_index == -1:
            print("‚ùå M√©thode _generate_ollama non trouv√©e")
            return False
        
        # Trouver la fin de la m√©thode (prochaine m√©thode ou fin de classe)
        lines = content[start_index:].split('\n')
        method_lines = [lines[0]]  # Premi√®re ligne (signature)
        
        for i, line in enumerate(lines[1:], 1):
            # Si on trouve une nouvelle m√©thode au m√™me niveau d'indentation, on s'arr√™te
            if line.strip() and not line.startswith('    ') and not line.startswith('\t'):
                break
            if line.strip().startswith('def ') and line.startswith('    '):
                break
            method_lines.append(line)
            # Si on trouve la fin typique de la m√©thode
            if "return None" in line and line.strip().endswith("return None"):
                break
        
        old_method = '\n'.join(method_lines)
        end_index = start_index + len(old_method)
        
        # Remplacer la m√©thode
        new_content = content[:start_index] + new_generate_ollama + content[end_index:]
        
        # √âcrire le fichier corrig√©
        with open(llm_manager_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print("‚úÖ LLM Manager corrig√© avec succ√®s!")
        print("üîß Corrections appliqu√©es:")
        print("  - API native Ollama (/api/generate)")
        print("  - Format de donn√©es correct")
        print("  - Gestion d'erreurs am√©lior√©e")
        print("  - Logs d√©taill√©s")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la correction: {e}")
        # Restaurer la sauvegarde en cas d'erreur
        if os.path.exists(backup_path):
            shutil.copy2(backup_path, llm_manager_path)
            print(f"üîÑ Fichier restaur√© depuis la sauvegarde")
        return False

def verify_fix():
    """V√©rifie que la correction a √©t√© appliqu√©e"""
    llm_manager_path = "LLM/llm_manager_enhanced.py"
    
    try:
        with open(llm_manager_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # V√©rifier que la correction est pr√©sente
        checks = [
            "VERSION CORRIG√âE" in content,
            "/api/generate" in content,
            "num_predict" in content,
            "repeat_penalty" in content
        ]
        
        if all(checks):
            print("‚úÖ V√©rification: Correction appliqu√©e avec succ√®s")
            return True
        else:
            print("‚ùå V√©rification: Correction incompl√®te")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur v√©rification: {e}")
        return False

def create_test_script():
    """Cr√©e un script de test pour v√©rifier Ollama"""
    test_script = '''#!/usr/bin/env python3
"""
Test rapide Ollama apr√®s correction
"""

import asyncio
import sys
import os
import pathlib

# Setup portable
current_file = pathlib.Path(__file__).resolve()
project_root = current_file.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
os.chdir(project_root)

# Configuration GPU RTX 3090
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

async def test_corrected_llm():
    """Test du LLM Manager corrig√©"""
    try:
        from LLM.llm_manager_enhanced import EnhancedLLMManager
        
        config = {
            'model': 'nous-hermes-2-mistral-7b-dpo:latest',
            'use_ollama': True,
            'timeout': 30.0
        }
        
        print("üß™ Test LLM Manager corrig√©...")
        llm_manager = EnhancedLLMManager(config)
        await llm_manager.initialize()
        
        # Test simple
        response = await llm_manager.generate_response("Quelle est la capitale de la France ?")
        
        if response and response.strip():
            print(f"‚úÖ Test r√©ussi: {response}")
            return True
        else:
            print("‚ùå Test √©chou√©: Pas de r√©ponse")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur test: {e}")
        return False

if __name__ == "__main__":
    asyncio.run(test_corrected_llm())
'''
    
    with open("test_ollama_corrected.py", "w", encoding="utf-8") as f:
        f.write(test_script)
    
    print("üìù Script de test cr√©√©: test_ollama_corrected.py")

def main():
    """Fonction principale"""
    print("üîß CORRECTION AUTOMATIQUE LLM MANAGER")
    print("=" * 50)
    
    print("\nüìã √âtapes de correction:")
    print("1. Sauvegarde du fichier original")
    print("2. Application de la correction API Ollama")
    print("3. V√©rification de la correction")
    print("4. Cr√©ation d'un script de test")
    
    # √âtape 1 & 2: Correction
    if fix_llm_manager():
        print("\n‚úÖ Correction appliqu√©e")
        
        # √âtape 3: V√©rification
        if verify_fix():
            print("‚úÖ V√©rification r√©ussie")
            
            # √âtape 4: Script de test
            create_test_script()
            
            print("\nüéØ PROCHAINES √âTAPES:")
            print("1. Ex√©cuter: python diagnostic_ollama_fix.py")
            print("2. Si Ollama fonctionne, tester: python test_ollama_corrected.py")
            print("3. Puis tester le pipeline complet: python test_pipeline_microphone_reel.py")
            
        else:
            print("‚ùå V√©rification √©chou√©e")
    else:
        print("‚ùå Correction √©chou√©e")

if __name__ == "__main__":
    main() 