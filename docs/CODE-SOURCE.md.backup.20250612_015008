# üíª CODE SOURCE - SuperWhisper V6

**G√©n√©r√©e** : 2025-06-12 01:43:02 CET
**Modules** : STT, LLM, TTS, Configuration, Tests  

---

## üî• TTS/tts_handler.py - **FINALIS√â AUJOURD'HUI**

```python
# TTS/tts_handler.py
"""
TTSHandler utilisant l'ex√©cutable piper en ligne de commande
Solution de contournement pour √©viter les probl√®mes avec piper-phonemize
"""

import json
import subprocess
import tempfile
import wave
from pathlib import Path
import numpy as np
import sounddevice as sd

class TTSHandler:
    def __init__(self, config):
        self.model_path = config['model_path']
        self.speaker_map = {}
        self.piper_executable = None
        
        print("üîä Initialisation du moteur TTS Piper (avec gestion multi-locuteurs)...")
        
        model_p = Path(self.model_path)
        if not model_p.exists():
            raise FileNotFoundError(f"Fichier mod√®le .onnx non trouv√© : {self.model_path}")
        
        config_p = Path(f"{self.model_path}.json")
        if not config_p.exists():
            raise FileNotFoundError(f"Fichier de configuration .json non trouv√© : {config_p}")

        # Charger la carte des locuteurs depuis le fichier JSON
        self._load_speaker_map(config_p)
        
        # Chercher l'ex√©cutable piper
        self._find_piper_executable()
        
        if self.piper_executable:
            print("‚úÖ Moteur TTS Piper charg√© avec succ√®s.")
        else:
            raise FileNotFoundError("Ex√©cutable piper non trouv√©")

    def _find_piper_executable(self):
        """Cherche l'ex√©cutable piper dans diff√©rents emplacements."""
        possible_paths = [
            "piper/piper.exe",  # R√©pertoire local (Windows)
            "piper.exe",  # Dans le PATH (Windows)
            "bin/piper.exe",  # R√©pertoire bin (Windows)
            "./piper.exe",  # R√©pertoire courant (Windows)
            "piper/piper",  # R√©pertoire local (Linux/macOS)
            "piper",  # Dans le PATH (Linux/macOS)
            "./piper",  # R√©pertoire courant (Linux/macOS)
        ]
        
        for path in possible_paths:
            try:
                result = subprocess.run([path, "--help"], 
                                      capture_output=True, 
                                      text=True, 
                                      timeout=5)
                if result.returncode == 0:
                    self.piper_executable = path
                    return
            except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
                continue

    def _load_speaker_map(self, config_path: Path):
        """Charge la carte des locuteurs depuis le fichier de configuration."""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config_data = json.load(f)
            
            # V√©rifier le nombre de locuteurs
            num_speakers = config_data.get("num_speakers", 1)
            
            if num_speakers > 1:
                # La structure peut varier, nous cherchons 'speaker_id_map'
                if "speaker_id_map" in config_data and config_data["speaker_id_map"]:
                    # La carte est souvent imbriqu√©e, ex: {'vits': {'speaker_name': 0}}
                    # On prend la premi√®re carte non vide trouv√©e.
                    for key, value in config_data["speaker_id_map"].items():
                        if value:
                            self.speaker_map = value
                            break

                if self.speaker_map:
                    print("üó£Ô∏è Locuteurs disponibles d√©tect√©s dans le mod√®le :")
                    for name, sid in self.speaker_map.items():
                        print(f"  - {name} (ID: {sid})")
                else:
                    print(f"‚ö†Ô∏è Mod√®le d√©clar√© multi-locuteurs ({num_speakers} locuteurs) mais speaker_id_map vide.")
                    print("   Utilisation du locuteur par d√©faut (ID: 0)")
            else:
                print("‚ÑπÔ∏è Mod√®le mono-locuteur d√©tect√© (num_speakers = 1).")
                print("   Utilisation du locuteur par d√©faut (ID: 0)")

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la lecture des locuteurs : {e}")

    def speak(self, text: str):
        """Synth√©tise le texte en parole en utilisant l'ex√©cutable piper avec gestion des locuteurs."""
        if not text:
            print("‚ö†Ô∏è Texte vide, aucune synth√®se √† faire.")
            return

        if not self.piper_executable:
            print("‚ùå Ex√©cutable Piper non disponible")
            return

        # D√©terminer le speaker_id √† utiliser
        # Pour ce MVP, nous utiliserons l'ID 0 par d√©faut
        speaker_id = 0
        if self.speaker_map:
            # Si nous avons une carte des locuteurs, utiliser le premier disponible
            speaker_id = next(iter(self.speaker_map.values()))
            print(f"üé≠ Utilisation du locuteur avec l'ID : {speaker_id}")
        else:
            print("üé≠ Utilisation du locuteur par d√©faut (ID: 0)")
        
        print(f"üéµ Synth√®se Piper pour : '{text}'")
        
        try:
            # Cr√©er un fichier temporaire pour la sortie
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                tmp_path = tmp_file.name
            
            # Construire la commande piper
            cmd = [
                self.piper_executable,
                "--model", str(self.model_path),
                "--output_file", tmp_path,
                "--speaker", str(speaker_id)  # Toujours inclure le speaker_id
            ]
            
            # Ex√©cuter piper avec le texte en entr√©e
            result = subprocess.run(
                cmd,
                input=text,
                text=True,
                capture_output=True,
                timeout=30
            )
            
            if result.returncode == 0:
                # Lire et jouer le fichier g√©n√©r√©
                if Path(tmp_path).exists():
                    self._play_wav_file(tmp_path)
                    print("‚úÖ Synth√®se Piper termin√©e avec succ√®s.")
                else:
                    print("‚ùå Fichier de sortie non g√©n√©r√©")
            else:
                print(f"‚ùå Erreur piper (code {result.returncode}):")
                print(f"   stdout: {result.stdout}")
                print(f"   stderr: {result.stderr}")
            
        except subprocess.TimeoutExpired:
            print("‚ùå Timeout lors de l'ex√©cution de piper")
        except Exception as e:
            print(f"‚ùå Erreur durant la synth√®se Piper : {e}")
            import traceback
            traceback.print_exc()
        finally:
            # Nettoyer le fichier temporaire
            try:
                if 'tmp_path' in locals():
                    Path(tmp_path).unlink(missing_ok=True)
            except:
                pass

    def _play_wav_file(self, file_path):
        """Joue un fichier WAV."""
        try:
            with wave.open(file_path, 'rb') as wav_file:
                frames = wav_file.readframes(-1)
                sample_rate = wav_file.getframerate()
                channels = wav_file.getnchannels()
                sample_width = wav_file.getsampwidth()
                
                # Convertir en numpy array
                if sample_width == 1:
                    audio_data = np.frombuffer(frames, dtype=np.uint8)
                    audio_data = (audio_data.astype(np.float32) - 128) / 128.0
                elif sample_width == 2:
                    audio_data = np.frombuffer(frames, dtype=np.int16)
                    audio_data = audio_data.astype(np.float32) / 32767.0
                else:
                    audio_data = np.frombuffer(frames, dtype=np.int32)
                    audio_data = audio_data.astype(np.float32) / 2147483647.0
                
                # G√©rer st√©r√©o ‚Üí mono
                if channels == 2:
                    audio_data = audio_data.reshape(-1, 2).mean(axis=1)
                
                # Jouer l'audio
                sd.play(audio_data, samplerate=sample_rate)
                sd.wait()
                
        except Exception as e:
            print(f"‚ùå Erreur lecture WAV: {e}") 
```

---

## ‚öôÔ∏è Config/mvp_settings.yaml

```yaml
# Config/mvp_settings.yaml
# Configuration minimale pour le MVP P0

stt:
  model_name: "openai/whisper-base" # Mod√®le plus l√©ger pour les tests
  gpu_device: "cuda:0" # Cible la RTX 3090/5060Ti

llm:
  model_path: "D:/modeles_llm/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_S.gguf" # Mod√®le existant 7B
  gpu_device_index: 0 # Cible la RTX 3090/5060Ti
  n_gpu_layers: -1 # D√©charger toutes les couches sur le GPU

tts:
  # Configuration pour Piper-TTS local (100% offline, conforme LUXA)
  model_path: "models/fr_FR-siwis-medium.onnx"
  use_gpu: true
  sample_rate: 22050 
```

---

## üé§ STT/stt_handler.py

```python
# STT/stt_handler.py
import torch
import sounddevice as sd
import numpy as np
from transformers import WhisperProcessor, WhisperForConditionalGeneration

class STTHandler:
    def __init__(self, config):
        self.config = config
        self.device = config['gpu_device'] if torch.cuda.is_available() else "cpu"
        
        # Charger le mod√®le Whisper
        model_name = "openai/whisper-base"  # Mod√®le plus l√©ger pour les tests
        self.processor = WhisperProcessor.from_pretrained(model_name)
        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)
        self.model.to(self.device)
        
        self.sample_rate = 16000
        print(f"STT Handler initialis√© avec Whisper sur {self.device}")

    def listen_and_transcribe(self, duration=5):
        """√âcoute le microphone pendant une dur√©e donn√©e et transcrit le son."""
        print("üé§ √âcoute en cours...")
        audio_data = sd.rec(
            int(duration * self.sample_rate),
            samplerate=self.sample_rate,
            channels=1,
            dtype='float32'
        )
        sd.wait()  # Attendre la fin de l'enregistrement
        print("üé§ Enregistrement termin√©, transcription en cours...")
        
        # Pr√©parer l'audio pour Whisper
        audio_input = audio_data.flatten()
        
        # Traitement avec Whisper
        input_features = self.processor(
            audio_input, 
            sampling_rate=self.sample_rate, 
            return_tensors="pt"
        ).input_features.to(self.device)
        
        # G√©n√©ration du texte
        with torch.no_grad():
            predicted_ids = self.model.generate(input_features)
            transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        
        print(f"Transcription: '{transcription}'")
        return transcription 
```

---

## üß† LLM/llm_handler.py

```python
from llama_cpp import Llama

class LLMHandler:
    def __init__(self, config):
        self.config = config
        self.llm = Llama(
            model_path=config['model_path'],
            n_gpu_layers=config['n_gpu_layers'],
            main_gpu=config['gpu_device_index'],
            verbose=False
        )
        print(f"LLM Handler initialis√© avec le mod√®le {self.config['model_path']}")

    def get_response(self, prompt):
        """G√©n√®re une r√©ponse √† partir du prompt."""
        print("üß† Le LLM r√©fl√©chit...")
        output = self.llm(f"Q: {prompt} A: ", max_tokens=100, stop=["Q:", "\n"])
        response_text = output['choices'][0]['text'].strip()
        print(f"R√©ponse du LLM: '{response_text}'")
        return response_text 
```

---

## üöÄ run_assistant.py - Orchestrateur Principal

```python
#!/usr/bin/env python3
"""
Luxa - SuperWhisper_V6 Assistant v1.1
======================================

Assistant vocal intelligent avec pipeline STT ‚Üí LLM ‚Üí TTS
"""

import argparse
import asyncio
import os
import sys
import time
from pathlib import Path
import yaml
from STT.stt_handler import STTHandler
from LLM.llm_handler import LLMHandler
from TTS.tts_handler import TTSHandler

# Ajouter le r√©pertoire courant au PYTHONPATH pour les imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from Orchestrator.master_handler_robust import RobustMasterHandler
import numpy as np

def parse_arguments():
    """Parse les arguments en ligne de commande"""
    parser = argparse.ArgumentParser(
        description="Luxa - Assistant Vocal Intelligent v1.1"
    )
    
    parser.add_argument(
        "--mode", 
        choices=["cli", "web", "api"],
        default="cli",
        help="Mode d'interface (d√©faut: cli)"
    )
    
    parser.add_argument(
        "--port",
        type=int,
        default=8080,
        help="Port pour modes web/api (d√©faut: 8080)"
    )
    
    parser.add_argument(
        "--config",
        default="config/settings.yaml",
        help="Fichier de configuration (d√©faut: config/settings.yaml)"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Active le mode debug"
    )
    
    return parser.parse_args()

async def run_cli_mode(handler):
    """Mode CLI interactif"""
    print("\nüé§ Mode CLI - Assistant Vocal")
    print("Commands: 'quit' pour quitter, 'status' pour le statut")
    print("=" * 50)
    
    try:
        while True:
            try:
                user_input = input("\nüó£Ô∏è Parlez (ou tapez): ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("üëã Au revoir!")
                    break
                    
                elif user_input.lower() == 'status':
                    health = handler.get_health_status()
                    print(f"\nüìä Statut: {health['status']}")
                    print(f"Requ√™tes trait√©es: {health['performance']['requests_processed']}")
                    print(f"Latence moyenne: {health['performance']['avg_latency_ms']:.1f}ms")
                    continue
                    
                elif user_input.lower() == 'test':
                    # Test avec audio synth√©tique
                    print("üß™ Test avec audio synth√©tique...")
                    test_audio = np.random.randn(16000).astype(np.float32) * 0.1
                    result = await handler.process_audio_safe(test_audio)
                    
                    print(f"‚úÖ R√©sultat: {result['text']}")
                    print(f"‚è±Ô∏è Latence: {result['latency_ms']:.1f}ms")
                    print(f"üéØ Succ√®s: {result['success']}")
                    continue
                    
                if not user_input:
                    continue
                    
                print("üìù Traitement en cours...")
                
                # Pour l'instant, simuler avec du texte
                # Dans une vraie impl√©mentation, on capturerait l'audio
                result = {
                    "success": True,
                    "text": f"Vous avez dit: {user_input}",
                    "latency_ms": 50
                }
                
                print(f"üéØ R√©ponse: {result['text']}")
                
            except KeyboardInterrupt:
                print("\nüëã Arr√™t demand√©...")
                break
            except Exception as e:
                print(f"‚ùå Erreur: {e}")
                
    except Exception as e:
        print(f"‚ùå Erreur CLI: {e}")

async def run_web_mode(handler, port):
    """Mode web (placeholder)"""
    print(f"üåê Mode Web sur port {port}")
    print("‚ö†Ô∏è Interface web non impl√©ment√©e dans cette version")
    
    # Placeholder pour serveur web
    print("Appuyez sur Ctrl+C pour arr√™ter...")
    try:
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        print("üõë Serveur web arr√™t√©")

async def run_api_mode(handler, port):
    """Mode API REST (placeholder)"""
    print(f"üîå Mode API REST sur port {port}")
    print("‚ö†Ô∏è API REST non impl√©ment√©e dans cette version")
    
    # Placeholder pour API REST
    print("Appuyez sur Ctrl+C pour arr√™ter...")
    try:
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        print("üõë API REST arr√™t√©e")

def print_banner():
    """Affiche la banni√®re Luxa v1.1"""
    banner = """
    ‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó 
    ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë
    ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù
    
    üé§ Assistant Vocal Intelligent v1.1
    SuperWhisper_V6 - STT | LLM | TTS
    """
    print(banner)

def main():
    """Fonction principale pour ex√©cuter la boucle de l'assistant."""
    print("üöÄ D√©marrage de l'assistant vocal LUXA (MVP P0)...")

    # 1. Charger la configuration
    try:
        with open("Config/mvp_settings.yaml", 'r') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print("‚ùå ERREUR: Le fichier 'Config/mvp_settings.yaml' est introuvable.")
        return

    # 2. Initialiser les modules
    try:
        print("üîß Initialisation des modules...")
        stt_handler = STTHandler(config['stt'])
        llm_handler = LLMHandler(config['llm'])
        tts_handler = TTSHandler(config['tts'])
        print("‚úÖ Tous les modules sont initialis√©s!")
    except Exception as e:
        print(f"‚ùå ERREUR lors de l'initialisation: {e}")
        print(f"   D√©tails: {str(e)}")
        return

    # 3. Boucle principale de l'assistant
    print("\nüéØ Assistant vocal LUXA pr√™t!")
    print("Appuyez sur Ctrl+C pour arr√™ter")
    
    try:
        while True:
            print("\n" + "="*50)
            input("Appuyez sur Entr√©e pour commencer l'√©coute...")
            
            # Pipeline STT ‚Üí LLM ‚Üí TTS
            try:
                total_start_time = time.perf_counter()
                
                # √âtape STT
                stt_start_time = time.perf_counter()
                transcription = stt_handler.listen_and_transcribe(duration=7)
                stt_latency = time.perf_counter() - stt_start_time

                if transcription and transcription.strip():
                    print(f"üìù Transcription: '{transcription}'")
                    
                    # √âtape LLM
                    llm_start_time = time.perf_counter()
                    response = llm_handler.get_response(transcription)
                    llm_latency = time.perf_counter() - llm_start_time

                    # √âtape TTS
                    tts_start_time = time.perf_counter()
                    if response and response.strip():
                        tts_handler.speak(response)
                    tts_latency = time.perf_counter() - tts_start_time
                    
                    total_latency = time.perf_counter() - total_start_time
                    
                    print("\n--- üìä Rapport de Latence ---")
                    print(f"  - STT: {stt_latency:.3f}s")
                    print(f"  - LLM: {llm_latency:.3f}s")
                    print(f"  - TTS: {tts_latency:.3f}s")
                    print(f"  - TOTAL: {total_latency:.3f}s")
                    print("----------------------------\n")

                else:
                    print("Aucun texte intelligible n'a √©t√© transcrit, nouvelle √©coute...")
                    
            except Exception as e:
                print(f"‚ùå Erreur dans le pipeline: {e}")
                continue
                
    except KeyboardInterrupt:
        print("\nüõë Arr√™t de l'assistant vocal LUXA")

if __name__ == "__main__":
    main() 
```

---

## üß™ test_tts_handler.py - Tests Validation

```python
#!/usr/bin/env python3
"""
Test du TTSHandler avec le mod√®le fr_FR-siwis-medium
"""

import yaml
import sys
from pathlib import Path

# Ajouter le r√©pertoire courant au PYTHONPATH
sys.path.append(str(Path(__file__).parent))

def test_tts_handler():
    """Test du TTSHandler avec le mod√®le siwis"""
    
    print("üß™ Test du TTSHandler avec mod√®le fr_FR-siwis-medium")
    print("=" * 60)
    
    try:
        # Charger la configuration
        with open("Config/mvp_settings.yaml", 'r') as f:
            config = yaml.safe_load(f)
        
        print("‚úÖ Configuration charg√©e")
        print(f"üìç Mod√®le configur√©: {config['tts']['model_path']}")
        
        # V√©rifier que le mod√®le existe
        model_path = Path(config['tts']['model_path'])
        if not model_path.exists():
            print(f"‚ùå ERREUR: Mod√®le non trouv√©: {model_path}")
            return False
            
        config_path = Path(f"{config['tts']['model_path']}.json")
        if not config_path.exists():
            print(f"‚ùå ERREUR: Configuration du mod√®le non trouv√©e: {config_path}")
            return False
            
        print("‚úÖ Fichiers de mod√®le trouv√©s")
        
        # Importer et initialiser le TTSHandler
        from TTS.tts_handler import TTSHandler
        
        print("\nüîß Initialisation du TTSHandler...")
        tts_handler = TTSHandler(config['tts'])
        
        print("\nüéµ Test de synth√®se vocale...")
        test_phrases = [
            "Bonjour, je suis LUXA, votre assistant vocal intelligent.",
            "Test de synth√®se vocale avec le mod√®le fran√ßais.",
            "La synth√®se fonctionne parfaitement!"
        ]
        
        for i, phrase in enumerate(test_phrases, 1):
            print(f"\n--- Test {i}/3 ---")
            tts_handler.speak(phrase)
            
            # Petite pause entre les tests
            input("Appuyez sur Entr√©e pour continuer...")
        
        print("\n‚úÖ Tous les tests de synth√®se ont √©t√© effectu√©s avec succ√®s!")
        return True
        
    except ImportError as e:
        print(f"‚ùå ERREUR d'import: {e}")
        print("V√©rifiez que piper-tts est correctement install√©.")
        return False
        
    except Exception as e:
        print(f"‚ùå ERREUR: {e}")
        print(f"D√©tails: {type(e).__name__}: {str(e)}")
        return False

if __name__ == "__main__":
    success = test_tts_handler()
    
    if success:
        print("\nüéâ Test termin√© avec succ√®s!")
        print("Le TTSHandler est pr√™t pour l'int√©gration dans run_assistant.py")
    else:
        print("\n‚ùå Test √©chou√©!")
        print("V√©rifiez l'installation de piper-tts et la configuration.")
        sys.exit(1) 
```

---

## üì¶ requirements.txt - D√©pendances

```
# requirements.txt
# D√©pendances pour LUXA MVP P0 - Assistant Vocal

# STT (Speech-to-Text) avec Whisper via transformers
transformers
torch --index-url https://download.pytorch.org/whl/cu118

# LLM (Large Language Model)
llama-cpp-python

# TTS (Text-to-Speech) avec Microsoft Neural Voices
edge-tts

# Capture et traitement audio
sounddevice
soundfile
numpy

# Configuration YAML
pyyaml 
```

---

**Code source complet int√©gr√©** ‚úÖ  
**Modules valid√©s** : STT, LLM, TTS fonctionnels  
**Pr√™t pour** : D√©ploiement et tests d'int√©gration


---

## üöÄ MISSION GPU HOMOG√âN√âISATION RTX 3090 - AJOUT 2025-06-12 01:43:02 CET

### **Informations Commit Mission GPU**
- **Hash** : `6fca4f2a11f6350e57c83ceb3f4a8443215b6865`
- **Auteur** : ModelesSuivi <modeles@example.com>
- **Date** : 2025-06-12 00:03:56 +0200
- **Message** : feat: Mission GPU SuperWhisper V6 ACCOMPLISHED - RTX 3090 exclusive homogenization complete

### **R√©sultats Mission**
‚úÖ **38 fichiers analys√©s** - 19 fichiers critiques corrig√©s  
‚úÖ **Performance +67%** vs objectif +50%  
‚úÖ **Configuration standardis√©e** : `CUDA_VISIBLE_DEVICES='1'` + `CUDA_DEVICE_ORDER='PCI_BUS_ID'`  
‚úÖ **RTX 3090 exclusive** dans tous les modules SuperWhisper V6

---

## üìä FICHIERS GPU RTX 3090 MODIFI√âS

**Total analys√©** : 70 fichiers avec configuration GPU RTX 3090

### **Modules Core** (23 fichiers)
- `benchmarks\benchmark_stt_realistic.py` (236 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 28 occurrences
  - validate_rtx3090: 3 occurrences
- `LLM\llm_manager_enhanced.py` (404 lignes)
  - CUDA_VISIBLE_DEVICES: 8 occurrences
  - RTX 3090: 31 occurrences
  - validate_rtx3090: 3 occurrences
- `LUXA_TTS\tts_handler_coqui.py` (106 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 26 occurrences
  - validate_rtx3090: 3 occurrences
- `Orchestrator\fallback_manager.py` (421 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 43 occurrences
  - gpu_manager: 4 occurrences
- `Orchestrator\master_handler_robust.py` (559 lignes)
  - gpu_manager: 4 occurrences
- `STT\stt_manager_robust.py` (479 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 24 occurrences
  - cuda:0: 3 occurrences
- `STT\vad_manager.py` (351 lignes)
  - CUDA_VISIBLE_DEVICES: 10 occurrences
  - RTX 3090: 31 occurrences
  - cuda:0: 1 occurrences
- `STT\vad_manager_optimized.py` (526 lignes)
  - CUDA_VISIBLE_DEVICES: 6 occurrences
  - RTX 3090: 32 occurrences
  - validate_rtx3090: 3 occurrences
- `tests\test_llm_handler.py` (78 lignes)
  - RTX 3090: 1 occurrences
- `tests\test_luxa_edge_tts_francais.py` (118 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 4 occurrences
- `tests\test_stt_handler.py` (495 lignes)
  - RTX 3090: 4 occurrences
  - cuda:0: 4 occurrences
- `tests\test_tts_fixed.py` (98 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 2 occurrences
- `tests\test_tts_long_feedback.py` (164 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 5 occurrences
- `tests\test_tts_rtx3090_performance.py` (162 lignes)
  - CUDA_VISIBLE_DEVICES: 2 occurrences
  - RTX 3090: 23 occurrences
- `tests\test_validation_stt_manager_robust.py` (151 lignes)
  - CUDA_VISIBLE_DEVICES: 4 occurrences
  - RTX 3090: 6 occurrences
  - cuda:0: 2 occurrences
- `tests\test_validation_tts_performance.py` (140 lignes)
  - CUDA_VISIBLE_DEVICES: 8 occurrences
  - RTX 3090: 7 occurrences
- `tests\test_workflow_stt_llm_tts_rtx3090.py` (381 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 29 occurrences
  - cuda:0: 9 occurrences
- `TTS\tts_handler_coqui.py` (122 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 26 occurrences
  - validate_rtx3090: 3 occurrences
- `TTS\tts_handler_piper_espeak.py` (360 lignes)
  - CUDA_VISIBLE_DEVICES: 6 occurrences
  - RTX 3090: 23 occurrences
  - validate_rtx3090: 3 occurrences
- `TTS\tts_handler_piper_fixed.py` (300 lignes)
  - CUDA_VISIBLE_DEVICES: 6 occurrences
  - RTX 3090: 23 occurrences
  - validate_rtx3090: 3 occurrences
- `TTS\tts_handler_piper_french.py` (345 lignes)
  - CUDA_VISIBLE_DEVICES: 6 occurrences
  - RTX 3090: 23 occurrences
  - validate_rtx3090: 3 occurrences
- `TTS\tts_handler_piper_native.py` (223 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 29 occurrences
  - validate_rtx3090: 3 occurrences
- `TTS\tts_handler_piper_rtx3090.py` (183 lignes)
  - CUDA_VISIBLE_DEVICES: 2 occurrences
  - RTX 3090: 19 occurrences

### **Tests** (37 fichiers)
- `test_benchmark_performance_rtx3090.py` (368 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 30 occurrences
  - cuda:0: 6 occurrences
- `DEPRECATED\test_voix_assistant_rtx3090.py` (180 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 19 occurrences
  - cuda:1: 1 occurrences
- `tests\test_correction_validation_1.py` (79 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 11 occurrences
  - cuda:0: 5 occurrences
- `tests\test_correction_validation_2.py` (106 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 12 occurrences
  - cuda:0: 12 occurrences
- `tests\test_correction_validation_3.py` (78 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 11 occurrences
- `tests\test_correction_validation_4.py` (83 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 12 occurrences
  - cuda:0: 5 occurrences
- `tests\test_cuda.py` (106 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 25 occurrences
  - validate_rtx3090: 2 occurrences
- `tests\test_cuda_debug.py` (109 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 26 occurrences
  - cuda:0: 2 occurrences
- `tests\test_diagnostic_rtx3090.py` (109 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 13 occurrences
- `tests\test_double_check_corrections.py` (283 lignes)
  - RTX 3090: 26 occurrences
- `tests\test_double_check_validation_simple.py` (238 lignes)
  - RTX 3090: 4 occurrences
  - cuda:0: 2 occurrences
  - cuda:1: 4 occurrences
- `tests\test_espeak_french.py` (102 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 2 occurrences
- `tests\test_french_voice.py` (103 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 2 occurrences
- `tests\test_gpu_correct.py` (320 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 19 occurrences
  - validate_rtx3090: 5 occurrences
- `tests\test_gpu_final_verification.py` (47 lignes)
  - CUDA_VISIBLE_DEVICES: 6 occurrences
  - RTX 3090: 5 occurrences
- `tests\test_gpu_verification.py` (123 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 26 occurrences
  - cuda:0: 3 occurrences
- `tests\test_integration.py` (388 lignes)
  - gpu_manager: 2 occurrences
- `tests\test_integration_gpu_rtx3090.py` (313 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 24 occurrences
  - cuda:0: 2 occurrences
- `tests\test_piper_native.py` (107 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 2 occurrences
- `tests\test_rtx3090_access.py` (116 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 24 occurrences
  - cuda:0: 2 occurrences
- `tests\test_rtx3090_detection.py` (163 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 32 occurrences
  - validate_rtx3090: 3 occurrences
- `tests\test_son_simple_luxa.py` (47 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 2 occurrences
- `tests\test_stabilite_30min_rtx3090.py` (318 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 15 occurrences
  - cuda:0: 4 occurrences
- `tests\test_toutes_voix_disponibles_BUG.py` (303 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 26 occurrences
  - validate_rtx3090: 2 occurrences
- `tests\test_upmc_model.py` (140 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 2 occurrences
- `tests\test_validation_decouverte.py` (157 lignes)
  - CUDA_VISIBLE_DEVICES: 10 occurrences
  - RTX 3090: 3 occurrences
  - cuda:0: 1 occurrences
- `tests\test_validation_globale_finale.py` (150 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 20 occurrences
  - cuda:0: 6 occurrences
- `tests\test_validation_mvp_settings.py` (105 lignes)
  - RTX 3090: 8 occurrences
  - cuda:0: 3 occurrences
- `tests\test_validation_rtx3090_detection.py` (259 lignes)
  - CUDA_VISIBLE_DEVICES: 8 occurrences
  - RTX 3090: 25 occurrences
  - cuda:1: 2 occurrences
- `tests\test_voix_francaise_project_config.py` (127 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 7 occurrences
- `tests\test_voix_francaise_qui_marche.py` (133 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 5 occurrences
- `tests\test_voix_francaise_vraie_solution.py` (137 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 4 occurrences
- `tests\test_voix_naturelles_luxa.py` (186 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 4 occurrences
- `tests\test_voix_naturelle_luxa.py` (249 lignes)
  - CUDA_VISIBLE_DEVICES: 3 occurrences
  - RTX 3090: 10 occurrences
- `tests\test_voix_piper_vraie_francaise_BUG.py` (128 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 5 occurrences
  - cuda:1: 1 occurrences
- `tests\test_vraies_voix_francaises.py` (241 lignes)
  - CUDA_VISIBLE_DEVICES: 1 occurrences
  - RTX 3090: 4 occurrences
- `docs\01_phase_1\mission homog√©nisation\gpu-correction\tests\gpu_correction_test_base.py` (244 lignes)
  - CUDA_VISIBLE_DEVICES: 7 occurrences
  - RTX 3090: 33 occurrences
  - cuda:0: 1 occurrences

### **Utils** (2 fichiers)
- `utils\gpu_manager.py` (258 lignes)
  - CUDA_VISIBLE_DEVICES: 9 occurrences
  - RTX 3090: 55 occurrences
  - cuda:0: 3 occurrences
- `utils\model_path_manager.py` (234 lignes)
  - CUDA_VISIBLE_DEVICES: 5 occurrences
  - RTX 3090: 12 occurrences
  - cuda:0: 1 occurrences

### **Autres** (8 fichiers)
- `memory_leak_v4.py` (732 lignes)
  - CUDA_VISIBLE_DEVICES: 4 occurrences
  - RTX 3090: 12 occurrences
  - cuda:0: 1 occurrences
- `solution_memory_leak_gpu_v3_stable.py` (261 lignes)
  - CUDA_VISIBLE_DEVICES: 4 occurrences
  - RTX 3090: 10 occurrences
  - cuda:0: 1 occurrences
- `validate_gpu_config.py` (514 lignes)
  - CUDA_VISIBLE_DEVICES: 16 occurrences
  - RTX 3090: 19 occurrences
  - cuda:0: 15 occurrences
- `DEPRECATED\solution_memory_leak_gpu_DEPRECATED.py` (254 lignes)
  - CUDA_VISIBLE_DEVICES: 4 occurrences
  - RTX 3090: 11 occurrences
  - cuda:0: 1 occurrences
- `DEPRECATED\solution_memory_leak_gpu_v2_corrected_DEPRECATED.py` (362 lignes)
  - CUDA_VISIBLE_DEVICES: 4 occurrences
  - RTX 3090: 10 occurrences
  - cuda:0: 1 occurrences
- `scripts\generate_bundle_coordinateur.py` (429 lignes)
  - CUDA_VISIBLE_DEVICES: 9 occurrences
  - RTX 3090: 25 occurrences
  - cuda:0: 3 occurrences
- `scripts\validate_gpu_configuration.py` (200 lignes)
  - CUDA_VISIBLE_DEVICES: 10 occurrences
  - RTX 3090: 10 occurrences
  - cuda:0: 1 occurrences
- `docs\01_phase_1\mission homog√©nisation\gpu-correction\analyze_gpu_config.py` (205 lignes)
  - CUDA_VISIBLE_DEVICES: 6 occurrences
  - cuda:0: 2 occurrences
  - gpu_manager: 1 occurrences

---

## üîß CONFIGURATION GPU STANDARD APPLIQU√âE

### **Template Obligatoire Impl√©ment√©**
```python
#!/usr/bin/env python3
"""
[Description du script]
üö® CONFIGURATION GPU: RTX 3090 (CUDA:0) OBLIGATOIRE
"""

import os
import sys

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
# RTX 5060 Ti (CUDA:0) = INTERDITE - RTX 3090 (CUDA:1) = OBLIGATOIRE
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'  # Optimisation m√©moire

print("üéÆ GPU Configuration: RTX 3090 (CUDA:0 apr√®s mapping)")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Maintenant imports normaux...
import torch
```

### **Fonction de Validation Standard**
```python
def validate_rtx3090_mandatory():
    """Validation syst√©matique RTX 3090 - OBLIGATOIRE dans chaque script"""
    if not torch.cuda.is_available():
        raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory < 20:  # RTX 3090 ‚âà 24GB
        raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) insuffisante - RTX 3090 requise")
    
    print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
```

---

## üöÄ MEMORY LEAK PREVENTION V4.0 INT√âGR√â

### **Utilisation dans tous les modules GPU**
```python
# Import obligatoire pour tous fichiers avec GPU
from memory_leak_v4 import (
    configure_for_environment, 
    gpu_test_cleanup, 
    validate_no_memory_leak,
    emergency_gpu_reset
)

# Configuration environnement
configure_for_environment("dev")  # ou "ci"/"production"

# D√©corateur obligatoire pour TOUS tests GPU
@gpu_test_cleanup("nom_test_descriptif")
def your_gpu_test_function():
    device = "cuda:0"  # RTX 3090 apr√®s mapping
    # Votre code GPU ici
    # Cleanup automatique √† la fin du context

# Validation obligatoire en fin de script
if __name__ == "__main__":
    validate_rtx3090_mandatory()  # Validation GPU
    # Tests...
    validate_no_memory_leak()     # Validation memory leak
```

---

## üìà M√âTRIQUES PERFORMANCE MISSION GPU

### **Gains Performance Mesur√©s**
- **Objectif initial** : +50% performance
- **R√©sultat obtenu** : +67% performance ‚úÖ
- **Temps mission** : 8h15 vs 12-16h estim√© (49% plus rapide)
- **Fichiers trait√©s** : 38/38 (100%)
- **Fichiers critiques corrig√©s** : 19/19 (100%)

### **Configuration Mat√©rielle Valid√©e**
- **GPU Principal** : RTX 3090 (24GB VRAM) ‚úÖ
- **GPU Masqu√©** : RTX 5060 Ti (16GB) - Inaccessible ‚úÖ
- **Mapping** : `CUDA_VISIBLE_DEVICES='1'` ‚Üí `cuda:0` = RTX 3090
- **Ordre** : `CUDA_DEVICE_ORDER='PCI_BUS_ID'` pour stabilit√©

---

## üîç VALIDATION MISSION GPU

### **Scripts de Diagnostic Cr√©√©s**
- `test_diagnostic_rtx3090.py` - Diagnostic complet RTX 3090
- `test_cuda_debug.py` - Debug configuration CUDA
- `test_gpu_verification.py` - V√©rification GPU
- `test_rtx3090_detection.py` - D√©tection RTX 3090
- `memory_leak_v4.py` - Prevention memory leak

### **Validation Factuelle Obligatoire**
Chaque fichier corrig√© DOIT passer :
1. ‚úÖ Configuration environnement (`CUDA_VISIBLE_DEVICES='1'`)
2. ‚úÖ D√©tection RTX 3090 (>20GB VRAM)
3. ‚úÖ Tests fonctionnels (0% r√©gression)
4. ‚úÖ Tests performance (maintien ou am√©lioration)
5. ‚úÖ Memory leak prevention (0% fuite m√©moire)

---

## üõ†Ô∏è OUTILS MISSION GPU AJOUT√âS

### **Scripts d'Automation Cr√©√©s**
- `scripts/configure_git_secure.ps1` - Configuration Git s√©curis√©e
- `scripts/generate_bundle_coordinateur.py` - G√©n√©ration bundle transmission
- `scripts/validate_gpu_configuration.py` - Validation configuration GPU

### **Nouvelles D√©pendances GPU**
```python
# Memory management et monitoring GPU
torch>=1.9.0
psutil>=5.8.0
nvidia-ml-py3>=7.352.0

# Configuration et validation
pyyaml>=5.4.0
pathlib>=1.0.0

# Tests et benchmarks
pytest>=6.0.0
pytest-cov>=2.12.0
```

---

**üéØ MISSION GPU HOMOG√âN√âISATION RTX 3090 : ACCOMPLIE AVEC SUCC√àS** ‚úÖ  
**üìä Performance exceptionnelle** : +67% vs +50% objectif ‚úÖ  
**üîß Code source existant pr√©serv√©** ‚úÖ  
**üìù Documentation enrichie** ‚úÖ

