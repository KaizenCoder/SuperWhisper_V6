# SuperWhisper V6 Pipeline Configuration
# Configuration complète STT → LLM → TTS

# Configuration STT (Speech-to-Text)
stt:
  # Backend principal RTX 3090
  primary_backend: "prism"
  
  # Configuration Prism Whisper2 (RTX 3090)
  prism:
    model_path: "D:/Models/Whisper/whisper-large-v3-turbo"
    device: "cuda:1"  # RTX 3090 obligatoire
    compute_type: "float16"
    batch_size: 1
    
  # Configuration VAD (Voice Activity Detection)
  vad:
    threshold: 0.3
    min_speech_duration_ms: 100
    max_speech_duration_s: .inf
    min_silence_duration_ms: 2000
    speech_pad_ms: 400
    
  # Configuration cache
  cache:
    enabled: true
    max_size: 1000
    ttl_seconds: 3600

# Configuration TTS (Text-to-Speech)
tts:
  # Backend principal (validé 14/06/2025)
  primary_backend: "unified"
  
  # Configuration UnifiedTTSManager (modèle validé)
  unified:
    model_path: "D:/TTS_Voices/piper/fr_FR-siwis-medium.onnx"
    config_path: "D:/TTS_Voices/piper/fr_FR-siwis-medium.onnx.json"
    language: "fr"
    device: "cuda:1"  # RTX 3090 obligatoire
    sample_rate: 22050
    format: "wav"
    
  # Configuration cache
  cache:
    enabled: true
    max_size: 500
    ttl_seconds: 1800
    
  # Configuration audio
  audio:
    sample_rate: 22050
    format: "wav"
    
  # Configuration circuit breaker
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    recovery_timeout: 30
    half_open_max_calls: 3

# Configuration LLM (Large Language Model)
llm:
  endpoint: "http://localhost:11434"
  model: "nous-hermes-2-mistral-7b-dpo:latest"
  timeout: 45.0
  
  # Health check
  health_check:
    endpoint: "/api/tags"
    timeout: 10.0
    retry_count: 3
    
  # Paramètres génération
  generation:
    temperature: 0.7
    max_tokens: 50
    top_p: 0.9

# Configuration Pipeline
pipeline:
  # Endpoint LLM
  llm_endpoint: "http://localhost:11434/api/chat"
  llm_profile: "balanced"
  llm_timeout: 45
  
  # Métriques Prometheus
  enable_metrics: false
  metrics_port: 9091
  
  # Configuration queues
  max_queue_size: 16
  worker_timeout: 30.0
  
  # Fallbacks
  fallback_enabled: true
  
  # Audio output
  audio_output:
    sample_rate: 22050
    channels: 1
    buffer_size: 1024

# Configuration GPU RTX 3090
gpu:
  cuda_visible_devices: "1"
  cuda_device_order: "PCI_BUS_ID"
  pytorch_cuda_alloc_conf: "max_split_size_mb:1024"
  
  # Validation RTX 3090
  validation:
    enabled: true
    min_vram_gb: 20
    required_gpu: "RTX 3090"

# Configuration logging
logging:
  level: "INFO"
  format: "%(asctime)s – %(levelname)s – %(name)s – %(message)s"
  
# Configuration monitoring
monitoring:
  enabled: false
  prometheus_port: 9091
  grafana_dashboard: true
  
  # Métriques collectées
  metrics:
    - "pipeline_latency_ms"
    - "pipeline_requests_total"
    - "pipeline_errors_total"
    - "stt_processing_time"
    - "llm_processing_time"
    - "tts_processing_time"

# Objectifs performance
performance:
  target_latency_ms: 1200  # < 1.2s end-to-end
  max_queue_wait_ms: 100
  audio_buffer_ms: 50 