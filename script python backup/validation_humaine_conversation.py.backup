#!/usr/bin/env python3
"""
Validation Humaine SuperWhisper V6 - TÃ¢che 4 CRITIQUE
ğŸš¨ CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

Tests conversation voix-Ã -voix en conditions rÃ©elles
âœ” Conversation fluide sans interruptions
âœ” QualitÃ© audio TTS acceptable
âœ” Latence perÃ§ue < 1.2s
âœ” Pipeline robuste en conditions rÃ©elles
"""

import os
import sys
import asyncio
import time
import signal
from pathlib import Path
from typing import Optional, Dict, Any, List
import logging
from datetime import datetime
import json

# =============================================================================
# ğŸš¨ CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

print("ğŸ® Validation Humaine: RTX 3090 (CUDA:1) forcÃ©e")
print(f"ğŸ”’ CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Ajouter le rÃ©pertoire parent au path pour imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# =============================================================================
# IMPORTS ET CONFIGURATION
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â€“ %(levelname)s â€“ %(name)s â€“ %(message)s",
)
LOGGER = logging.getLogger("ValidationHumaine")

# =============================================================================
# CLASSE VALIDATION HUMAINE
# =============================================================================

class ValidationHumaine:
    """Tests conversation voix-Ã -voix en conditions rÃ©elles"""
    
    def __init__(self):
        self.config_path = "PIPELINE/config/pipeline.yaml"  # Utiliser config par dÃ©faut d'abord
        self.results_path = "PIPELINE/reports/validation_humaine_results.json"
        self.orchestrator = None
        self.conversation_turns = []
        self.test_phrases = [
            "Bonjour, comment Ã§a va aujourd'hui ?",
            "Pouvez-vous me parler de la mÃ©tÃ©o ?", 
            "Quel est votre plat prÃ©fÃ©rÃ© ?",
            "Racontez-moi une histoire courte",
            "Merci pour cette conversation"
        ]
        self.validation_results = {
            "timestamp": datetime.now().isoformat(),
            "tests_performed": [],
            "latency_measurements": [],
            "quality_evaluations": [],
            "success_criteria": {
                "conversation_fluide": False,
                "qualite_audio_acceptable": False,
                "latence_sous_1_2s": False,
                "pipeline_robuste": False
            },
            "overall_success": False
        }
    
    async def run_validation(self):
        """ExÃ©cuter validation humaine complÃ¨te"""
        print("\n" + "="*70)
        print("ğŸ¯ VALIDATION HUMAINE SUPERWHISPER V6 - TÃ‚CHE 4 CRITIQUE")
        print("ğŸš¨ TESTS CONVERSATION VOIX-Ã€-VOIX EN CONDITIONS RÃ‰ELLES")
        print("="*70)
        
        try:
            # Validation prÃ©liminaire
            await self._validate_environment()
            
            # Initialisation pipeline
            await self._initialize_pipeline()
            
            # Tests interactifs
            await self._run_interactive_tests()
            
            # Ã‰valuation finale
            self._evaluate_results()
            
            # Sauvegarde rÃ©sultats
            self._save_results()
            
        except KeyboardInterrupt:
            print("\nğŸ›‘ Validation interrompue par l'utilisateur")
        except Exception as e:
            LOGGER.error(f"âŒ Erreur validation: {e}")
            print(f"âŒ Erreur: {e}")
            self.validation_results["error"] = str(e)
        finally:
            if self.orchestrator:
                await self._cleanup_pipeline()
    
    async def _validate_environment(self):
        """Validation environnement avant tests"""
        print("\nğŸ” VALIDATION ENVIRONNEMENT PRÃ‰LIMINAIRE")
        print("-" * 50)
        
        # Test GPU RTX 3090
        try:
            from PIPELINE.scripts.assert_gpu_env import main as validate_gpu
            validate_gpu()
            print("âœ… GPU RTX 3090 validÃ©e")
        except Exception as e:
            raise RuntimeError(f"âŒ GPU validation failed: {e}")
        
        # Test audio devices
        try:
            print("ğŸ” Validation devices audio...")
            import sounddevice as sd
            devices = sd.query_devices()
            print(f"âœ… {len(devices)} devices audio dÃ©tectÃ©s")
        except Exception as e:
            print(f"âš ï¸ Avertissement audio: {e}")
        
        # VÃ©rification configuration
        config_path = Path(self.config_path)
        if not config_path.exists():
            print(f"âš ï¸ Configuration non trouvÃ©e: {config_path}")
            print(f"ğŸ“‹ Utilisation configuration par dÃ©faut vide")
        else:
            print(f"âœ… Configuration trouvÃ©e: {config_path}")
    
    async def _initialize_pipeline(self):
        """Initialisation pipeline avec code obligatoire"""
        print("\nğŸš€ INITIALISATION PIPELINE - CODE OBLIGATOIRE")
        print("-" * 50)
        
        try:
            import yaml
            
            # Charger configuration
            cfg = {}
            if Path(self.config_path).exists():
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    cfg = yaml.safe_load(f)
                print(f"ğŸ“ Configuration chargÃ©e: {self.config_path}")
            else:
                print("ğŸ“‹ Utilisation configuration par dÃ©faut vide")
            
            # Import components
            from STT.unified_stt_manager_optimized import OptimizedUnifiedSTTManager
            from TTS.tts_manager import UnifiedTTSManager
            from PIPELINE.pipeline_orchestrator import PipelineOrchestrator
            
            # Initialiser composants avec gestion d'erreur
            print("ğŸ¯ Initialisation STT...")
            try:
                stt = OptimizedUnifiedSTTManager(cfg.get("stt", {}))
            except Exception as e:
                print(f"âš ï¸ Erreur STT: {e}")
                print("ğŸ“‹ Utilisation configuration STT minimale")
                stt = OptimizedUnifiedSTTManager({})
            
            print("ğŸ”Š Initialisation TTS...")
            try:
                tts = UnifiedTTSManager(cfg.get("tts", {}))
            except Exception as e:
                print(f"âš ï¸ Erreur TTS: {e}")
                print("ğŸ“‹ Utilisation configuration TTS minimale")
                tts = UnifiedTTSManager({})
            
            print("ğŸ¯ Initialisation Orchestrator...")
            self.orchestrator = PipelineOrchestrator(
                stt,
                tts,
                llm_endpoint=cfg.get("pipeline", {}).get("llm_endpoint", "http://localhost:8000"),
                metrics_enabled=True,  # Activer mÃ©triques pour validation
            )
            
            print("ğŸš€ DÃ©marrage pipeline...")
            await self.orchestrator.start()
            
            print("âœ… Pipeline initialisÃ© avec succÃ¨s")
            
        except Exception as e:
            raise RuntimeError(f"âŒ Ã‰chec initialisation pipeline: {e}")
    
    async def _run_interactive_tests(self):
        """Tests interactifs conversation voix-Ã -voix"""
        print("\nğŸ¤ TESTS CONVERSATION VOIX-Ã€-VOIX INTERACTIFS")
        print("="*60)
        print("ğŸ“‹ Instructions:")
        print("  â€¢ Parlez clairement dans votre microphone")
        print("  â€¢ Ã‰coutez les rÃ©ponses audio")
        print("  â€¢ Ã‰valuez la qualitÃ© de chaque interaction")
        print("  â€¢ Appuyez sur Ctrl+C pour arrÃªter")
        print("\nğŸ¯ Objectifs de validation:")
        print("  â€¢ Latence perÃ§ue < 1.2s")
        print("  â€¢ Conversation fluide sans interruptions")
        print("  â€¢ QualitÃ© audio TTS acceptable")
        print("  â€¢ Pipeline robuste en conditions rÃ©elles")
        
        # Test 1: Conversation libre
        await self._test_conversation_libre()
        
        # Test 2: Phrases prÃ©dÃ©finies (optionnel)
        if self._ask_user_continue("Voulez-vous tester avec des phrases prÃ©dÃ©finies ?"):
            await self._test_phrases_predefinies()
        
        # Test 3: Test stress (optionnel)
        if self._ask_user_continue("Voulez-vous effectuer un test de robustesse ?"):
            await self._test_robustesse()
    
    async def _test_conversation_libre(self):
        """Test conversation libre avec Ã©valuation humaine"""
        print("\nğŸ—£ï¸ TEST 1: CONVERSATION LIBRE")
        print("-" * 40)
        print("ğŸ¤ Commencez Ã  parler naturellement...")
        print("â±ï¸ Observez la latence de chaque rÃ©ponse")
        print("ğŸ”Š Ã‰valuez la qualitÃ© audio des rÃ©ponses")
        
        try:
            # Laisser le pipeline tourner pendant 5 minutes max
            start_time = time.time()
            conversation_duration = 0
            
            while time.time() - start_time < 300:  # 5 minutes max
                await asyncio.sleep(1)
                conversation_duration = time.time() - start_time
                
                # Collecter mÃ©triques pÃ©riodiquement
                if int(conversation_duration) % 10 == 0:  # Toutes les 10s
                    metrics = self.orchestrator.get_metrics()
                    if metrics.total_requests > 0:
                        avg_latency = metrics.total_latency_ms / metrics.total_requests
                        print(f"ğŸ“Š Latence moyenne: {avg_latency:.1f}ms | "
                              f"RequÃªtes: {metrics.total_requests} | "
                              f"SuccÃ¨s: {metrics.successful_requests}")
                        
                        self.validation_results["latency_measurements"].append({
                            "timestamp": datetime.now().isoformat(),
                            "average_latency_ms": avg_latency,
                            "total_requests": metrics.total_requests,
                            "success_rate": metrics.successful_requests / metrics.total_requests if metrics.total_requests > 0 else 0
                        })
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ Test conversation libre terminÃ©")
        
        # Ã‰valuation utilisateur
        self._evaluate_conversation_libre()
    
    def _evaluate_conversation_libre(self):
        """Ã‰valuation humaine de la conversation libre"""
        print("\nğŸ“ Ã‰VALUATION CONVERSATION LIBRE")
        print("-" * 40)
        
        # Latence perÃ§ue
        latence_ok = self._ask_user_yes_no("La latence Ã©tait-elle acceptable (< 1.2s perÃ§ue) ?")
        self.validation_results["success_criteria"]["latence_sous_1_2s"] = latence_ok
        
        # FluiditÃ© conversation
        fluidite_ok = self._ask_user_yes_no("La conversation Ã©tait-elle fluide sans interruptions ?")
        self.validation_results["success_criteria"]["conversation_fluide"] = fluidite_ok
        
        # QualitÃ© audio
        qualite_ok = self._ask_user_yes_no("La qualitÃ© audio des rÃ©ponses Ã©tait-elle acceptable ?")
        self.validation_results["success_criteria"]["qualite_audio_acceptable"] = qualite_ok
        
        # Robustesse
        robustesse_ok = self._ask_user_yes_no("Le pipeline a-t-il fonctionnÃ© de maniÃ¨re robuste ?")
        self.validation_results["success_criteria"]["pipeline_robuste"] = robustesse_ok
        
        # Commentaires
        commentaires = input("ğŸ’¬ Commentaires additionnels (optionnel): ").strip()
        
        self.validation_results["tests_performed"].append({
            "test_name": "conversation_libre",
            "timestamp": datetime.now().isoformat(),
            "latence_acceptable": latence_ok,
            "conversation_fluide": fluidite_ok,
            "qualite_audio": qualite_ok,
            "pipeline_robuste": robustesse_ok,
            "commentaires": commentaires
        })
    
    async def _test_phrases_predefinies(self):
        """Test avec phrases prÃ©dÃ©finies pour consistance"""
        print("\nğŸ“ TEST 2: PHRASES PRÃ‰DÃ‰FINIES")
        print("-" * 40)
        print("ğŸ¯ Lisez chaque phrase clairement et Ã©valuez la rÃ©ponse")
        
        for i, phrase in enumerate(self.test_phrases, 1):
            print(f"\nğŸ“¢ Phrase {i}/{len(self.test_phrases)}: '{phrase}'")
            input("ğŸ¤ Appuyez sur EntrÃ©e quand vous Ãªtes prÃªt Ã  parler...")
            
            start_time = time.time()
            
            # Attendre un peu pour laisser le temps de parler et avoir une rÃ©ponse
            await asyncio.sleep(10)
            
            elapsed = time.time() - start_time
            print(f"â±ï¸ Temps Ã©coulÃ©: {elapsed:.1f}s")
            
            # Ã‰valuation de cette phrase
            reponse_ok = self._ask_user_yes_no(f"La rÃ©ponse Ã  '{phrase}' Ã©tait-elle satisfaisante ?")
            
            self.validation_results["quality_evaluations"].append({
                "phrase": phrase,
                "timestamp": datetime.now().isoformat(),
                "reponse_satisfaisante": reponse_ok,
                "temps_ecoule_s": elapsed
            })
    
    async def _test_robustesse(self):
        """Test robustesse avec conditions difficiles"""
        print("\nğŸ”§ TEST 3: ROBUSTESSE PIPELINE")
        print("-" * 40)
        print("ğŸ¯ Test dans des conditions plus difficiles:")
        print("  â€¢ Parlez plus vite ou plus lentement")
        print("  â€¢ Variez le volume de votre voix")
        print("  â€¢ Testez avec du bruit de fond")
        
        input("ğŸ¤ Appuyez sur EntrÃ©e pour commencer le test de robustesse...")
        
        try:
            # Test pendant 2 minutes
            await asyncio.sleep(120)
        except KeyboardInterrupt:
            print("\nğŸ›‘ Test robustesse terminÃ©")
        
        robustesse_ok = self._ask_user_yes_no("Le pipeline a-t-il bien gÃ©rÃ© les conditions difficiles ?")
        
        self.validation_results["tests_performed"].append({
            "test_name": "robustesse",
            "timestamp": datetime.now().isoformat(),
            "conditions_difficiles_ok": robustesse_ok
        })
    
    def _evaluate_results(self):
        """Ã‰valuation finale des rÃ©sultats"""
        print("\nğŸ“Š Ã‰VALUATION FINALE VALIDATION HUMAINE")
        print("="*50)
        
        criteria = self.validation_results["success_criteria"]
        
        print("ğŸ¯ CritÃ¨res de succÃ¨s:")
        print(f"  âœ… Conversation fluide: {'âœ… OUI' if criteria['conversation_fluide'] else 'âŒ NON'}")
        print(f"  ğŸ”Š QualitÃ© audio TTS: {'âœ… OUI' if criteria['qualite_audio_acceptable'] else 'âŒ NON'}")
        print(f"  â±ï¸ Latence < 1.2s: {'âœ… OUI' if criteria['latence_sous_1_2s'] else 'âŒ NON'}")
        print(f"  ğŸ”§ Pipeline robuste: {'âœ… OUI' if criteria['pipeline_robuste'] else 'âŒ NON'}")
        
        # DÃ©terminer succÃ¨s global
        all_criteria_met = all(criteria.values())
        self.validation_results["overall_success"] = all_criteria_met
        
        print(f"\nğŸŠ RÃ‰SULTAT GLOBAL: {'âœ… SUCCÃˆS' if all_criteria_met else 'âŒ Ã‰CHEC'}")
        
        if all_criteria_met:
            print("ğŸ‰ SuperWhisper V6 est prÃªt pour la production !")
        else:
            print("âš ï¸ Des amÃ©liorations sont nÃ©cessaires avant production")
    
    def _save_results(self):
        """Sauvegarde rÃ©sultats de validation"""
        try:
            # Ajouter mÃ©triques finales
            if self.orchestrator:
                final_metrics = self.orchestrator.get_metrics()
                self.validation_results["final_metrics"] = {
                    "total_requests": final_metrics.total_requests,
                    "successful_requests": final_metrics.successful_requests,
                    "average_latency_ms": final_metrics.total_latency_ms / max(final_metrics.total_requests, 1),
                    "stt_latency_ms": final_metrics.stt_latency_ms,
                    "llm_latency_ms": final_metrics.llm_latency_ms,
                    "tts_latency_ms": final_metrics.tts_latency_ms
                }
            
            # CrÃ©er rÃ©pertoire si nÃ©cessaire
            Path(self.results_path).parent.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarder rÃ©sultats
            with open(self.results_path, 'w', encoding='utf-8') as f:
                json.dump(self.validation_results, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s: {self.results_path}")
            
        except Exception as e:
            LOGGER.error(f"Erreur sauvegarde rÃ©sultats: {e}")
    
    async def _cleanup_pipeline(self):
        """Nettoyage pipeline"""
        try:
            if hasattr(self.orchestrator, 'stop'):
                await self.orchestrator.stop()
            print("ğŸ§¹ Pipeline nettoyÃ©")
        except Exception as e:
            LOGGER.error(f"Erreur nettoyage: {e}")
    
    def _ask_user_yes_no(self, question: str) -> bool:
        """Demander oui/non Ã  l'utilisateur"""
        while True:
            response = input(f"â“ {question} (o/n): ").strip().lower()
            if response in ['o', 'oui', 'y', 'yes']:
                return True
            elif response in ['n', 'non', 'no']:
                return False
            else:
                print("   RÃ©pondez par 'o' (oui) ou 'n' (non)")
    
    def _ask_user_continue(self, question: str) -> bool:
        """Demander si continuer"""
        return self._ask_user_yes_no(question)

# =============================================================================
# MAIN FONCTION
# =============================================================================

async def main():
    """Point d'entrÃ©e principal"""
    print("ğŸš€ DÃ‰MARRAGE VALIDATION HUMAINE SUPERWHISPER V6")
    
    validator = ValidationHumaine()
    await validator.run_validation()
    
    print("\nğŸ‘‹ Validation humaine terminÃ©e")

def signal_handler(signum, frame):
    """Gestionnaire signal pour arrÃªt propre"""
    print(f"\nğŸ›‘ Signal {signum} reÃ§u - ArrÃªt en cours...")
    sys.exit(0)

if __name__ == "__main__":
    # Gestionnaire signaux
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Lancer validation
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ›‘ Validation interrompue")
    except Exception as e:
        print(f"âŒ Erreur fatale: {e}")
        sys.exit(1) 