#!/usr/bin/env python3
"""
üì• IMPORT LLAMA3 8B DANS OLLAMA - SUPERWHISPER V6
===============================================
Script pour importer le mod√®le Llama3 8B Instruct Q6_K dans Ollama

MOD√àLE SOURCE:
- Fichier: Meta-Llama-3-8B-Instruct-Q6_K.gguf
- Localisation: D:\modeles_llm\lmstudio-community\Meta-Llama-3-8B-Instruct-GGUF\
- Taille: ~6.1GB

ACTIONS:
1. Cr√©ation Modelfile Ollama
2. Import mod√®le depuis fichier local
3. Validation mod√®le charg√©
4. Test rapide inf√©rence

Usage: python scripts/import_llama3_8b_ollama.py

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import subprocess
import time
import logging
import tempfile
import httpx
import asyncio

# Configuration logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("llama3_import")

class Llama3OllamaImporter:
    """Importateur Llama3 8B dans Ollama"""
    
    def __init__(self):
        self.model_name = "llama3:8b-instruct-q6_k"
        self.model_path = r"D:\modeles_llm\lmstudio-community\Meta-Llama-3-8B-Instruct-GGUF\Meta-Llama-3-8B-Instruct-Q6_K.gguf"
        self.ollama_base_url = "http://localhost:11434"
        
        # Template Modelfile pour Llama3 8B Instruct
        self.modelfile_template = """FROM {model_path}

# Param√®tres optimis√©s pour conversation fran√ßaise
PARAMETER temperature 0.3
PARAMETER top_p 0.8
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_predict 50

# Template Llama3 Instruct
TEMPLATE \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Tu es un assistant vocal fran√ßais. R√©ponds de mani√®re tr√®s concise et naturelle.<|eot_id|><|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

\"\"\"

# Messages syst√®me
SYSTEM \"Tu es un assistant vocal fran√ßais. R√©ponds de mani√®re tr√®s concise et naturelle.\"

# Param√®tres d'arr√™t
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|end_of_text|>"
"""
    
    def check_model_file_exists(self) -> bool:
        """V√©rification existence fichier mod√®le"""
        if os.path.exists(self.model_path):
            file_size_gb = os.path.getsize(self.model_path) / (1024**3)
            logger.info(f"‚úÖ Fichier mod√®le trouv√©: {file_size_gb:.1f}GB")
            return True
        else:
            logger.error(f"‚ùå Fichier mod√®le non trouv√©: {self.model_path}")
            return False
    
    async def check_ollama_available(self) -> bool:
        """V√©rification Ollama disponible"""
        try:
            async with httpx.AsyncClient(timeout=10) as client:
                response = await client.get(f"{self.ollama_base_url}/api/tags")
                if response.status_code == 200:
                    logger.info("‚úÖ Ollama disponible")
                    return True
                else:
                    logger.error(f"‚ùå Ollama erreur: {response.status_code}")
                    return False
        except Exception as e:
            logger.error(f"‚ùå Ollama non disponible: {e}")
            return False
    
    async def check_model_already_loaded(self) -> bool:
        """V√©rification si mod√®le d√©j√† charg√©"""
        try:
            async with httpx.AsyncClient(timeout=10) as client:
                response = await client.get(f"{self.ollama_base_url}/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    models = [model['name'] for model in data.get('models', [])]
                    
                    if self.model_name in models:
                        logger.info(f"‚úÖ Mod√®le {self.model_name} d√©j√† charg√©")
                        return True
                    else:
                        logger.info(f"üìã Mod√®les disponibles: {models}")
                        logger.info(f"‚ùå Mod√®le {self.model_name} non trouv√©")
                        return False
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification mod√®les: {e}")
            return False
    
    def create_modelfile(self) -> str:
        """Cr√©ation Modelfile temporaire"""
        modelfile_content = self.modelfile_template.format(model_path=self.model_path)
        
        # Cr√©ation fichier temporaire
        with tempfile.NamedTemporaryFile(mode='w', suffix='.modelfile', delete=False) as f:
            f.write(modelfile_content)
            modelfile_path = f.name
        
        logger.info(f"üìù Modelfile cr√©√©: {modelfile_path}")
        return modelfile_path
    
    def import_model_ollama(self, modelfile_path: str) -> bool:
        """Import mod√®le dans Ollama via CLI"""
        try:
            logger.info(f"üì• Import mod√®le {self.model_name} dans Ollama...")
            logger.info("‚è≥ Cette op√©ration peut prendre plusieurs minutes...")
            
            # Commande ollama create
            cmd = ["ollama", "create", self.model_name, "-f", modelfile_path]
            
            logger.info(f"üîß Commande: {' '.join(cmd)}")
            
            # Ex√©cution avec capture output
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600  # 10 minutes max
            )
            
            if result.returncode == 0:
                logger.info("‚úÖ Import r√©ussi!")
                logger.info(f"üì§ Output: {result.stdout}")
                return True
            else:
                logger.error(f"‚ùå Import √©chou√© (code {result.returncode})")
                logger.error(f"üì§ Stdout: {result.stdout}")
                logger.error(f"üì§ Stderr: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("‚ùå Import timeout (>10min)")
            return False
        except FileNotFoundError:
            logger.error("‚ùå Commande 'ollama' non trouv√©e - Ollama CLI requis")
            return False
        except Exception as e:
            logger.error(f"‚ùå Erreur import: {e}")
            return False
    
    async def test_imported_model(self) -> bool:
        """Test rapide mod√®le import√©"""
        try:
            logger.info("üß™ Test mod√®le import√©...")
            
            payload = {
                "model": self.model_name,
                "prompt": "Bonjour !",
                "stream": False,
                "options": {
                    "num_predict": 10
                }
            }
            
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(
                    f"{self.ollama_base_url}/api/generate",
                    json=payload
                )
                
                if response.status_code == 200:
                    data = response.json()
                    response_text = data.get('response', '').strip()
                    logger.info(f"‚úÖ Test r√©ussi: '{response_text}'")
                    return True
                else:
                    logger.error(f"‚ùå Test √©chou√©: {response.status_code}")
                    return False
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur test: {e}")
            return False
    
    def cleanup_modelfile(self, modelfile_path: str):
        """Nettoyage fichier temporaire"""
        try:
            os.unlink(modelfile_path)
            logger.info("üßπ Modelfile temporaire supprim√©")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur nettoyage: {e}")
    
    async def run_import_process(self) -> bool:
        """Processus complet d'import"""
        logger.info("üì• D√âMARRAGE IMPORT LLAMA3 8B DANS OLLAMA")
        
        # 1. V√©rifications pr√©alables
        if not self.check_model_file_exists():
            return False
        
        if not await self.check_ollama_available():
            return False
        
        # 2. V√©rification si d√©j√† charg√©
        if await self.check_model_already_loaded():
            logger.info("‚úÖ Mod√®le d√©j√† disponible - import non n√©cessaire")
            return await self.test_imported_model()
        
        # 3. Cr√©ation Modelfile
        modelfile_path = self.create_modelfile()
        
        try:
            # 4. Import mod√®le
            if not self.import_model_ollama(modelfile_path):
                return False
            
            # 5. V√©rification import
            if not await self.check_model_already_loaded():
                logger.error("‚ùå Mod√®le non trouv√© apr√®s import")
                return False
            
            # 6. Test fonctionnel
            return await self.test_imported_model()
            
        finally:
            # 7. Nettoyage
            self.cleanup_modelfile(modelfile_path)

async def main():
    """Import principal Llama3 8B"""
    try:
        importer = Llama3OllamaImporter()
        success = await importer.run_import_process()
        
        if success:
            print("\nüéä IMPORT LLAMA3 8B R√âUSSI")
            print("üöÄ Mod√®le pr√™t pour validation SuperWhisper V6")
            print(f"üìã Nom mod√®le: {importer.model_name}")
            print("üîÑ Vous pouvez maintenant ex√©cuter: python scripts/test_llama3_8b_validation.py")
            return 0
        else:
            print("\n‚ùå IMPORT LLAMA3 8B √âCHOU√â")
            print("üîß V√©rifiez Ollama CLI et fichier mod√®le")
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Erreur import: {e}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 