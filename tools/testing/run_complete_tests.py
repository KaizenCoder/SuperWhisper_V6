#!/usr/bin/env python3
"""
Script d'Ex√©cution Compl√®te des Tests TTS - SuperWhisper V6
Orchestration de tous les tests : pytest, d√©monstration, monitoring
üß™ Suite compl√®te de validation Phase 3

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import subprocess
import time
import json
from pathlib import Path
from datetime import datetime

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

class TTSTestSuite:
    """Suite compl√®te de tests TTS avec orchestration et rapports"""
    
    def __init__(self):
        self.start_time = time.perf_counter()
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'gpu_config': os.environ.get('CUDA_VISIBLE_DEVICES'),
            'tests': {}
        }
        
    def log(self, message, level="INFO"):
        """Logging avec timestamp"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")
        
    def run_command(self, command, description, timeout=300):
        """Ex√©cution d'une commande avec capture des r√©sultats"""
        self.log(f"üîÑ {description}")
        self.log(f"   Commande: {' '.join(command)}")
        
        start_time = time.perf_counter()
        
        try:
            result = subprocess.run(
                command,
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=Path.cwd()
            )
            
            execution_time = time.perf_counter() - start_time
            
            success = result.returncode == 0
            
            test_result = {
                'success': success,
                'execution_time': execution_time,
                'returncode': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'command': ' '.join(command)
            }
            
            if success:
                self.log(f"‚úÖ {description} r√©ussi ({execution_time:.1f}s)")
            else:
                self.log(f"‚ùå {description} √©chou√© ({execution_time:.1f}s)")
                self.log(f"   Code retour: {result.returncode}")
                if result.stderr:
                    self.log(f"   Erreur: {result.stderr[:200]}...")
                    
            return test_result
            
        except subprocess.TimeoutExpired:
            execution_time = time.perf_counter() - start_time
            self.log(f"‚è∞ {description} timeout apr√®s {execution_time:.1f}s")
            return {
                'success': False,
                'execution_time': execution_time,
                'error': 'timeout',
                'command': ' '.join(command)
            }
        except Exception as e:
            execution_time = time.perf_counter() - start_time
            self.log(f"üí• {description} erreur: {e}")
            return {
                'success': False,
                'execution_time': execution_time,
                'error': str(e),
                'command': ' '.join(command)
            }
    
    def run_pytest_tests(self):
        """Ex√©cution des tests pytest"""
        self.log("üß™ PHASE 1: Tests Pytest d'Int√©gration", "PHASE")
        
        # V√©rification de l'existence du fichier de test
        test_file = Path("tests/test_tts_manager_integration.py")
        if not test_file.exists():
            self.log("‚ùå Fichier de test pytest non trouv√©", "ERROR")
            return {
                'success': False,
                'error': 'Test file not found',
                'execution_time': 0
            }
        
        # Ex√©cution des tests pytest
        command = [
            sys.executable, "-m", "pytest",
            str(test_file),
            "-v", "-s",
            "--tb=short",
            "--disable-warnings",
            "--color=yes"
        ]
        
        return self.run_command(command, "Tests Pytest d'Int√©gration", timeout=600)
    
    def run_demo_batch(self):
        """Ex√©cution de la d√©monstration batch"""
        self.log("üéµ PHASE 2: D√©monstration Batch", "PHASE")
        
        demo_file = Path("scripts/demo_tts.py")
        if not demo_file.exists():
            self.log("‚ùå Script de d√©monstration non trouv√©", "ERROR")
            return {
                'success': False,
                'error': 'Demo script not found',
                'execution_time': 0
            }
        
        # Simulation d'entr√©e pour le mode batch (choix 2)
        command = [sys.executable, str(demo_file)]
        
        # Pour automatiser, on utilise echo pour simuler l'entr√©e
        if os.name == 'nt':  # Windows
            full_command = f'echo 2 | python "{demo_file}"'
            result = self.run_command(["cmd", "/c", full_command], "D√©monstration Batch TTS", timeout=300)
        else:  # Unix/Linux
            full_command = f'echo "2" | python "{demo_file}"'
            result = self.run_command(["bash", "-c", full_command], "D√©monstration Batch TTS", timeout=300)
        
        return result
    
    def run_performance_tests(self):
        """Ex√©cution des tests de performance existants"""
        self.log("‚ö° PHASE 3: Tests de Performance", "PHASE")
        
        performance_tests = [
            "test_performance_simple.py",
            "monitor_phase3_demo.py"
        ]
        
        results = {}
        
        for test_script in performance_tests:
            test_path = Path(test_script)
            if test_path.exists():
                command = [sys.executable, str(test_path)]
                result = self.run_command(command, f"Test Performance {test_script}", timeout=180)
                results[test_script] = result
            else:
                self.log(f"‚ö†Ô∏è Script {test_script} non trouv√©", "WARNING")
                results[test_script] = {
                    'success': False,
                    'error': 'Script not found',
                    'execution_time': 0
                }
        
        return results
    
    def check_system_requirements(self):
        """V√©rification des pr√©requis syst√®me"""
        self.log("üîç PHASE 0: V√©rification des Pr√©requis", "PHASE")
        
        checks = {}
        
        # V√©rification Python
        python_version = sys.version_info
        checks['python_version'] = {
            'version': f"{python_version.major}.{python_version.minor}.{python_version.micro}",
            'valid': python_version >= (3, 8)
        }
        
        # V√©rification des modules critiques
        critical_modules = ['torch', 'yaml', 'pytest', 'asyncio']
        for module in critical_modules:
            try:
                __import__(module)
                checks[f'module_{module}'] = {'available': True}
                self.log(f"‚úÖ Module {module} disponible")
            except ImportError:
                checks[f'module_{module}'] = {'available': False}
                self.log(f"‚ùå Module {module} manquant", "ERROR")
        
        # V√©rification GPU
        try:
            import torch
            gpu_available = torch.cuda.is_available()
            if gpu_available:
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                checks['gpu'] = {
                    'available': True,
                    'name': gpu_name,
                    'memory_gb': gpu_memory,
                    'cuda_device': os.environ.get('CUDA_VISIBLE_DEVICES')
                }
                self.log(f"‚úÖ GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            else:
                checks['gpu'] = {'available': False}
                self.log("‚ùå GPU CUDA non disponible", "ERROR")
        except Exception as e:
            checks['gpu'] = {'available': False, 'error': str(e)}
            self.log(f"‚ùå Erreur v√©rification GPU: {e}", "ERROR")
        
        # V√©rification fichiers de configuration
        config_files = ['config/tts.yaml']
        for config_file in config_files:
            config_path = Path(config_file)
            checks[f'config_{config_file}'] = {
                'exists': config_path.exists(),
                'path': str(config_path)
            }
            if config_path.exists():
                self.log(f"‚úÖ Configuration {config_file} trouv√©e")
            else:
                self.log(f"‚ùå Configuration {config_file} manquante", "ERROR")
        
        return checks
    
    def generate_report(self):
        """G√©n√©ration du rapport final"""
        total_time = time.perf_counter() - self.start_time
        
        self.log("üìä G√âN√âRATION DU RAPPORT FINAL", "PHASE")
        
        # Calcul des statistiques
        all_tests = []
        for phase, tests in self.results['tests'].items():
            if isinstance(tests, dict):
                if 'success' in tests:  # Test unique
                    all_tests.append(tests)
                else:  # Groupe de tests
                    for test_name, test_result in tests.items():
                        if isinstance(test_result, dict) and 'success' in test_result:
                            all_tests.append(test_result)
        
        successful_tests = [t for t in all_tests if t.get('success', False)]
        failed_tests = [t for t in all_tests if not t.get('success', False)]
        
        total_execution_time = sum(t.get('execution_time', 0) for t in all_tests)
        
        # Rapport de synth√®se
        report = {
            'summary': {
                'total_duration': total_time,
                'total_tests': len(all_tests),
                'successful_tests': len(successful_tests),
                'failed_tests': len(failed_tests),
                'success_rate': len(successful_tests) / len(all_tests) if all_tests else 0,
                'total_execution_time': total_execution_time
            },
            'details': self.results
        }
        
        # Sauvegarde du rapport
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"test_report_complete_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # Affichage du rapport
        print("\n" + "="*80)
        print("üìä RAPPORT FINAL DES TESTS TTS SUPERWHISPER V6")
        print("="*80)
        print(f"üïí Dur√©e totale: {total_time:.1f}s")
        print(f"üß™ Tests ex√©cut√©s: {len(all_tests)}")
        print(f"‚úÖ R√©ussis: {len(successful_tests)}")
        print(f"‚ùå √âchecs: {len(failed_tests)}")
        print(f"üìà Taux de r√©ussite: {len(successful_tests)/len(all_tests)*100:.1f}%" if all_tests else "üìà Aucun test ex√©cut√©")
        print(f"üíæ Rapport d√©taill√©: {report_file}")
        
        if failed_tests:
            print(f"\n‚ùå TESTS √âCHOU√âS:")
            for i, test in enumerate(failed_tests, 1):
                print(f"   {i}. {test.get('command', 'Commande inconnue')}")
                if 'error' in test:
                    print(f"      Erreur: {test['error']}")
        
        print("\n" + "="*80)
        
        return report
    
    def run_all_tests(self):
        """Ex√©cution de tous les tests"""
        self.log("üöÄ D√âBUT DE LA SUITE COMPL√àTE DE TESTS TTS", "START")
        
        try:
            # Phase 0: Pr√©requis
            self.results['tests']['system_requirements'] = self.check_system_requirements()
            
            # Phase 1: Tests Pytest
            self.results['tests']['pytest_integration'] = self.run_pytest_tests()
            
            # Phase 2: D√©monstration Batch
            self.results['tests']['demo_batch'] = self.run_demo_batch()
            
            # Phase 3: Tests de Performance
            self.results['tests']['performance_tests'] = self.run_performance_tests()
            
            # G√©n√©ration du rapport final
            report = self.generate_report()
            
            # D√©termination du statut global
            success_rate = report['summary']['success_rate']
            if success_rate >= 0.8:
                self.log("üéâ SUITE DE TESTS GLOBALEMENT R√âUSSIE", "SUCCESS")
                return True
            else:
                self.log(f"‚ö†Ô∏è SUITE DE TESTS PARTIELLEMENT R√âUSSIE ({success_rate:.1%})", "WARNING")
                return False
                
        except Exception as e:
            self.log(f"üí• ERREUR CRITIQUE DANS LA SUITE DE TESTS: {e}", "CRITICAL")
            import traceback
            traceback.print_exc()
            return False

def main():
    """Point d'entr√©e principal"""
    print("üß™ SuperWhisper V6 - Suite Compl√®te de Tests TTS")
    print("üöÄ Validation Phase 3 : Pytest + D√©monstration + Performance")
    print()
    
    # V√©rification des arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == '--help':
            print("Usage: python run_complete_tests.py [--help]")
            print()
            print("Options:")
            print("  --help    Affiche cette aide")
            print()
            print("Ce script ex√©cute automatiquement:")
            print("  1. V√©rification des pr√©requis syst√®me")
            print("  2. Tests pytest d'int√©gration")
            print("  3. D√©monstration batch TTS")
            print("  4. Tests de performance")
            print("  5. G√©n√©ration du rapport final")
            return
    
    # Ex√©cution de la suite de tests
    test_suite = TTSTestSuite()
    success = test_suite.run_all_tests()
    
    # Code de sortie
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main() 