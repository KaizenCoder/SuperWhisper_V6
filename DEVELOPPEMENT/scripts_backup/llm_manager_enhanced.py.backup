"""
EnhancedLLMManager - Gestionnaire LLM avanc√© avec contexte conversationnel
Conforme aux sp√©cifications du Plan de D√©veloppement LUXA Final
"""
import asyncio
import logging
import time
from typing import Dict, Any, List, Optional
from llama_cpp import Llama
from dataclasses import dataclass
import json
from prometheus_client import Counter, Histogram, Gauge

# M√©triques Prometheus pour monitoring LLM
llm_requests_total = Counter('llm_requests_total', 'Total LLM requests')
llm_errors_total = Counter('llm_errors_total', 'Total LLM errors')
llm_response_time_seconds = Histogram('llm_response_time_seconds', 'LLM response time')
llm_tokens_generated_total = Counter('llm_tokens_generated_total', 'Total tokens generated')
llm_context_resets_total = Counter('llm_context_resets_total', 'Total context resets')

@dataclass
class ConversationTurn:
    """Un tour de conversation"""
    timestamp: float
    user_input: str
    assistant_response: str
    metadata: Dict[str, Any]

class EnhancedLLMManager:
    """
    Manager LLM avanc√© avec:
    - Gestion contexte conversationnel intelligent
    - M√©triques temps r√©el (latence, tokens, resets)
    - Optimisations performance (timeout, cleanup)
    - Health checks et monitoring
    - Post-processing intelligent des r√©ponses
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.conversation_history: List[ConversationTurn] = []
        self.max_context_turns = config.get("max_context_turns", 10)
        self.system_prompt = config.get("system_prompt", self._default_system_prompt())
        
        # M√©triques internes pour monitoring
        self.metrics = {
            "total_requests": 0,
            "avg_response_time": 0,
            "total_tokens_generated": 0,
            "context_resets": 0
        }
        
    def _default_system_prompt(self) -> str:
        """Prompt syst√®me par d√©faut pour LUXA"""
        return """Tu es LUXA, un assistant vocal intelligent et bienveillant.
        
Directives:
- R√©ponds de mani√®re naturelle et conversationnelle
- Sois concis mais informatif (max 100 mots par d√©faut)
- Adapte ton ton √† l'utilisateur
- Si la question n'est pas claire, demande des pr√©cisions
- Pour les commandes vocales, ex√©cute et confirme l'action
        
Contexte: Assistant vocal int√©gr√©, fran√ßais prioritaire."""

    async def initialize(self):
        """Initialisation du mod√®le LLM avec optimisations"""
        self.logger.info("Initialisation EnhancedLLMManager...")
        
        # CRITIQUE: V√©rifier configuration GPU RTX 3090 via CUDA_VISIBLE_DEVICES='1'
        gpu_index = self.config.get("gpu_device_index", 0)
        if gpu_index != 0:
            self.logger.warning(f"‚ö†Ô∏è gpu_device_index={gpu_index} - Avec CUDA_VISIBLE_DEVICES='1', utiliser index 0 (RTX 3090 visible)")
        
        self.logger.info(f"üéÆ GPU CONFIG: RTX 3090 via CUDA_VISIBLE_DEVICES='1' (main_gpu={gpu_index})")
        
        try:
            # Configuration optimis√©e - RTX 3090 UNIQUEMENT
            model_config = {
                "model_path": self.config["model_path"],
                "n_gpu_layers": self.config.get("n_gpu_layers", 35),
                "main_gpu": self.config.get("gpu_device_index", 0),  # RTX 3090 via CUDA_VISIBLE_DEVICES='1' - Device 0 visible = RTX 3090
                "n_ctx": self.config.get("context_length", 4096),
                "n_threads": self.config.get("n_threads", 4),
                "verbose": False,
                # Optimisations performance
                "use_mmap": True,
                "use_mlock": False,
                "f16_kv": True  # Optimisation m√©moire
            }
            
            start_time = time.time()
            self.model = Llama(**model_config)
            load_time = time.time() - start_time
            
            self.logger.info(f"‚úÖ LLM initialis√© en {load_time:.2f}s")
            
            # Test de fonctionnement
            await self._health_check()
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation LLM: {e}")
            raise
    
    async def _health_check(self):
        """Test rapide du mod√®le"""
        try:
            test_response = await self.generate_response(
                "Test", 
                max_tokens=5,
                internal_check=True
            )
            if not test_response:
                raise Exception("Health check failed - no response")
                
            self.logger.info("‚úÖ Health check LLM r√©ussi")
                
        except Exception as e:
            self.logger.error(f"‚ùå Health check LLM failed: {e}")
            raise

    async def generate_response(self, 
                              user_input: str,
                              max_tokens: int = 150,
                              temperature: float = 0.7,
                              include_context: bool = True,
                              internal_check: bool = False) -> str:
        """G√©n√©ration de r√©ponse avec contexte conversationnel"""
        
        if not internal_check:
            self.metrics["total_requests"] += 1
            llm_requests_total.inc()
        
        start_time = time.time()
        
        try:
            # Construction du prompt avec contexte
            full_prompt = self._build_contextual_prompt(
                user_input, 
                include_context=include_context
            )
            
            self.logger.debug(f"G√©n√©ration r√©ponse pour: '{user_input[:50]}...'")
            
            # G√©n√©ration avec timeout
            response = await asyncio.wait_for(
                asyncio.to_thread(self._generate_sync, full_prompt, max_tokens, temperature),
                timeout=30.0
            )
            
            # Post-traitement
            cleaned_response = self._clean_response(response)
            
            # Sauvegarde dans l'historique
            if not internal_check:
                self._add_to_history(user_input, cleaned_response)
                self._update_metrics(cleaned_response, time.time() - start_time)
            
            response_time = time.time() - start_time
            llm_response_time_seconds.observe(response_time)
            
            self.logger.info(f"‚úÖ R√©ponse g√©n√©r√©e en {response_time:.2f}s")
            return cleaned_response
            
        except asyncio.TimeoutError:
            self.logger.error("‚è±Ô∏è Timeout g√©n√©ration LLM")
            llm_errors_total.inc()
            return "D√©sol√©, le traitement prend trop de temps. Pouvez-vous r√©p√©ter ?"
        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration LLM: {e}")
            llm_errors_total.inc()
            return "D√©sol√©, je rencontre un probl√®me technique. Pouvez-vous reformuler ?"
    
    def _generate_sync(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """G√©n√©ration synchrone (appel√©e via to_thread)"""
        result = self.model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=0.9,
            repeat_penalty=1.1,
            stop=["Human:", "User:", "\n\n"],
            echo=False
        )
        
        return result['choices'][0]['text']
    
    def _build_contextual_prompt(self, user_input: str, include_context: bool = True) -> str:
        """Construit le prompt avec contexte conversationnel"""
        
        prompt_parts = [self.system_prompt, "\n\n"]
        
        # Ajouter contexte r√©cent si demand√©
        if include_context and self.conversation_history:
            prompt_parts.append("Conversation r√©cente:\n")
            
            # Prendre les N derniers tours
            recent_history = self.conversation_history[-self.max_context_turns:]
            
            for turn in recent_history:
                prompt_parts.append(f"Utilisateur: {turn.user_input}\n")
                prompt_parts.append(f"Assistant: {turn.assistant_response}\n\n")
        
        # Requ√™te actuelle
        prompt_parts.append(f"Utilisateur: {user_input}\n")
        prompt_parts.append("Assistant: ")
        
        return "".join(prompt_parts)
    
    def _clean_response(self, raw_response: str) -> str:
        """Nettoie la r√©ponse du mod√®le"""
        # Supprimer les artifacts communs
        cleaned = raw_response.strip()
        
        # Supprimer les r√©p√©titions du prompt
        artifacts = ["Assistant:", "Utilisateur:", "Human:", "User:"]
        for artifact in artifacts:
            if cleaned.startswith(artifact):
                cleaned = cleaned[len(artifact):].strip()
        
        # Limiter la longueur si trop verbose
        if len(cleaned) > 500:
            # Couper √† la derni√®re phrase compl√®te
            sentences = cleaned.split('. ')
            if len(sentences) > 1:
                cleaned = '. '.join(sentences[:-1]) + '.'
        
        return cleaned
    
    def _add_to_history(self, user_input: str, assistant_response: str):
        """Ajoute un tour √† l'historique conversationnel"""
        turn = ConversationTurn(
            timestamp=time.time(),
            user_input=user_input,
            assistant_response=assistant_response,
            metadata={"source": "voice_assistant"}
        )
        
        self.conversation_history.append(turn)
        
        # Limiter la taille de l'historique
        max_history = self.config.get("max_history_size", 50)
        if len(self.conversation_history) > max_history:
            self.conversation_history = self.conversation_history[-max_history:]
            self.metrics["context_resets"] += 1
            llm_context_resets_total.inc()
            self.logger.info("üîÑ Historique conversationnel nettoy√©")
    
    def _update_metrics(self, response: str, response_time: float):
        """Met √† jour les m√©triques"""
        # Moyenne mobile du temps de r√©ponse
        alpha = 0.1
        self.metrics["avg_response_time"] = (
            alpha * response_time + 
            (1 - alpha) * self.metrics["avg_response_time"]
        )
        
        # Estimation tokens g√©n√©r√©s (approximative)
        estimated_tokens = len(response.split())
        self.metrics["total_tokens_generated"] += estimated_tokens
        llm_tokens_generated_total.inc(estimated_tokens)
    
    def clear_conversation(self):
        """Efface l'historique conversationnel"""
        self.conversation_history.clear()
        self.metrics["context_resets"] += 1
        llm_context_resets_total.inc()
        self.logger.info("üßπ Historique conversationnel effac√©")
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """R√©sum√© de la conversation actuelle"""
        if not self.conversation_history:
            return {"status": "no_conversation"}
        
        return {
            "total_turns": len(self.conversation_history),
            "duration_minutes": (
                (time.time() - self.conversation_history[0].timestamp) / 60
            ),
            "last_interaction": time.time() - self.conversation_history[-1].timestamp,
            "topics": self._extract_topics(),
            "sentiment": self._analyze_sentiment()
        }
    
    def _extract_topics(self) -> List[str]:
        """Extraction basique des sujets abord√©s"""
        # Impl√©mentation simple bas√©e sur mots-cl√©s
        # Dans un vrai syst√®me, utiliser NLP avanc√©
        common_words = set()
        for turn in self.conversation_history:
            words = turn.user_input.lower().split()
            common_words.update(word for word in words if len(word) > 4)
        
        return list(common_words)[:5]  # Top 5
    
    def _analyze_sentiment(self) -> str:
        """Analyse basique du sentiment"""
        # Impl√©mentation simple - dans un vrai syst√®me, utiliser mod√®le d√©di√©
        positive_words = ["merci", "super", "parfait", "excellent", "g√©nial"]
        negative_words = ["probl√®me", "erreur", "mauvais", "nul", "d√©√ßu"]
        
        pos_count = neg_count = 0
        for turn in self.conversation_history:
            text = turn.user_input.lower()
            pos_count += sum(1 for word in positive_words if word in text)
            neg_count += sum(1 for word in negative_words if word in text)
        
        if pos_count > neg_count:
            return "positive"
        elif neg_count > pos_count:
            return "negative"
        else:
            return "neutral"
    
    def get_status(self) -> Dict[str, Any]:
        """Status d√©taill√© du gestionnaire LLM"""
        return {
            "model_loaded": self.model is not None,
            "conversation_turns": len(self.conversation_history),
            "metrics": self.metrics.copy(),
            "memory_context": len(self._build_contextual_prompt("test", True)),
            "config": {
                "max_context_turns": self.max_context_turns,
                "model_path": self.config.get("model_path", "N/A")
            }
        }

    def get_metrics(self) -> Dict[str, Any]:
        """Export m√©triques pour monitoring"""
        return {
            **self.metrics,
            "model_loaded": self.model is not None,
            "conversation_turns": len(self.conversation_history),
            "last_interaction": (
                time.time() - self.conversation_history[-1].timestamp
                if self.conversation_history else None
            )
        }

    async def cleanup(self):
        """Nettoyage ressources et historique"""
        self.logger.info("Nettoyage EnhancedLLMManager...")
        
        # Effacer historique
        self.conversation_history.clear()
        
        # Lib√©ration mod√®le si n√©cessaire
        if self.model:
            # Note: llama_cpp ne n√©cessite pas de cleanup explicite
            self.model = None
            
        self.logger.info("‚úÖ Nettoyage termin√©") 