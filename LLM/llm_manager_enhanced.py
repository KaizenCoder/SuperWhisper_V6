#!/usr/bin/env python3
"""
EnhancedLLMManager - Gestionnaire LLM avanc√© avec contexte conversationnel
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE

Conforme aux sp√©cifications du Plan de D√©veloppement LUXA Final

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional
from llama_cpp import Llama
from dataclasses import dataclass
import json
from prometheus_client import Counter, Histogram, Gauge

def validate_rtx3090_mandatory():
    """Validation obligatoire de la configuration RTX 3090"""
    try:
        import torch
        if not torch.cuda.is_available():
            raise RuntimeError("üö´ CUDA non disponible - RTX 3090 requise")
        
        cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
        if cuda_devices != '1':
            raise RuntimeError(f"üö´ CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit √™tre '1'")
        
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory < 20:  # RTX 3090 = ~24GB
            raise RuntimeError(f"üö´ GPU ({gpu_memory:.1f}GB) trop petite - RTX 3090 requise")
        
        print(f"‚úÖ RTX 3090 valid√©e: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
    except ImportError:
        print("‚ö†Ô∏è PyTorch non disponible - validation GPU ignor√©e pour llama-cpp")

# M√©triques Prometheus pour monitoring LLM
llm_requests_total = Counter('llm_requests_total', 'Total LLM requests')
llm_errors_total = Counter('llm_errors_total', 'Total LLM errors')
llm_response_time_seconds = Histogram('llm_response_time_seconds', 'LLM response time')
llm_tokens_generated_total = Counter('llm_tokens_generated_total', 'Total tokens generated')
llm_context_resets_total = Counter('llm_context_resets_total', 'Total context resets')

@dataclass
class ConversationTurn:
    """Un tour de conversation"""
    timestamp: float
    user_input: str
    assistant_response: str
    metadata: Dict[str, Any]

class EnhancedLLMManager:
    """
    Manager LLM avanc√© avec:
    - Gestion contexte conversationnel intelligent
    - M√©triques temps r√©el (latence, tokens, resets)
    - Optimisations performance (timeout, cleanup)
    - Health checks et monitoring
    - Post-processing intelligent des r√©ponses
    - Configuration GPU RTX 3090 exclusive
    """
    
    def __init__(self, config: Dict[str, Any]):
        # Validation RTX 3090 obligatoire
        validate_rtx3090_mandatory()
        
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.conversation_history: List[ConversationTurn] = []
        self.max_context_turns = config.get("max_context_turns", 10)
        self.system_prompt = config.get("system_prompt", self._default_system_prompt())
        
        # M√©triques internes pour monitoring
        self.metrics = {
            "total_requests": 0,
            "avg_response_time": 0,
            "total_tokens_generated": 0,
            "context_resets": 0
        }
        
    def _default_system_prompt(self) -> str:
        """Prompt syst√®me par d√©faut pour LUXA"""
        return """Tu es LUXA, un assistant vocal intelligent et bienveillant.
        
Directives:
- R√©ponds de mani√®re naturelle et conversationnelle
- Sois concis mais informatif (max 100 mots par d√©faut)
- Adapte ton ton √† l'utilisateur
- Si la question n'est pas claire, demande des pr√©cisions
- Pour les commandes vocales, ex√©cute et confirme l'action
        
Contexte: Assistant vocal int√©gr√©, fran√ßais prioritaire."""

    async def initialize(self):
        """Initialisation du mod√®le LLM avec support Ollama et fallback local"""
        self.logger.info("Initialisation EnhancedLLMManager...")
        
        # Essayer d'abord Ollama
        use_ollama = self.config.get('use_ollama', True)
        base_url = self.config.get('base_url', 'http://127.0.0.1:11434/v1')
        model_name = self.config.get('model', 'nous-hermes')
        
        if use_ollama:
            try:
                import httpx
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get('http://127.0.0.1:11434/api/tags')
                    if response.status_code == 200:
                        models = response.json().get('models', [])
                        model_names = [m['name'] for m in models]
                        self.logger.info(f"‚úÖ Ollama accessible - Mod√®les: {model_names}")
                        
                        # V√©rifier si notre mod√®le existe (recherche exacte ou partielle)
                        exact_match = model_name in model_names
                        partial_match = any(model_name in name or name in model_name for name in model_names)
                        
                        if exact_match or partial_match:
                            # Utiliser le nom exact du mod√®le trouv√©
                            if exact_match:
                                actual_model = model_name
                            else:
                                actual_model = next(name for name in model_names if model_name in name or name in model_name)
                            
                            self.actual_model_name = actual_model
                            self.use_ollama = True
                            self.logger.info(f"‚úÖ Mod√®le trouv√©: {actual_model}")
                            return
                        else:
                            self.logger.warning(f"‚ö†Ô∏è Mod√®le {model_name} non trouv√© - Fallback local")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Ollama non accessible: {e} - Fallback local")
        
        # Fallback : mod√®le local
        self.use_ollama = False
        
        # V√©rifier si model_path est fourni pour fallback local
        model_path = self.config.get("model_path")
        if not model_path:
            self.logger.warning("‚ö†Ô∏è Aucun model_path configur√© - Utilisation fallback simple")
            self.model = None
            return
        
        # Configuration GPU RTX 3090 pour mod√®le local
        gpu_index = self.config.get("gpu_device_index", 0)
        if gpu_index != 0:
            self.logger.warning(f"‚ö†Ô∏è gpu_device_index={gpu_index} - Avec CUDA_VISIBLE_DEVICES='1', utiliser index 0 (RTX 3090 visible)")
            self.config["gpu_device_index"] = 0
        
        self.logger.info(f"üéÆ GPU CONFIG: RTX 3090 exclusif via CUDA_VISIBLE_DEVICES='1' (main_gpu=0)")
        
        try:
            # Configuration optimis√©e - RTX 3090 UNIQUEMENT
            model_config = {
                "model_path": model_path,
                "n_gpu_layers": self.config.get("n_gpu_layers", 35),
                "main_gpu": 0,  # RTX 3090 seule visible = index 0
                "n_ctx": self.config.get("context_length", 4096),
                "n_threads": self.config.get("n_threads", 4),
                "verbose": False,
                # Optimisations performance RTX 3090
                "use_mmap": True,
                "use_mlock": False,
                "f16_kv": True  # Optimisation m√©moire RTX 3090
            }
            
            start_time = time.time()
            self.model = Llama(**model_config)
            load_time = time.time() - start_time
            
            self.logger.info(f"‚úÖ LLM RTX 3090 initialis√© en {load_time:.2f}s")
            
            # Test de fonctionnement
            await self._health_check()
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation LLM RTX 3090: {e}")
            raise
    
    async def _health_check(self):
        """Test rapide du mod√®le"""
        try:
            test_response = await self.generate_response(
                "Test", 
                max_tokens=5,
                internal_check=True
            )
            if not test_response:
                raise Exception("Health check failed - no response")
                
            self.logger.info("‚úÖ Health check LLM RTX 3090 r√©ussi")
                
        except Exception as e:
            self.logger.error(f"‚ùå Health check LLM RTX 3090 failed: {e}")
            raise

    async def generate_response(self, 
                              user_input: str,
                              max_tokens: int = 150,
                              temperature: float = 0.7,
                              include_context: bool = True,
                              internal_check: bool = False) -> str:
        """G√©n√©ration de r√©ponse avec contexte conversationnel"""
        
        if not internal_check:
            self.metrics["total_requests"] += 1
            llm_requests_total.inc()
        
        start_time = time.time()
        
        try:
            # Construction du prompt avec contexte
            full_prompt = self._build_contextual_prompt(
                user_input, 
                include_context=include_context
            )
            
            # Essayer Ollama d'abord si disponible
            if hasattr(self, 'use_ollama') and self.use_ollama:
                try:
                    response = await self._generate_ollama(user_input, max_tokens, temperature)
                    if response:
                        if not internal_check:
                            self._add_to_history(user_input, response)
                            self._update_metrics(response, time.time() - start_time)
                        
                        response_time = time.time() - start_time
                        llm_response_time_seconds.observe(response_time)
                        
                        self.logger.info(f"‚úÖ R√©ponse Ollama g√©n√©r√©e en {response_time:.2f}s")
                        return response
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Ollama √©chou√©, fallback local: {e}")
            
            # Fallback : mod√®le local ou r√©ponse simple
            if self.model is None:
                self.logger.warning("‚ö†Ô∏è Aucun mod√®le LLM disponible - R√©ponse fallback")
                fallback_response = f"Je re√ßois votre message : '{user_input}'. Le syst√®me LLM n'est pas disponible actuellement, mais la reconnaissance vocale et la synth√®se fonctionnent parfaitement."
                
                if not internal_check:
                    self._add_to_history(user_input, fallback_response)
                    self._update_metrics(fallback_response, time.time() - start_time)
                
                response_time = time.time() - start_time
                llm_response_time_seconds.observe(response_time)
                
                self.logger.info(f"‚úÖ R√©ponse fallback g√©n√©r√©e en {response_time:.2f}s")
                return fallback_response
            
            self.logger.debug(f"G√©n√©ration r√©ponse RTX 3090 pour: '{user_input[:50]}...'")
            
            # G√©n√©ration avec timeout
            response = await asyncio.wait_for(
                asyncio.to_thread(self._generate_sync, full_prompt, max_tokens, temperature),
                timeout=30.0
            )
            
            # Post-traitement
            cleaned_response = self._clean_response(response)
            
            # Sauvegarde dans l'historique
            if not internal_check:
                self._add_to_history(user_input, cleaned_response)
                self._update_metrics(cleaned_response, time.time() - start_time)
            
            response_time = time.time() - start_time
            llm_response_time_seconds.observe(response_time)
            
            self.logger.info(f"‚úÖ R√©ponse RTX 3090 g√©n√©r√©e en {response_time:.2f}s")
            return cleaned_response
            
        except asyncio.TimeoutError:
            self.logger.error("‚è±Ô∏è Timeout g√©n√©ration LLM")
            llm_errors_total.inc()
            return "D√©sol√©, le traitement prend trop de temps. Pouvez-vous r√©p√©ter ?"
        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration LLM: {e}")
            llm_errors_total.inc()
            return "D√©sol√©, je rencontre un probl√®me technique. Pouvez-vous reformuler ?"
    
    async def _generate_ollama(self, user_input: str, max_tokens: int, temperature: float) -> str:
        """G√©n√©ration via Ollama API - VERSION CORRIG√âE"""
        try:
            import httpx
            
            # ‚úÖ CORRECTION: Utiliser l'API native Ollama avec le bon format
            actual_model = getattr(self, 'actual_model_name', self.config.get('model', 'nous-hermes'))
            
            # Format correct pour l'API native Ollama
            data = {
                "model": actual_model,
                "prompt": f"{self.system_prompt}\n\nUser: {user_input}\nAssistant:",
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1,
                    "stop": ["User:", "\n\n"]
                }
            }
            
            self.logger.info(f"üß† Requ√™te Ollama: model={actual_model}, tokens={max_tokens}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    'http://127.0.0.1:11434/api/generate',
                    json=data
                )
                
                if response.status_code == 200:
                    result = response.json()
                    response_text = result.get('response', '').strip()
                    
                    if response_text:
                        self.logger.info(f"‚úÖ Ollama r√©ponse: {response_text[:50]}...")
                        return response_text
                    else:
                        self.logger.warning("‚ö†Ô∏è Ollama r√©ponse vide")
                        return None
                else:
                    self.logger.error(f"‚ùå Ollama API error: {response.status_code} - {response.text}")
                    return None
                    
        except Exception as e:
            self.logger.error(f"‚ùå Erreur Ollama: {e}")
            return None
                    
        except Exception as e:
            self.logger.error(f"Erreur Ollama: {e}")
            return None
    
    def _generate_sync(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """G√©n√©ration synchrone (appel√©e via to_thread)"""
        result = self.model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=0.9,
            repeat_penalty=1.1,
            stop=["Human:", "User:", "\n\n"],
            echo=False
        )
        
        return result['choices'][0]['text']
    
    def _build_contextual_prompt(self, user_input: str, include_context: bool = True) -> str:
        """Construit le prompt avec contexte conversationnel"""
        
        prompt_parts = [self.system_prompt, "\n\n"]
        
        # Ajouter contexte r√©cent si demand√©
        if include_context and self.conversation_history:
            prompt_parts.append("Conversation r√©cente:\n")
            
            # Prendre les N derniers tours
            recent_history = self.conversation_history[-self.max_context_turns:]
            
            for turn in recent_history:
                prompt_parts.append(f"Utilisateur: {turn.user_input}\n")
                prompt_parts.append(f"Assistant: {turn.assistant_response}\n\n")
        
        # Requ√™te actuelle
        prompt_parts.append(f"Utilisateur: {user_input}\n")
        prompt_parts.append("Assistant: ")
        
        return "".join(prompt_parts)
    
    def _clean_response(self, raw_response: str) -> str:
        """Nettoie la r√©ponse du mod√®le"""
        # Supprimer les artifacts communs
        cleaned = raw_response.strip()
        
        # Supprimer les r√©p√©titions du prompt
        artifacts = ["Assistant:", "Utilisateur:", "Human:", "User:"]
        for artifact in artifacts:
            if cleaned.startswith(artifact):
                cleaned = cleaned[len(artifact):].strip()
        
        # Limiter la longueur si trop verbose
        if len(cleaned) > 500:
            # Couper √† la derni√®re phrase compl√®te
            sentences = cleaned.split('. ')
            if len(sentences) > 1:
                cleaned = '. '.join(sentences[:-1]) + '.'
        
        return cleaned
    
    def _add_to_history(self, user_input: str, assistant_response: str):
        """Ajoute un tour √† l'historique conversationnel"""
        turn = ConversationTurn(
            timestamp=time.time(),
            user_input=user_input,
            assistant_response=assistant_response,
            metadata={"source": "voice_assistant"}
        )
        
        self.conversation_history.append(turn)
        
        # Limiter la taille de l'historique
        max_history = self.config.get("max_history_size", 50)
        if len(self.conversation_history) > max_history:
            self.conversation_history = self.conversation_history[-max_history:]
            self.metrics["context_resets"] += 1
            llm_context_resets_total.inc()
            self.logger.info("üîÑ Historique conversationnel nettoy√©")
    
    def _update_metrics(self, response: str, response_time: float):
        """Met √† jour les m√©triques"""
        # Moyenne mobile du temps de r√©ponse
        alpha = 0.1
        self.metrics["avg_response_time"] = (
            alpha * response_time + 
            (1 - alpha) * self.metrics["avg_response_time"]
        )
        
        # Estimation tokens g√©n√©r√©s (approximative)
        estimated_tokens = len(response.split())
        self.metrics["total_tokens_generated"] += estimated_tokens
        llm_tokens_generated_total.inc(estimated_tokens)
    
    def clear_conversation(self):
        """Efface l'historique conversationnel"""
        self.conversation_history.clear()
        self.metrics["context_resets"] += 1
        llm_context_resets_total.inc()
        self.logger.info("üßπ Historique conversationnel effac√©")
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """R√©sum√© de la conversation actuelle"""
        if not self.conversation_history:
            return {"status": "no_conversation"}
        
        return {
            "total_turns": len(self.conversation_history),
            "duration_minutes": (
                (time.time() - self.conversation_history[0].timestamp) / 60
            ),
            "last_interaction": time.time() - self.conversation_history[-1].timestamp,
            "topics": self._extract_topics(),
            "sentiment": self._analyze_sentiment()
        }
    
    def _extract_topics(self) -> List[str]:
        """Extraction basique des sujets abord√©s"""
        # Impl√©mentation simple bas√©e sur mots-cl√©s
        # Dans un vrai syst√®me, utiliser NLP avanc√©
        common_words = set()
        for turn in self.conversation_history:
            words = turn.user_input.lower().split()
            common_words.update(word for word in words if len(word) > 4)
        
        return list(common_words)[:5]  # Top 5
    
    def _analyze_sentiment(self) -> str:
        """Analyse basique du sentiment"""
        # Impl√©mentation simple - dans un vrai syst√®me, utiliser mod√®le d√©di√©
        positive_words = ["merci", "super", "parfait", "excellent", "g√©nial"]
        negative_words = ["probl√®me", "erreur", "mauvais", "nul", "d√©√ßu"]
        
        pos_count = neg_count = 0
        for turn in self.conversation_history:
            text = turn.user_input.lower()
            pos_count += sum(1 for word in positive_words if word in text)
            neg_count += sum(1 for word in negative_words if word in text)
        
        if pos_count > neg_count:
            return "positive"
        elif neg_count > pos_count:
            return "negative"
        else:
            return "neutral"
    
    def get_status(self) -> Dict[str, Any]:
        """Status d√©taill√© du gestionnaire LLM"""
        return {
            "model_loaded": self.model is not None,
            "conversation_turns": len(self.conversation_history),
            "metrics": self.metrics.copy(),
            "memory_context": len(self._build_contextual_prompt("test", True)),
            "config": {
                "max_context_turns": self.max_context_turns,
                "model_path": self.config.get("model_path", "N/A"),
                "gpu_rtx3090": True
            }
        }

    def get_metrics(self) -> Dict[str, Any]:
        """Export m√©triques pour monitoring"""
        return {
            **self.metrics,
            "model_loaded": self.model is not None,
            "conversation_turns": len(self.conversation_history),
            "last_interaction": (
                time.time() - self.conversation_history[-1].timestamp
                if self.conversation_history else None
            ),
            "gpu_rtx3090": True
        }

    async def cleanup(self):
        """Nettoyage ressources et historique"""
        self.logger.info("Nettoyage EnhancedLLMManager RTX 3090...")
        
        # Effacer historique
        self.conversation_history.clear()
        
        # Lib√©ration mod√®le si n√©cessaire  
        if self.model:
            # Note: llama_cpp ne n√©cessite pas de cleanup explicite
            self.model = None
            
        self.logger.info("‚úÖ Nettoyage RTX 3090 termin√©")

# APPELER DANS TOUS LES SCRIPTS PRINCIPAUX
if __name__ == "__main__":
    validate_rtx3090_mandatory() 