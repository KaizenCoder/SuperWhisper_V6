#!/usr/bin/env python3
"""
Test des Optimisations Phase 3 - SuperWhisper V6 TTS
Validation compl√®te des 5 axes d'optimisation impl√©ment√©s
üöÄ Performance cible: <100ms par appel, textes 5000+ chars

üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import pathlib

# =============================================================================
# üöÄ PORTABILIT√â AUTOMATIQUE - EX√âCUTABLE DEPUIS N'IMPORTE O√ô
# =============================================================================
def _setup_portable_environment():
    """Configure l'environnement pour ex√©cution portable"""
    # D√©terminer le r√©pertoire racine du projet
    current_file = pathlib.Path(__file__).resolve()
    
    # Chercher le r√©pertoire racine (contient .git ou marqueurs projet)
    project_root = current_file
    for parent in current_file.parents:
        if any((parent / marker).exists() for marker in ['.git', 'pyproject.toml', 'requirements.txt', '.taskmaster']):
            project_root = parent
            break
    
    # Ajouter le projet root au Python path
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # Changer le working directory vers project root
    os.chdir(project_root)
    
    # Configuration GPU RTX 3090 obligatoire
    os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
    
    print(f"üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
    print(f"üìÅ Project Root: {project_root}")
    print(f"üíª Working Directory: {os.getcwd()}")
    
    return project_root

# Initialiser l'environnement portable
_PROJECT_ROOT = _setup_portable_environment()

# Maintenant imports normaux...

import asyncio
import logging
import time
import yaml
from pathlib import Path
from typing import Dict, Any, List

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Import des composants Phase 3
try:
    from TTS.handlers.piper_native_optimized import create_piper_native_handler
    from TTS.handlers.piper_daemon import create_piper_daemon_handler
    from TTS.utils.text_chunker import IntelligentTextChunker, AudioConcatenator
    from TTS.components.cache_optimized import OptimizedTTSCache
    from TTS.utils_audio import is_valid_wav, get_wav_info
    PHASE3_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Composants Phase 3 non disponibles: {e}")
    PHASE3_AVAILABLE = False

class Phase3OptimizationTester:
    """
    Testeur complet des optimisations Phase 3
    
    üöÄ TESTS COUVERTS:
    1. Binding Python natif Piper (vs CLI)
    2. Pipeline asynchrone daemon
    3. Chunking intelligent textes longs
    4. Cache LRU optimis√©
    5. Concat√©nation audio fluide
    """
    
    def __init__(self):
        self.config = self._load_config()
        self.test_results = {}
        self.total_tests = 0
        self.passed_tests = 0
        
        # Textes de test de complexit√© croissante
        self.test_texts = {
            'court': "Bonjour, ceci est un test court.",
            'moyen': "Ceci est un test de longueur moyenne pour valider les performances du syst√®me TTS optimis√©. " * 5,
            'long': "Voici un texte long pour tester le chunking intelligent. " * 50,  # ~2500 chars
            'tres_long': "Test de texte tr√®s long pour valider la gestion des textes de 5000+ caract√®res. " * 60,  # ~5000 chars
            'recurrent': "Message r√©current pour test cache."
        }
        
        print("üß™ Phase 3 Optimization Tester initialis√©")
        print(f"üìä {len(self.test_texts)} textes de test pr√©par√©s")
    
    def _load_config(self) -> Dict[str, Any]:
        """Chargement de la configuration TTS"""
        config_path = Path("config/tts.yaml")
        if not config_path.exists():
            raise FileNotFoundError(f"Configuration TTS introuvable: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    async def run_all_tests(self):
        """Ex√©cution de tous les tests Phase 3"""
        print("\n" + "="*80)
        print("üöÄ D√âMARRAGE TESTS OPTIMISATIONS PHASE 3")
        print("="*80)
        
        if not PHASE3_AVAILABLE:
            print("‚ùå Composants Phase 3 non disponibles - Tests annul√©s")
            return
        
        # Test 1: Binding Python natif
        await self._test_native_binding()
        
        # Test 2: Pipeline daemon (si activ√©)
        if self.config['backends'].get('piper_daemon', {}).get('enabled', False):
            await self._test_daemon_pipeline()
        
        # Test 3: Chunking intelligent
        await self._test_intelligent_chunking()
        
        # Test 4: Cache LRU optimis√©
        await self._test_optimized_cache()
        
        # Test 5: Concat√©nation audio
        await self._test_audio_concatenation()
        
        # Test 6: Performance globale
        await self._test_global_performance()
        
        # Rapport final
        self._generate_final_report()
    
    async def _test_native_binding(self):
        """Test 1: Binding Python natif vs CLI"""
        print("\nüî¨ TEST 1: BINDING PYTHON NATIF")
        print("-" * 50)
        
        try:
            # Configuration du handler optimis√©
            handler_config = self.config['backends']['piper_native_optimized']
            
            # Test de cr√©ation du handler
            start_time = time.perf_counter()
            handler = create_piper_native_handler(handler_config)
            init_time = (time.perf_counter() - start_time) * 1000
            
            print(f"‚úÖ Handler cr√©√© en {init_time:.1f}ms")
            
            # Test de synth√®se
            test_text = self.test_texts['moyen']
            start_time = time.perf_counter()
            
            audio_data = await handler.synthesize(test_text)
            synthesis_time = (time.perf_counter() - start_time) * 1000
            
            # Validation
            is_valid = is_valid_wav(audio_data)
            audio_info = get_wav_info(audio_data)
            
            print(f"‚úÖ Synth√®se r√©ussie en {synthesis_time:.1f}ms")
            print(f"   Audio: {len(audio_data)} bytes, {audio_info.get('duration_ms', 0):.0f}ms")
            print(f"   Format WAV valide: {is_valid}")
            
            # V√©rification performance cible
            target_latency = handler_config.get('target_latency_ms', 80)
            performance_ok = synthesis_time <= target_latency
            
            print(f"üéØ Performance: {synthesis_time:.1f}ms (cible: {target_latency}ms) {'‚úÖ' if performance_ok else '‚ö†Ô∏è'}")
            
            self.test_results['native_binding'] = {
                'success': True,
                'init_time_ms': init_time,
                'synthesis_time_ms': synthesis_time,
                'target_latency_ms': target_latency,
                'performance_ok': performance_ok,
                'audio_size_bytes': len(audio_data),
                'audio_valid': is_valid
            }
            
            self.passed_tests += 1
            
        except Exception as e:
            print(f"‚ùå Erreur test binding natif: {e}")
            self.test_results['native_binding'] = {'success': False, 'error': str(e)}
        
        self.total_tests += 1
    
    async def _test_daemon_pipeline(self):
        """Test 2: Pipeline daemon asynchrone"""
        print("\nüî¨ TEST 2: PIPELINE DAEMON ASYNCHRONE")
        print("-" * 50)
        
        try:
            handler_config = self.config['backends']['piper_daemon']
            
            # Test de cr√©ation du daemon
            start_time = time.perf_counter()
            daemon_handler = create_piper_daemon_handler(handler_config)
            init_time = (time.perf_counter() - start_time) * 1000
            
            print(f"‚úÖ Daemon handler cr√©√© en {init_time:.1f}ms")
            
            # Test de synth√®se
            test_text = self.test_texts['moyen']
            start_time = time.perf_counter()
            
            audio_data = await daemon_handler.synthesize(test_text)
            synthesis_time = (time.perf_counter() - start_time) * 1000
            
            # Validation
            is_valid = is_valid_wav(audio_data)
            
            print(f"‚úÖ Synth√®se daemon r√©ussie en {synthesis_time:.1f}ms")
            print(f"   Audio: {len(audio_data)} bytes, WAV valide: {is_valid}")
            
            # Nettoyage
            if hasattr(daemon_handler, 'cleanup'):
                await daemon_handler.cleanup()
            
            self.test_results['daemon_pipeline'] = {
                'success': True,
                'synthesis_time_ms': synthesis_time,
                'audio_valid': is_valid
            }
            
            self.passed_tests += 1
            
        except Exception as e:
            print(f"‚ùå Erreur test daemon: {e}")
            self.test_results['daemon_pipeline'] = {'success': False, 'error': str(e)}
        
        self.total_tests += 1
    
    async def _test_intelligent_chunking(self):
        """Test 3: Chunking intelligent pour textes longs"""
        print("\nüî¨ TEST 3: CHUNKING INTELLIGENT TEXTES LONGS")
        print("-" * 50)
        
        try:
            # Configuration du chunker
            chunker_config = self.config['advanced']['text_chunking']
            chunker = IntelligentTextChunker(chunker_config)
            
            # Test sur texte tr√®s long
            long_text = self.test_texts['tres_long']
            print(f"üìù Texte original: {len(long_text)} caract√®res")
            
            # D√©coupage
            start_time = time.perf_counter()
            chunks = chunker.chunk_text(long_text, backend_max_length=800)
            chunking_time = (time.perf_counter() - start_time) * 1000
            
            print(f"‚úÖ D√©coupage r√©ussi en {chunking_time:.1f}ms")
            print(f"   {len(chunks)} chunks g√©n√©r√©s")
            
            # Analyse des chunks
            total_chars = sum(len(chunk.text) for chunk in chunks)
            avg_chunk_size = total_chars / len(chunks) if chunks else 0
            estimated_duration = chunker.get_total_estimated_duration(chunks)
            
            print(f"   Taille moyenne: {avg_chunk_size:.0f} chars/chunk")
            print(f"   Dur√©e estim√©e: {estimated_duration/1000:.1f}s")
            
            # Validation des limites
            max_chunk_size = max(len(chunk.text) for chunk in chunks) if chunks else 0
            chunks_within_limits = max_chunk_size <= 800
            
            print(f"üéØ Respect limites: {max_chunk_size} chars max {'‚úÖ' if chunks_within_limits else '‚ùå'}")
            
            # Statistiques d√©taill√©es
            stats = chunker.get_chunking_stats(long_text, chunks)
            print(f"   Efficacit√©: {stats['chunking_efficiency']:.1%}")
            print(f"   Fronti√®res phrases: {stats['sentence_boundaries']}/{len(chunks)}")
            
            self.test_results['intelligent_chunking'] = {
                'success': True,
                'original_length': len(long_text),
                'chunk_count': len(chunks),
                'chunking_time_ms': chunking_time,
                'max_chunk_size': max_chunk_size,
                'within_limits': chunks_within_limits,
                'efficiency': stats['chunking_efficiency'],
                'estimated_duration_s': estimated_duration / 1000
            }
            
            self.passed_tests += 1
            
        except Exception as e:
            print(f"‚ùå Erreur test chunking: {e}")
            self.test_results['intelligent_chunking'] = {'success': False, 'error': str(e)}
        
        self.total_tests += 1
    
    async def _test_optimized_cache(self):
        """Test 4: Cache LRU optimis√©"""
        print("\nüî¨ TEST 4: CACHE LRU OPTIMIS√â")
        print("-" * 50)
        
        try:
            # Configuration du cache
            cache_config = self.config['cache']
            cache = OptimizedTTSCache(cache_config)
            
            # Test d'ajout au cache
            test_text = self.test_texts['recurrent']
            fake_audio = b'RIFF\x24\x00\x00\x00WAVEfmt \x10\x00\x00\x00\x01\x00\x01\x00\x22\x56\x00\x00\x44\xAC\x00\x00\x02\x00\x10\x00data\x00\x00\x00\x00'
            
            cache_key = cache.generate_key(test_text, {'voice': None, 'speed': None})
            
            # PUT
            start_time = time.perf_counter()
            put_success = await cache.put(cache_key, fake_audio, test_text, 'test_backend', 1000.0)
            put_time = (time.perf_counter() - start_time) * 1000
            
            print(f"‚úÖ Cache PUT r√©ussi en {put_time:.2f}ms")
            
            # GET (cache hit)
            start_time = time.perf_counter()
            cached_audio = await cache.get(cache_key)
            get_time = (time.perf_counter() - start_time) * 1000
            
            cache_hit = cached_audio is not None
            print(f"‚úÖ Cache GET {'HIT' if cache_hit else 'MISS'} en {get_time:.2f}ms")
            
            # Test de performance r√©p√©t√©e
            hit_times = []
            for i in range(10):
                start_time = time.perf_counter()
                await cache.get(cache_key)
                hit_times.append((time.perf_counter() - start_time) * 1000)
            
            avg_hit_time = sum(hit_times) / len(hit_times)
            print(f"üöÄ Performance moyenne: {avg_hit_time:.2f}ms/hit")
            
            # Statistiques du cache
            stats = cache.get_stats()
            print(f"   Hit rate: {stats['hit_rate_percent']:.1f}%")
            print(f"   Entr√©es: {stats['cache_entries']}")
            print(f"   Taille: {stats['cache_size_mb']:.2f} MB")
            
            # Nettoyage
            await cache.cleanup()
            
            self.test_results['optimized_cache'] = {
                'success': True,
                'put_time_ms': put_time,
                'get_time_ms': get_time,
                'avg_hit_time_ms': avg_hit_time,
                'cache_hit': cache_hit,
                'hit_rate_percent': stats['hit_rate_percent']
            }
            
            self.passed_tests += 1
            
        except Exception as e:
            print(f"‚ùå Erreur test cache: {e}")
            self.test_results['optimized_cache'] = {'success': False, 'error': str(e)}
        
        self.total_tests += 1
    
    async def _test_audio_concatenation(self):
        """Test 5: Concat√©nation audio fluide"""
        print("\nüî¨ TEST 5: CONCAT√âNATION AUDIO FLUIDE")
        print("-" * 50)
        
        try:
            # Configuration du concatenator
            concat_config = self.config['advanced']['text_chunking']
            concatenator = AudioConcatenator(concat_config)
            
            # Cr√©ation de chunks audio factices
            fake_wav_chunks = []
            for i in range(3):
                # Header WAV minimal + donn√©es factices
                wav_chunk = b'RIFF\x24\x00\x00\x00WAVEfmt \x10\x00\x00\x00\x01\x00\x01\x00\x22\x56\x00\x00\x44\xAC\x00\x00\x02\x00\x10\x00data\x00\x00\x00\x00'
                fake_wav_chunks.append(wav_chunk)
            
            print(f"üéµ {len(fake_wav_chunks)} chunks audio √† concat√©ner")
            
            # Concat√©nation
            start_time = time.perf_counter()
            concatenated_audio = concatenator.concatenate_wav_chunks(fake_wav_chunks)
            concat_time = (time.perf_counter() - start_time) * 1000
            
            print(f"‚úÖ Concat√©nation r√©ussie en {concat_time:.1f}ms")
            print(f"   Audio final: {len(concatenated_audio)} bytes")
            
            # Validation format
            is_valid = is_valid_wav(concatenated_audio)
            print(f"   Format WAV valide: {is_valid}")
            
            self.test_results['audio_concatenation'] = {
                'success': True,
                'concat_time_ms': concat_time,
                'chunk_count': len(fake_wav_chunks),
                'final_size_bytes': len(concatenated_audio),
                'audio_valid': is_valid
            }
            
            self.passed_tests += 1
            
        except Exception as e:
            print(f"‚ùå Erreur test concat√©nation: {e}")
            self.test_results['audio_concatenation'] = {'success': False, 'error': str(e)}
        
        self.total_tests += 1
    
    async def _test_global_performance(self):
        """Test 6: Performance globale int√©gr√©e"""
        print("\nüî¨ TEST 6: PERFORMANCE GLOBALE INT√âGR√âE")
        print("-" * 50)
        
        try:
            # Test avec le texte le plus long
            long_text = self.test_texts['tres_long']
            print(f"üìù Test performance sur {len(long_text)} caract√®res")
            
            # Simulation du pipeline complet
            start_time = time.perf_counter()
            
            # 1. Chunking
            chunker_config = self.config['advanced']['text_chunking']
            chunker = IntelligentTextChunker(chunker_config)
            chunks = chunker.chunk_text(long_text, backend_max_length=800)
            
            # 2. Cache check (simulation)
            cache_config = self.config['cache']
            cache = OptimizedTTSCache(cache_config)
            
            # 3. Synth√®se simul√©e par chunk
            chunk_times = []
            for chunk in chunks[:3]:  # Test sur 3 premiers chunks
                chunk_start = time.perf_counter()
                # Simulation synth√®se (80ms target)
                await asyncio.sleep(0.08)  # 80ms
                chunk_time = (time.perf_counter() - chunk_start) * 1000
                chunk_times.append(chunk_time)
            
            total_time = (time.perf_counter() - start_time) * 1000
            avg_chunk_time = sum(chunk_times) / len(chunk_times)
            
            print(f"‚úÖ Pipeline complet simul√© en {total_time:.1f}ms")
            print(f"   Chunks trait√©s: {len(chunk_times)}")
            print(f"   Temps moyen/chunk: {avg_chunk_time:.1f}ms")
            
            # Estimation temps total r√©el
            estimated_total = len(chunks) * avg_chunk_time
            print(f"üéØ Estimation temps total: {estimated_total:.1f}ms pour {len(chunks)} chunks")
            
            # Objectif performance: <100ms par appel
            performance_target_met = avg_chunk_time <= 100
            print(f"   Objectif <100ms/chunk: {'‚úÖ' if performance_target_met else '‚ö†Ô∏è'}")
            
            await cache.cleanup()
            
            self.test_results['global_performance'] = {
                'success': True,
                'total_time_ms': total_time,
                'avg_chunk_time_ms': avg_chunk_time,
                'estimated_total_ms': estimated_total,
                'performance_target_met': performance_target_met,
                'chunk_count': len(chunks)
            }
            
            self.passed_tests += 1
            
        except Exception as e:
            print(f"‚ùå Erreur test performance globale: {e}")
            self.test_results['global_performance'] = {'success': False, 'error': str(e)}
        
        self.total_tests += 1
    
    def _generate_final_report(self):
        """G√©n√©ration du rapport final"""
        print("\n" + "="*80)
        print("üìä RAPPORT FINAL - OPTIMISATIONS PHASE 3")
        print("="*80)
        
        success_rate = (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0
        
        print(f"üéØ R√©sultats: {self.passed_tests}/{self.total_tests} tests r√©ussis ({success_rate:.1f}%)")
        print()
        
        # D√©tail par test
        for test_name, result in self.test_results.items():
            status = "‚úÖ R√âUSSI" if result.get('success', False) else "‚ùå √âCHEC"
            print(f"{status} - {test_name.replace('_', ' ').title()}")
            
            if result.get('success', False):
                # Affichage des m√©triques cl√©s
                if 'synthesis_time_ms' in result:
                    print(f"    Latence: {result['synthesis_time_ms']:.1f}ms")
                if 'performance_ok' in result:
                    print(f"    Performance cible: {'‚úÖ' if result['performance_ok'] else '‚ö†Ô∏è'}")
                if 'hit_rate_percent' in result:
                    print(f"    Hit rate cache: {result['hit_rate_percent']:.1f}%")
            else:
                print(f"    Erreur: {result.get('error', 'Inconnue')}")
            print()
        
        # Recommandations
        print("üöÄ RECOMMANDATIONS:")
        
        if success_rate >= 80:
            print("‚úÖ Optimisations Phase 3 op√©rationnelles")
            print("   ‚Üí D√©ploiement en production recommand√©")
        elif success_rate >= 60:
            print("‚ö†Ô∏è Optimisations partiellement fonctionnelles")
            print("   ‚Üí Corriger les √©checs avant d√©ploiement")
        else:
            print("‚ùå Optimisations n√©cessitent des corrections majeures")
            print("   ‚Üí R√©vision compl√®te requise")
        
        print("\nüéâ Tests Phase 3 termin√©s!")


async def main():
    """Point d'entr√©e principal"""
    tester = Phase3OptimizationTester()
    await tester.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main()) 