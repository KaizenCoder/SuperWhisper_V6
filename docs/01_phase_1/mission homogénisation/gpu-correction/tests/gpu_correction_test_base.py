#!/usr/bin/env python3
"""
ðŸš¨ TEMPLATE BASE POUR TESTS GPU - RTX 3090 OBLIGATOIRE
Base class pour validation GPU homogÃ¨ne SuperWhisper V6
"""

import os
import sys
import unittest
import torch
import functools
import gc
import time
from typing import Optional, Any

# =============================================================================
# ðŸš¨ CONFIGURATION GPU CRITIQUE - RTX 3090 EXCLUSIVEMENT
# =============================================================================
# Configuration physique : RTX 5060 Ti (CUDA:0) + RTX 3090 (CUDA:1)
# RTX 5060 Ti (CUDA:0) = STRICTEMENT INTERDITE
# RTX 3090 (CUDA:1) = SEULE GPU AUTORISÃ‰E VIA MAPPING CUDA:0

# FORCER RTX 3090 EXCLUSIVEMENT
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 Bus PCI 1 â†’ CUDA:0 logique
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre physique stable
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

print("ðŸŽ® GPU Configuration: RTX 3090 (CUDA:1â†’CUDA:0) forcÃ©e")
print(f"ðŸ”’ CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
print(f"ðŸ”’ CUDA_DEVICE_ORDER: {os.environ.get('CUDA_DEVICE_ORDER')}")

def validate_rtx3090_mandatory() -> None:
    """
    Validation OBLIGATOIRE de la configuration RTX 3090
    LÃ¨ve une exception si RTX 3090 non dÃ©tectÃ©e
    """
    if not torch.cuda.is_available():
        raise RuntimeError("ðŸš« CUDA non disponible - RTX 3090 requise")
    
    # VÃ©rification variables d'environnement critiques
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    if cuda_devices != '1':
        raise RuntimeError(f"ðŸš« CUDA_VISIBLE_DEVICES='{cuda_devices}' incorrect - doit Ãªtre '1' (RTX 3090)")
    
    cuda_order = os.environ.get('CUDA_DEVICE_ORDER', '')
    if cuda_order != 'PCI_BUS_ID':
        raise RuntimeError(f"ðŸš« CUDA_DEVICE_ORDER='{cuda_order}' incorrect - doit Ãªtre 'PCI_BUS_ID'")
    
    # VÃ©rification GPU physique (RTX 3090 = ~24GB VRAM)
    device_props = torch.cuda.get_device_properties(0)  # CUDA:0 logique = RTX 3090 physique
    gpu_memory_gb = device_props.total_memory / 1024**3
    gpu_name = device_props.name
    
    if gpu_memory_gb < 20:  # RTX 3090 â‰ˆ 24GB, seuil sÃ©curisÃ© 20GB
        raise RuntimeError(f"ðŸš« GPU ({gpu_name}, {gpu_memory_gb:.1f}GB) insuffisante - RTX 3090 (24GB) requise")
    
    print(f"âœ… RTX 3090 validÃ©e: {gpu_name} ({gpu_memory_gb:.1f}GB)")
    print(f"âœ… Mapping CUDA:0 â†’ RTX 3090 Bus PCI 1 : OPÃ‰RATIONNEL")

def gpu_test_cleanup(func):
    """
    DÃ©corateur pour nettoyage GPU automatique aprÃ¨s tests
    IntÃ¨gre Memory Leak V4.0 - Nettoyage agressif
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            # Validation obligatoire avant test
            validate_rtx3090_mandatory()
            
            # ExÃ©cution du test
            result = func(*args, **kwargs)
            
            return result
            
        except Exception as e:
            print(f"ðŸš« Erreur test GPU: {e}")
            raise
        finally:
            # Nettoyage agressif Memory Leak V4.0
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    gc.collect()
                    
                    # Attendre stabilisation GPU
                    time.sleep(0.1)
                    
                    # VÃ©rification mÃ©moire libÃ©rÃ©e
                    memory_freed = torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)
                    if memory_freed > 0:
                        print(f"ðŸ§¹ GPU cleanup: {memory_freed / 1024**2:.1f}MB libÃ©rÃ©s")
                        
                except Exception as cleanup_error:
                    print(f"âš ï¸ Erreur nettoyage GPU: {cleanup_error}")
    
    return wrapper

class GPUCorrectionTestBase(unittest.TestCase):
    """
    Classe de base pour tous les tests de correction GPU
    Assure validation RTX 3090 et nettoyage automatique
    """
    
    @classmethod
    def setUpClass(cls):
        """Setup class unique - Validation RTX 3090 obligatoire"""
        print(f"\nðŸ§ª DÃ‰MARRAGE TESTS GPU: {cls.__name__}")
        print("=" * 50)
        
        # Validation critique RTX 3090
        validate_rtx3090_mandatory()
        
        # Configuration device par dÃ©faut
        cls.device = "cuda:0"  # RTX 3090 via mapping CUDA_VISIBLE_DEVICES='1'
        cls.torch_device = torch.device(cls.device)
        
        print(f"ðŸŽ¯ Device configurÃ©: {cls.device} â†’ RTX 3090")
        print(f"ðŸŽ¯ Torch device: {cls.torch_device}")
    
    def setUp(self):
        """Setup avant chaque test - Nettoyage prÃ©ventif"""
        # Nettoyage prÃ©ventif
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
        
        # MÃ©triques initiales
        self.initial_memory = torch.cuda.memory_allocated(0) if torch.cuda.is_available() else 0
        
    def tearDown(self):
        """Cleanup aprÃ¨s chaque test - Memory Leak V4.0"""
        if torch.cuda.is_available():
            try:
                # Nettoyage agressif
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                gc.collect()
                
                # VÃ©rification memory leak
                final_memory = torch.cuda.memory_allocated(0)
                memory_diff = final_memory - self.initial_memory
                
                if memory_diff > 50 * 1024**2:  # Seuil 50MB
                    print(f"âš ï¸ Potentiel memory leak dÃ©tectÃ©: +{memory_diff / 1024**2:.1f}MB")
                else:
                    print(f"âœ… MÃ©moire GPU stable: {memory_diff / 1024**2:.1f}MB")
                    
            except Exception as e:
                print(f"âš ï¸ Erreur mesure mÃ©moire: {e}")
    
    @classmethod
    def tearDownClass(cls):
        """Cleanup class final - Nettoyage complet"""
        print(f"\nðŸ FIN TESTS GPU: {cls.__name__}")
        
        if torch.cuda.is_available():
            try:
                # Nettoyage final agressif
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                gc.collect()
                
                # Statistiques finales
                memory_allocated = torch.cuda.memory_allocated(0)
                memory_reserved = torch.cuda.memory_reserved(0)
                
                print(f"ðŸ“Š MÃ©moire finale allouÃ©e: {memory_allocated / 1024**2:.1f}MB")
                print(f"ðŸ“Š MÃ©moire finale rÃ©servÃ©e: {memory_reserved / 1024**2:.1f}MB")
                
            except Exception as e:
                print(f"âš ï¸ Erreur cleanup final: {e}")
        
        print("=" * 50)
    
    def get_gpu_device(self) -> str:
        """Retourne le device GPU configurÃ© (RTX 3090)"""
        return self.device
    
    def get_torch_device(self) -> torch.device:
        """Retourne le torch.device configurÃ© (RTX 3090)"""
        return self.torch_device
    
    def assert_gpu_available(self):
        """Assertion : GPU RTX 3090 disponible"""
        self.assertTrue(torch.cuda.is_available(), "ðŸš« CUDA/RTX 3090 non disponible")
        
        # VÃ©rifier que c'est bien la RTX 3090
        device_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        self.assertGreaterEqual(gpu_memory, 20, f"ðŸš« GPU {device_name} ({gpu_memory:.1f}GB) insuffisante - RTX 3090 requise")
        
        print(f"âœ… GPU validÃ©e dans test: {device_name} ({gpu_memory:.1f}GB)")
    
    @gpu_test_cleanup
    def test_gpu_configuration(self):
        """Test de base : Configuration GPU correcte"""
        # Validation RTX 3090
        self.assert_gpu_available()
        
        # VÃ©rification variables d'environnement
        self.assertEqual(os.environ.get('CUDA_VISIBLE_DEVICES'), '1')
        self.assertEqual(os.environ.get('CUDA_DEVICE_ORDER'), 'PCI_BUS_ID')
        
        # Test device fonctionnel
        test_tensor = torch.tensor([1.0, 2.0, 3.0]).to(self.torch_device)
        self.assertEqual(test_tensor.device.type, 'cuda')
        self.assertEqual(test_tensor.device.index, 0)  # CUDA:0 logique = RTX 3090
        
        print("âœ… Test configuration GPU: SUCCÃˆS")

# Fonction utilitaire pour validation rapide
def quick_gpu_validation():
    """Validation rapide RTX 3090 - Utilisable dans n'importe quel script"""
    try:
        validate_rtx3090_mandatory()
        print("âœ… Validation GPU rapide: RTX 3090 opÃ©rationnelle")
        return True
    except Exception as e:
        print(f"ðŸš« Validation GPU rapide: Ã‰CHEC - {e}")
        return False

if __name__ == "__main__":
    # Test autonome de la configuration
    print("ðŸ§ª TEST AUTONOME - Template Base GPU")
    print("=" * 50)
    
    # Validation RTX 3090
    validate_rtx3090_mandatory()
    
    # Tests de base
    suite = unittest.TestLoader().loadTestsFromTestCase(GPUCorrectionTestBase)
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    if result.wasSuccessful():
        print("\nðŸŽ¯ TEMPLATE GPU BASE: VALIDATION RÃ‰USSIE")
        sys.exit(0)
    else:
        print("\nðŸš« TEMPLATE GPU BASE: Ã‰CHEC VALIDATION")
        sys.exit(1) 