# tests/demo_enhanced_llm_interface.py
"""
D√©monstration de l'interface utilisateur avec EnhancedLLMManager
Validation de l'int√©gration compl√®te selon PRD v3.1
"""
import asyncio
import yaml
from pathlib import Path
import sys
import time

# Ajout du r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from LLM.llm_manager_enhanced import EnhancedLLMManager

async def demo_conversation_interface():
    """D√©monstration interactive de l'interface conversationnelle"""
    print("\n" + "="*80)
    print("üé§ D√âMONSTRATION INTERFACE CONVERSATIONNELLE LUXA")
    print("EnhancedLLMManager - Validation Interface Utilisateur")
    print("="*80)
    
    # 1. Configuration
    config_path = Path("Config/mvp_settings.yaml")
    if not config_path.exists():
        print(f"‚ùå Configuration non trouv√©e: {config_path}")
        return
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Configuration optimis√©e pour d√©mo
    llm_config = config.get('llm', {})
    llm_config.update({
        'max_context_turns': 5,  # Contexte √©tendu pour d√©mo
        'max_history_size': 20,
        'model_path': llm_config.get('model_path', './models/llm_model.gguf')
    })
    
    # 2. Initialisation
    print("\nüöÄ Initialisation EnhancedLLMManager...")
    try:
        llm_manager = EnhancedLLMManager(llm_config)
        await llm_manager.initialize()
        print("‚úÖ LLM Manager pr√™t pour conversation")
    except Exception as e:
        print(f"‚ùå Erreur initialisation: {e}")
        return
    
    # 3. D√©monstration conversation interactive
    print("\nüí¨ Mode Conversation Interactive")
    print("Commandes sp√©ciales:")
    print("  - 'quit' : Quitter")
    print("  - 'status' : Voir statut conversation")
    print("  - 'clear' : Vider historique")
    print("  - 'summary' : R√©sum√© conversation")
    print("-" * 50)
    
    try:
        while True:
            # Interface utilisateur
            try:
                user_input = input("\nüó£Ô∏è Vous: ").strip()
                
                if not user_input:
                    continue
                
                # Commandes sp√©ciales
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("üëã Au revoir! Merci pour cette conversation.")
                    break
                
                elif user_input.lower() == 'status':
                    status = llm_manager.get_status()
                    print(f"\nüìä Statut LLM Manager:")
                    print(f"   - Mod√®le charg√©: {status['model_loaded']}")
                    print(f"   - Tours conversation: {status['conversation_turns']}")
                    print(f"   - Contexte m√©moire: {status['memory_context']} caract√®res")
                    print(f"   - Requ√™tes totales: {status['metrics']['total_requests']}")
                    print(f"   - Latence moyenne: {status['metrics']['avg_response_time']:.3f}s")
                    continue
                
                elif user_input.lower() == 'clear':
                    llm_manager.clear_conversation()
                    print("üßπ Historique conversation effac√©")
                    continue
                
                elif user_input.lower() == 'summary':
                    summary = llm_manager.get_conversation_summary()
                    if summary['status'] == 'no_conversation':
                        print("üìù Aucune conversation en cours")
                    else:
                        print(f"\nüìã R√©sum√© Conversation:")
                        print(f"   - Tours: {summary['total_turns']}")
                        print(f"   - Dur√©e: {summary['duration_minutes']:.1f} min")
                        print(f"   - Derni√®re interaction: {summary['last_interaction']:.1f}s")
                        print(f"   - Topics: {', '.join(summary['topics'][:3])}")
                        print(f"   - Sentiment: {summary['sentiment']}")
                    continue
                
                # G√©n√©ration r√©ponse normale
                print("üß† LUXA r√©fl√©chit...")
                start_time = time.time()
                
                response = await llm_manager.generate_response(
                    user_input,
                    max_tokens=150,
                    temperature=0.7,
                    include_context=True
                )
                
                latency = time.time() - start_time
                
                # Affichage r√©ponse avec m√©triques
                print(f"ü§ñ LUXA: {response}")
                print(f"‚è±Ô∏è Latence: {latency:.2f}s")
                
                # Indicateur contexte
                if len(llm_manager.conversation_history) > 1:
                    print(f"üí≠ Contexte: {len(llm_manager.conversation_history)} tours")
                
            except KeyboardInterrupt:
                print("\nüõë Interruption utilisateur...")
                break
            except Exception as e:
                print(f"‚ùå Erreur: {e}")
                continue
    
    finally:
        # 4. Rapport final et nettoyage
        print("\n" + "="*50)
        print("üìä RAPPORT FINAL CONVERSATION")
        print("="*50)
        
        # M√©triques finales
        final_metrics = llm_manager.get_metrics()
        print(f"üìà M√©triques Session:")
        print(f"   - Requ√™tes trait√©es: {final_metrics['total_requests']}")
        print(f"   - Tours conversation: {final_metrics['conversation_turns']}")
        print(f"   - Tokens g√©n√©r√©s: {final_metrics['total_tokens_generated']}")
        print(f"   - Resets contexte: {final_metrics['context_resets']}")
        print(f"   - Latence moyenne: {final_metrics['avg_response_time']:.3f}s")
        
        # R√©sum√© final si conversation
        if final_metrics['conversation_turns'] > 0:
            final_summary = llm_manager.get_conversation_summary()
            print(f"\nüéØ R√©sum√© Final:")
            print(f"   - Dur√©e totale: {final_summary['duration_minutes']:.1f} min")
            print(f"   - Topics abord√©s: {', '.join(final_summary['topics'][:5])}")
            print(f"   - Sentiment global: {final_summary['sentiment']}")
        
        # Nettoyage
        await llm_manager.cleanup()
        print("\n‚úÖ D√©monstration termin√©e - EnhancedLLMManager valid√©")

async def demo_performance_test():
    """Test de performance de l'interface"""
    print("\nüöÄ TEST PERFORMANCE INTERFACE")
    
    config_path = Path("Config/mvp_settings.yaml")
    if not config_path.exists():
        print("Configuration manquante")
        return
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    llm_manager = EnhancedLLMManager(config['llm'])
    
    try:
        await llm_manager.initialize()
        
        # Test rapidit√© r√©ponses
        test_prompts = [
            "Bonjour",
            "Comment allez-vous ?",
            "Quel temps fait-il ?",
            "Racontez-moi une blague",
            "Au revoir"
        ]
        
        print(f"‚è±Ô∏è Test {len(test_prompts)} requ√™tes...")
        
        total_start = time.time()
        latencies = []
        
        for i, prompt in enumerate(test_prompts, 1):
            start = time.time()
            response = await llm_manager.generate_response(prompt, max_tokens=50)
            latency = time.time() - start
            latencies.append(latency)
            
            print(f"  {i}. '{prompt}' ‚Üí {latency:.2f}s")
        
        total_time = time.time() - total_start
        avg_latency = sum(latencies) / len(latencies)
        
        print(f"\nüìä R√©sultats Performance:")
        print(f"   - Temps total: {total_time:.2f}s")
        print(f"   - Latence moyenne: {avg_latency:.2f}s")
        print(f"   - Latence max: {max(latencies):.2f}s")
        print(f"   - Latence min: {min(latencies):.2f}s")
        
        # V√©rification crit√®res PRD v3.1
        target_latency = 0.5  # 500ms objectif
        success_rate = sum(1 for lat in latencies if lat < target_latency) / len(latencies)
        
        print(f"\nüéØ Conformit√© PRD v3.1:")
        print(f"   - Cible latence: <{target_latency}s")
        print(f"   - Taux succ√®s: {success_rate*100:.1f}%")
        
        if success_rate >= 0.8:
            print("‚úÖ Performance acceptable")
        else:
            print("‚ö†Ô∏è Performance √† am√©liorer")
    
    finally:
        await llm_manager.cleanup()

async def main():
    """Fonction principale de d√©monstration"""
    print("üéÆ D√âMONSTRATION ENHANCED LLM MANAGER")
    print("Choisissez votre mode:")
    print("1. Conversation interactive")
    print("2. Test de performance")
    print("3. Les deux")
    
    choice = input("\nVotre choix (1-3): ").strip()
    
    if choice == "1":
        await demo_conversation_interface()
    elif choice == "2":
        await demo_performance_test()
    elif choice == "3":
        await demo_conversation_interface()
        await demo_performance_test()
    else:
        print("Choix invalide")

if __name__ == "__main__":
    asyncio.run(main()) 