#!/usr/bin/env python3
"""
Test Pipeline Rapide SuperWhisper V6
üö® CONFIGURATION GPU: RTX 3090 (CUDA:1) OBLIGATOIRE
"""

import os
import sys
import asyncio
import httpx
import yaml
from pathlib import Path

# =============================================================================
# üö® CONFIGURATION CRITIQUE GPU - RTX 3090 UNIQUEMENT 
# =============================================================================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 24GB EXCLUSIVEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Ordre stable des GPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

print("üéÆ GPU Configuration: RTX 3090 (CUDA:1) forc√©e")
print(f"üîí CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

async def test_llm_ollama():
    """Test rapide LLM Ollama"""
    print("\nüß™ Test LLM Ollama...")
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Test sant√©
            response = await client.get("http://localhost:11434/api/tags")
            if response.status_code == 200:
                print("‚úÖ Ollama op√©rationnel")
                
                # Test g√©n√©ration rapide
                payload = {
                    "model": "nous-hermes-2-mistral-7b-dpo:latest",
                    "messages": [{"role": "user", "content": "Dis juste 'Bonjour'"}],
                    "stream": False,
                    "options": {"num_predict": 10}
                }
                
                response = await client.post("http://localhost:11434/api/chat", json=payload)
                if response.status_code == 200:
                    data = response.json()
                    print(f"‚úÖ LLM g√©n√©ration: \"{data['message']['content'][:50]}...\"")
                    return True
                else:
                    print(f"‚ùå G√©n√©ration √©chou√©e: {response.status_code}")
                    return False
            else:
                print(f"‚ùå Ollama sant√© √©chou√©e: {response.status_code}")
                return False
                
    except Exception as e:
        print(f"‚ùå Erreur LLM: {e}")
        return False

def test_tts_files():
    """Test pr√©sence fichiers TTS valid√©s"""
    print("\nüîä Test fichiers TTS valid√©s...")
    
    files_to_check = [
        "D:/TTS_Voices/piper/fr_FR-siwis-medium.onnx",
        "D:/TTS_Voices/piper/fr_FR-siwis-medium.onnx.json"
    ]
    
    all_ok = True
    for file_path in files_to_check:
        if Path(file_path).exists():
            size_mb = Path(file_path).stat().st_size / (1024*1024)
            print(f"‚úÖ {Path(file_path).name} ({size_mb:.1f}MB)")
        else:
            print(f"‚ùå MANQUANT: {file_path}")
            all_ok = False
    
    if all_ok:
        print("‚úÖ TTS fr_FR-siwis-medium.onnx valid√© (14/06/2025)")
    
    return all_ok

def test_config_pipeline():
    """Test configuration pipeline.yaml"""
    print("\n‚öôÔ∏è Test configuration...")
    
    try:
        config_path = Path("PIPELINE/config/pipeline.yaml")
        if not config_path.exists():
            print("‚ùå pipeline.yaml manquant")
            return False
            
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # V√©rifications critiques
        checks = [
            (config.get('llm', {}).get('endpoint') == "http://localhost:11434", "LLM endpoint Ollama"),
            (config.get('llm', {}).get('model') == "nous-hermes-2-mistral-7b-dpo:latest", "LLM mod√®le Hermes"),
            (config.get('tts', {}).get('primary_backend') == "unified", "TTS backend UnifiedTTSManager"),
            (config.get('pipeline', {}).get('llm_endpoint') == "http://localhost:11434/api/chat", "Pipeline LLM endpoint"),
        ]
        
        all_ok = True
        for check, description in checks:
            if check:
                print(f"‚úÖ {description}")
            else:
                print(f"‚ùå {description}")
                all_ok = False
        
        return all_ok
        
    except Exception as e:
        print(f"‚ùå Erreur config: {e}")
        return False

async def main():
    """Test complet rapide"""
    print("üöÄ TEST PIPELINE RAPIDE SUPERWHISPER V6")
    print("=" * 50)
    
    # Tests composants
    tests = [
        ("Configuration", test_config_pipeline()),
        ("TTS Fichiers", test_tts_files()),
        ("LLM Ollama", await test_llm_ollama()),
    ]
    
    # Rapport
    print("\n" + "=" * 50)
    print("üìä RAPPORT TESTS RAPIDES")
    print("=" * 50)
    
    for test_name, result in tests:
        status = "‚úÖ OK" if result else "‚ùå √âCHEC"
        print(f"{test_name:15} {status}")
    
    all_passed = all(result for _, result in tests)
    
    if all_passed:
        print(f"\nüéä TOUS LES TESTS R√âUSSIS !")
        print("‚úÖ Pipeline pr√™t pour validation compl√®te")
    else:
        print(f"\n‚ö†Ô∏è PROBL√àMES D√âTECT√âS")
        print("‚ùå Corriger avant validation compl√®te")
    
    return 0 if all_passed else 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 